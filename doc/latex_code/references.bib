
@article{harris_good_2008,
	title = {The {Good} {Engineer}: {Giving} {Virtue} its {Due} in {Engineering} {Ethics}},
	volume = {14},
	issn = {1471-5546},
	shorttitle = {The {Good} {Engineer}},
	url = {https://doi.org/10.1007/s11948-008-9068-3},
	doi = {10.1007/s11948-008-9068-3},
	abstract = {During the past few decades, engineering ethics has been oriented towards protecting the public from professional misconduct by engineers and from the harmful effects of technology. This “preventive ethics” project has been accomplished primarily by means of the promulgation of negative rules. However, some aspects of engineering professionalism, such as (1) sensitivity to risk (2) awareness of the social context of technology, (3) respect for nature, and (4) commitment to the public good, cannot be adequately accounted for in terms of rules, certainly not negative rules. Virtue ethics is a more appropriate vehicle for expressing these aspects of engineering professionalism. Some of the unique features of virtue ethics are the greater place it gives for discretion and judgment and also for inner motivation and commitment. Four of the many professional virtues that are important for engineers correspond to the four aspects of engineering professionalism listed above. Finally, the importance of the humanities and social sciences in promoting these virtues suggests that these disciplines are crucial in the professional education of engineers.},
	language = {en},
	number = {2},
	urldate = {2021-05-05},
	journal = {Science and Engineering Ethics},
	author = {Harris, Charles E.},
	month = may,
	year = {2008},
	pages = {153},
}

@article{rooney_how_2016,
	title = {How credible are the study results? {Evaluating} and applying internal validity tools to literature-based assessments of environmental health hazards},
	volume = {92-93},
	issn = {0160-4120},
	shorttitle = {How credible are the study results?},
	url = {https://www.sciencedirect.com/science/article/pii/S0160412016300058},
	doi = {10.1016/j.envint.2016.01.005},
	abstract = {Environmental health hazard assessments are routinely relied upon for public health decision-making. The evidence base used in these assessments is typically developed from a collection of diverse sources of information of varying quality. It is critical that literature-based evaluations consider the credibility of individual studies used to reach conclusions through consistent, transparent and accepted methods. Systematic review procedures address study credibility by assessing internal validity or “risk of bias” — the assessment of whether the design and conduct of a study compromised the credibility of the link between exposure/intervention and outcome. This paper describes the commonalities and differences in risk-of-bias methods developed or used by five groups that conduct or provide methodological input for performing environmental health hazard assessments: the Grading of Recommendations Assessment, Development, and Evaluation (GRADE) Working Group, the Navigation Guide, the National Toxicology Program's (NTP) Office of Health Assessment and Translation (OHAT) and Office of the Report on Carcinogens (ORoC), and the Integrated Risk Information System of the U.S. Environmental Protection Agency (EPA-IRIS). Each of these groups have been developing and applying rigorous assessment methods for integrating across a heterogeneous collection of human and animal studies to inform conclusions on potential environmental health hazards. There is substantial consistency across the groups in the consideration of risk-of-bias issues or “domains” for assessing observational human studies. There is a similar overlap in terms of domains addressed for animal studies; however, the groups differ in the relative emphasis placed on different aspects of risk of bias. Future directions for the continued harmonization and improvement of these methods are also discussed.},
	language = {en},
	urldate = {2021-05-05},
	journal = {Environment International},
	author = {Rooney, Andrew A. and Cooper, Glinda S. and Jahnke, Gloria D. and Lam, Juleen and Morgan, Rebecca L. and Boyles, Abee L. and Ratcliffe, Jennifer M. and Kraft, Andrew D. and Schünemann, Holger J. and Schwingl, Pamela and Walker, Teneille D. and Thayer, Kristina A. and Lunn, Ruth M.},
	month = jul,
	year = {2016},
	keywords = {Environmental health, Hazard assessment, Internal validity, Risk of bias, Systematic review},
	pages = {617--629},
}

@inproceedings{cacho_state_2020,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {The {State} of {Reproducible} {Research} in {Computer} {Science}},
	isbn = {978-3-030-43020-7},
	doi = {10.1007/978-3-030-43020-7_68},
	abstract = {Reproducible research is the cornerstone of cumulative science and yet is one of the most serious crisis that we face today in all fields. This paper aims to describe the ongoing reproducible research crisis along with counter-arguments of whether it really is a crisis, suggest solutions to problems limiting reproducible research along with the tools to implement such solutions by covering the latest publications involving reproducible research.},
	language = {en},
	booktitle = {17th {International} {Conference} on {Information} {Technology}–{New} {Generations} ({ITNG} 2020)},
	publisher = {Springer International Publishing},
	author = {Cacho, Jorge Ramón Fonseca and Taghva, Kazem},
	editor = {Latifi, Shahram},
	year = {2020},
	keywords = {Docker, Improving transparency, OCR, Open science, Replicability, Reproducibility},
	pages = {519--524},
}

@article{cockburn_threats_2020,
	title = {Threats of a replication crisis in empirical computer science},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3360311},
	doi = {10.1145/3360311},
	abstract = {Research replication only works if there is confidence built into the results.},
	language = {en},
	number = {8},
	urldate = {2021-05-05},
	journal = {Communications of the ACM},
	author = {Cockburn, Andy and Dragicevic, Pierre and Besançon, Lonni and Gutwin, Carl},
	month = jul,
	year = {2020},
	pages = {70--79},
}

@article{raychev_learning_2016,
	title = {Learning programs from noisy data},
	volume = {51},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2914770.2837671},
	doi = {10.1145/2914770.2837671},
	abstract = {We present a new approach for learning programs from noisy datasets. Our approach is based on two new concepts: a regularized program generator which produces a candidate program based on a small sample of the entire dataset while avoiding overfitting, and a dataset sampler which carefully samples the dataset by leveraging the candidate program's score on that dataset. The two components are connected in a continuous feedback-directed loop. We show how to apply this approach to two settings: one where the dataset has a bound on the noise, and another without a noise bound. The second setting leads to a new way of performing approximate empirical risk minimization on hypotheses classes formed by a discrete search space. We then present two new kinds of program synthesizers which target the two noise settings. First, we introduce a novel regularized bitstream synthesizer that successfully generates programs even in the presence of incorrect examples. We show that the synthesizer can detect errors in the examples while combating overfitting -- a major problem in existing synthesis techniques. We also show how the approach can be used in a setting where the dataset grows dynamically via new examples (e.g., provided by a human). Second, we present a novel technique for constructing statistical code completion systems. These are systems trained on massive datasets of open source programs, also known as ``Big Code''. The key idea is to introduce a domain specific language (DSL) over trees and to learn functions in that DSL directly from the dataset. These learned functions then condition the predictions made by the system. This is a flexible and powerful technique which generalizes several existing works as we no longer need to decide a priori on what the prediction should be conditioned (another benefit is that the learned functions are a natural mechanism for explaining the prediction). As a result, our code completion system surpasses the prediction capabilities of existing, hard-wired systems.},
	number = {1},
	urldate = {2021-05-05},
	journal = {ACM SIGPLAN Notices},
	author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin and Krause, Andreas},
	month = jan,
	year = {2016},
	keywords = {Anomaly Detection, Big Code, Noisy Data, Program Synthesis, Regularization, Statistical Code Completion},
	pages = {761--774},
}

@article{cockburn_threats_2020-1,
	title = {Threats of a replication crisis in empirical computer science},
	volume = {63},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3360311},
	doi = {10.1145/3360311},
	abstract = {Research replication only works if there is confidence built into the results.},
	number = {8},
	urldate = {2021-05-05},
	journal = {Communications of the ACM},
	author = {Cockburn, Andy and Dragicevic, Pierre and Besançon, Lonni and Gutwin, Carl},
	month = jul,
	year = {2020},
	pages = {70--79},
}

@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2021-05-05},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{jiang_context-based_2007,
	address = {Dubrovnik, Croatia},
	title = {Context-based detection of clone-related bugs},
	isbn = {978-1-59593-811-4},
	url = {http://portal.acm.org/citation.cfm?doid=1287624.1287634},
	doi = {10.1145/1287624.1287634},
	abstract = {Studies show that programs contain much similar code, commonly known as clones. One of the main reasons for introducing clones is programmers’ tendency to copy and paste code to quickly duplicate functionality. We commonly believe that clones can make programs difﬁcult to maintain and introduce subtle bugs. Although much research has proposed techniques for detecting and removing clones to improve software maintainability, little has considered how to detect latent bugs introduced by clones. In this paper, we introduce a general notion of context-based inconsistencies among clones and develop an efﬁcient algorithm to detect such inconsistencies for locating bugs. We have implemented our algorithm and evaluated it on large open source projects including the latest versions of the Linux kernel and Eclipse. We have discovered many previously unknown bugs and programming style issues in both projects (with 57 for the Linux kernel and 38 for Eclipse). We have also categorized the bugs and style issues and noticed that they exhibit diverse characteristics and are difﬁcult to detect with any single existing bug detection technique. We believe that our approach complements well these existing techniques.},
	language = {en},
	urldate = {2021-05-01},
	booktitle = {Proceedings of the the 6th joint meeting of the {European} software engineering conference and the {ACM} {SIGSOFT} symposium on {The} foundations of software engineering  - {ESEC}-{FSE} '07},
	publisher = {ACM Press},
	author = {Jiang, Lingxiao and Su, Zhendong and Chiu, Edwin},
	year = {2007},
	pages = {55},
}

@inproceedings{liu_nomen_2016,
	address = {Austin Texas},
	title = {Nomen est omen: exploring and exploiting similarities between argument and parameter names},
	isbn = {978-1-4503-3900-1},
	shorttitle = {Nomen est omen},
	url = {https://dl.acm.org/doi/10.1145/2884781.2884841},
	doi = {10.1145/2884781.2884841},
	abstract = {Programmer-provided identiﬁer names convey information about the semantics of a program. This information can complement traditional program analyses in various software engineering tasks, such as bug ﬁnding, code completion, and documentation. Even though identiﬁer names appear to be a rich source of information, little is known about their properties and their potential usefulness. This paper presents an empirical study of the lexical similarity between arguments and parameters of methods, which is one prominent situation where names can provide otherwise missing information. The study involves 60 real-world Java programs. We ﬁnd that, for most arguments, the similarity is either very high or very low, and that short and generic names often cause low similarities. Furthermore, we show that inferring a set of low-similarity parameter names from one set of programs allows for pruning such names in another set of programs. Finally, the study shows that many arguments are more similar to the corresponding parameter than any alternative argument available in the call site’s scope. As applications of our ﬁndings, we present an anomaly detection technique that identiﬁes 144 renaming opportunities and incorrect arguments in 14 programs, and a code recommendation system that suggests correct arguments with a precision of 83\%.},
	language = {en},
	urldate = {2021-05-01},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Liu, Hui and Liu, Qiurong and Staicu, Cristian-Alexandru and Pradel, Michael and Luo, Yue},
	month = may,
	year = {2016},
	pages = {1063--1073},
}

@inproceedings{sadowski_modern_2018,
	address = {Gothenburg Sweden},
	title = {Modern code review: a case study at google},
	isbn = {978-1-4503-5659-6},
	shorttitle = {Modern code review},
	url = {https://dl.acm.org/doi/10.1145/3183519.3183525},
	doi = {10.1145/3183519.3183525},
	abstract = {Employing lightweight, tool-based code review of code changes (aka modern code review) has become the norm for a wide variety of open-source and industrial systems. In this paper, we make an exploratory investigation of modern code review at Google. Google introduced code review early on and evolved it over the years; our study sheds light on why Google introduced this practice and analyzes its current status, after the process has been reﬁned through decades of code changes and millions of code reviews. By means of 12 interviews, a survey with 44 respondents, and the analysis of review logs for 9 million reviewed changes, we investigate motivations behind code review at Google, current practices, and developers’ satisfaction and challenges.},
	language = {en},
	urldate = {2021-05-01},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice}},
	publisher = {ACM},
	author = {Sadowski, Caitlin and Söderberg, Emma and Church, Luke and Sipko, Michal and Bacchelli, Alberto},
	month = may,
	year = {2018},
	pages = {181--190},
}

@article{shepperd_role_2018,
	title = {The role and value of replication in empirical software engineering results},
	volume = {99},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584917304305},
	doi = {10.1016/j.infsof.2018.01.006},
	abstract = {Objective: We aim to better understand the value of replication studies, the level of conﬁrmation between replication and original studies, what conﬁrmation means in a statistical sense and what factors modify this relationship.
Method: We perform a systematic review to identify relevant replication experimental studies in the areas of (i) software project eﬀort prediction and (ii) pair programming. Where suﬃcient details are provided we compute prediction intervals.
Results: Our review locates 28 unique articles that describe replications of 35 original studies that address 75 research questions. Of these 10 are external, 15 internal and 3 internal-same-article replications. The odds ratio of internal to external (conducted by independent researchers) replications of obtaining a ‘conﬁrmatory’ result is 8.64. We also found incomplete reporting hampered our ability to extract estimates of eﬀect sizes. Where we are able to compute replication prediction intervals these were surprisingly large.
Conclusion: We show that there is substantial evidence to suggest that current approaches to empirical replications are highly problematic. There is a consensus that replications are important, but there is a need for better reporting of both original and replicated studies. Given the low power and incomplete reporting of many original studies, it can be unclear the extent to which a replication is conﬁrmatory and to what extent it yields additional knowledge to the software engineering community. We recommend attention is switched from replication research to meta-analysis.},
	language = {en},
	urldate = {2021-05-01},
	journal = {Information and Software Technology},
	author = {Shepperd, Martin and Ajienka, Nemitari and Counsell, Steve},
	month = jul,
	year = {2018},
	pages = {120--132},
}

@article{hovemeyer_evaluating_nodate,
	title = {Evaluating and {Tuning} a {Static} {Analysis} to {Find} {Null} {Pointer} {Bugs}},
	abstract = {Using static analysis to detect memory access errors, such as null pointer dereferences, is not a new problem. However, much of the previous work has used rather sophisticated analysis techniques in order to detect such errors.},
	language = {en},
	author = {Hovemeyer, David and Spacco, Jaime and Pugh, William},
	pages = {7},
}

@inproceedings{karampatsis_how_2020,
	address = {Seoul Republic of Korea},
	title = {How {Often} {Do} {Single}-{Statement} {Bugs} {Occur}?: {The} {ManySStuBs4J} {Dataset}},
	isbn = {978-1-4503-7517-7},
	shorttitle = {How {Often} {Do} {Single}-{Statement} {Bugs} {Occur}?},
	url = {https://dl.acm.org/doi/10.1145/3379597.3387491},
	doi = {10.1145/3379597.3387491},
	abstract = {Program repair is an important but difficult software engineering problem. One way to achieve acceptable performance is to focus on classes of simple bugs, such as bugs with single statement fixes, or that match a small set of bug templates. However, it is very difficult to estimate the recall of repair techniques for simple bugs, as there are no datasets about how often the associated bugs occur in code. To fill this gap, we provide a dataset of 153,652 single statement bugfix changes mined from 1,000 popular open-source Java projects, annotated by whether they match any of a set of 16 bug templates, inspired by state-of-the-art program repair techniques. In an initial analysis, we find that about 33\% of the simple bug fixes match the templates, indicating that a remarkable number of single-statement bugs can be repaired with a relatively small set of templates. Further, we find that template fitting bugs appear with a frequency of about one bug per 1,600-2,500 lines of code (as measured by the size of the project’s latest version). We hope that the dataset will prove a resource for both future work in program repair and studies in empirical software engineering.},
	language = {en},
	urldate = {2021-05-01},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Karampatsis, Rafael-Michael and Sutton, Charles},
	month = jun,
	year = {2020},
	pages = {573--577},
}

@misc{noauthor_acornjsacorn_2021,
	title = {acornjs/acorn},
	url = {https://github.com/acornjs/acorn},
	abstract = {A small, fast, JavaScript-based JavaScript parser. Contribute to acornjs/acorn development by creating an account on GitHub.},
	urldate = {2021-05-01},
	publisher = {acornjs},
	month = may,
	year = {2021},
	note = {original-date: 2012-09-24T10:05:00Z},
}

@misc{noauthor_ais_nodate,
	title = {{AIS} {Transactions} on {Replication} {Research} {\textbar} {AIS} {Journals} {\textbar} {Association} for {Information} {Systems}},
	url = {https://aisel.aisnet.org/trr/},
	urldate = {2021-05-01},
}

@misc{noauthor_rose_nodate,
	title = {{ROSE} {Festival} 2018 - {Recognizing} and {Rewarding} {Open} {Science} in {Software} {Engineering} - {ESEC}/{FSE} 2018},
	url = {https://2018.fseconference.org/track/rosefest-2018},
	abstract = {ESEC/FSE hosting a new initiative that aims to take SE research artifacts to the next level. The ROSE festival is a world-wide salute to replication and reproducibility in SE. At the time of this writing, similar events are being considered for RE’19, ICSE’19, ESEC/FSE’19 and ESEM’19. 
Our aim is to create a venue where researchers can receive public credit for facilitating and participating in open science in SE (specifically, in creating replicated and reproduced results). ROSE is needed since most current conferences only evaluate research artifacts generated by that venue’s accepted pa ...},
	urldate = {2021-05-01},
}

@misc{pradel_michaelpradeldeepbugs_2021,
	title = {michaelpradel/{DeepBugs}},
	copyright = {MIT License         ,                 MIT License},
	url = {https://github.com/michaelpradel/DeepBugs},
	abstract = {DeepBugs is a framework for learning bug detectors from an existing code corpus.},
	urldate = {2021-05-01},
	author = {Pradel, Michael},
	month = apr,
	year = {2021},
	note = {original-date: 2017-11-23T10:18:25Z},
}

@misc{noauthor_150k_nodate,
	title = {150k {Javascript} {Dataset}},
	url = {https://www.sri.inf.ethz.ch/js150},
	abstract = {This dataset is released as a part of Machine Learning for Programming project that aims to create new kinds of programming tools and techniques based on machine learning and statistical models learned over massive codebases. For more information about the project, tools and other resources please visit the main project page.},
	language = {en},
	urldate = {2021-05-01},
	journal = {Secure, Reliable, and Intelligent Systems Lab},
}

@inproceedings{appel_modular_2016,
	address = {Lisbon},
	title = {Modular {Verification} for {Computer} {Security}},
	isbn = {978-1-5090-2607-4},
	url = {https://ieeexplore.ieee.org/document/7536361/},
	doi = {10.1109/CSF.2016.8},
	abstract = {For many software components, it is useful and important to verify their security. This can be done by an analysis of the software itself, or by isolating the software behind a protection mechanism such as an operating system kernel (virtual-memory protection) or cryptographic authentication (don’t accepted untrusted inputs). But the protection mechanisms themselves must then be veriﬁed not just for safety but for functional correctness. Several recent projects have demonstrated that formal, deductive functional-correctness veriﬁcation is now possible for kernels, crypto, and compilers. Here I explain some of the modularity principles that make these veriﬁcations possible.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2016 {IEEE} 29th {Computer} {Security} {Foundations} {Symposium} ({CSF})},
	publisher = {IEEE},
	author = {Appel, Andrew W.},
	month = jun,
	year = {2016},
	pages = {1--8},
}

@incollection{chang_bringing_2017,
	address = {Cham},
	title = {Bringing {Order} to the {Separation} {Logic} {Jungle}},
	volume = {10695},
	isbn = {978-3-319-71236-9 978-3-319-71237-6},
	url = {http://link.springer.com/10.1007/978-3-319-71237-6_10},
	abstract = {Research results from so-called “classical” separation logics are not easily ported to so-called “intuitionistic” separation logics, and vice versa. Basic questions like, “Can the frame rule be proved independently of whether the programming language is garbage-collected?” “Can amortized resource analysis be ported from one separation logic to another?” should be straightforward. But they are not. Proofs done in a particular separation logic are diﬃcult to generalize. We argue that this limitation is caused by incompatible semantics. For example, emp sometimes holds everywhere and sometimes only on units.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer International Publishing},
	author = {Cao, Qinxiang and Cuellar, Santiago and Appel, Andrew W.},
	editor = {Chang, Bor-Yuh Evan},
	year = {2017},
	doi = {10.1007/978-3-319-71237-6_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {190--211},
}

@incollection{ter_beek_abstraction_2019,
	address = {Cham},
	title = {Abstraction and {Subsumption} in {Modular} {Verification} of {C} {Programs}},
	volume = {11800},
	isbn = {978-3-030-30941-1 978-3-030-30942-8},
	url = {http://link.springer.com/10.1007/978-3-030-30942-8_34},
	abstract = {Representation predicates enable data abstraction in separation logic, but when the same concrete implementation may need to be abstracted in diﬀerent ways, one needs a notion of subsumption. We demonstrate function-speciﬁcation subtyping, analogous to subtyping, with a subsumption rule: if φ is a funspec-sub of ψ, that is φ {\textless}: ψ, then x : φ implies x : ψ, meaning that any function satisfying speciﬁcation φ can be used wherever a function satisfying ψ is demanded. We extend previous notions of Hoare-logic sub-speciﬁcation, which already included parameter adaption, to include framing (necessary for separation logic) and impredicative bifunctors (necessary for higher-order functions, i.e. function pointers). We show intersection speciﬁcations, with the expected relation to subtyping. We show how this enables compositional modular veriﬁcation of the functional correctness of C programs, in Coq, with foundational machine-checked proofs of soundness.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {Formal {Methods} – {The} {Next} 30 {Years}},
	publisher = {Springer International Publishing},
	author = {Beringer, Lennart and Appel, Andrew W.},
	editor = {ter Beek, Maurice H. and McIver, Annabelle and Oliveira, José N.},
	year = {2019},
	doi = {10.1007/978-3-030-30942-8_34},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {573--590},
}

@article{hobor_theory_nodate,
	title = {A {Theory} of {Indirection} via {Approximation}},
	abstract = {Building semantic models that account for various kinds of indirect reference has traditionally been a difﬁcult problem. Indirect reference can appear in many guises, such as heap pointers, higher-order functions, object references, and shared-memory mutexes.},
	language = {en},
	author = {Hobor, Aquinas and Dockins, Robert and Appel, Andrew W},
	pages = {14},
}

@incollection{hutchison_fresh_2009,
	address = {Berlin, Heidelberg},
	title = {A {Fresh} {Look} at {Separation} {Algebras} and {Share} {Accounting}},
	volume = {5904},
	isbn = {978-3-642-10671-2 978-3-642-10672-9},
	url = {http://link.springer.com/10.1007/978-3-642-10672-9_13},
	abstract = {Separation Algebras serve as models of Separation Logics; Share Accounting allows reasoning about concurrent-read/exclusive-write resources in Separation Logic. In designing a Concurrent Separation Logic and in mechanizing proofs of its soundness, we found previous axiomatizations of separation algebras and previous systems of share accounting to be useful but ﬂawed. We adjust the axioms of separation algebras; we demonstrate an operator calculus for constructing new separation algebras; we present a more powerful system of share accounting with a new, simple model; and we provide a reusable Coq development.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dockins, Robert and Hobor, Aquinas and Appel, Andrew W.},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Hu, Zhenjiang},
	year = {2009},
	doi = {10.1007/978-3-642-10672-9_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {161--177},
}

@incollection{drossopoulou_oracle_2008,
	address = {Berlin, Heidelberg},
	title = {Oracle {Semantics} for {Concurrent} {Separation} {Logic}},
	volume = {4960},
	isbn = {978-3-540-78738-9 978-3-540-78739-6},
	url = {http://link.springer.com/10.1007/978-3-540-78739-6_27},
	abstract = {We deﬁne (with machine-checked proofs in Coq) a modular operational semantics for Concurrent C minor—a language with shared memory, spawnable threads, and ﬁrst-class locks. By modular we mean that one can reason about sequential control and data-ﬂow knowing almost nothing about concurrency, and one can reason about concurrency knowing almost nothing about sequential control and data-ﬂow constructs. We present a Concurrent Separation Logic with ﬁrst-class locks and threads, and prove its soundness with respect to the operational semantics. Using our modularity principle, we proved the sequential C.S.L. rules (those inherited from sequential Separation Logic) simply by adapting Appel \& Blazy’s machine-checked soundness proofs. Our Concurrent C minor operational semantics is designed to connect to Leroy’s optimizing (sequential) C minor compiler; we propose our modular semantics as a way to adapt Leroy’s compiler-correctness proofs to the concurrent setting. Thus we will obtain end-to-end proofs: the properties you prove in Concurrent Separation Logic will be true of the program that actually executes on the machine.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hobor, Aquinas and Appel, Andrew W. and Nardelli, Francesco Zappa},
	editor = {Drossopoulou, Sophia},
	year = {2008},
	doi = {10.1007/978-3-540-78739-6_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {353--367},
}

@article{berger_impact_2019,
	title = {On the {Impact} of {Programming} {Languages} on {Code} {Quality}},
	url = {http://arxiv.org/abs/1901.10220},
	doi = {10.1145/3340571},
	abstract = {This paper is a reproduction of work by Ray et al. which claimed to have uncovered a statistically significant association between eleven programming languages and software defects in projects hosted on GitHub. First we conduct an experimental repetition, repetition is only partially successful, but it does validate one of the key claims of the original work about the association of ten programming languages with defects. Next, we conduct a complete, independent reanalysis of the data and statistical modeling steps of the original study. We uncover a number of flaws that undermine the conclusions of the original study as only four languages are found to have a statistically significant association with defects, and even for those the effect size is exceedingly small. We conclude with some additional sources of bias that should be investigated in follow up work and a few best practice recommendations for similar efforts.},
	urldate = {2021-02-24},
	journal = {arXiv:1901.10220 [cs]},
	author = {Berger, Emery D. and Hollenbeck, Celeste and Maj, Petr and Vitek, Olga and Vitek, Jan},
	month = apr,
	year = {2019},
	note = {arXiv: 1901.10220},
	keywords = {Computer Science - Software Engineering},
}

@article{ray_large_nodate,
	title = {A {Large} {Scale} {Study} of {Programming} {Languages} and {Code} {Quality} in {Github}},
	abstract = {What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static v.s. dynamic typing, strong v.s. weak typing on software quality. By triangulating ﬁndings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a signiﬁcant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing. We also ﬁnd that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages.},
	language = {en},
	author = {Ray, Baishakhi and Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar},
	pages = {11},
}

@misc{francisco_boffins_nodate,
	title = {Boffins debunk study claiming certain languages (cough, {C}, {PHP}, {JS}...) lead to more buggy code than others},
	url = {https://www.theregister.com/2019/01/30/programming_bugs/},
	abstract = {Hard evidence that some coding lingo encourage flaws remains elusive},
	language = {en},
	urldate = {2021-02-24},
	author = {Francisco, Thomas Claburn in San},
}

@misc{noauthor_convolutional_nodate,
	title = {A {Convolutional} {Attention} {Network} for {Extreme} {Summarization} of {Source} {Code}},
	url = {http://proceedings.mlr.press/v48/allamanis16.html},
	urldate = {2021-02-21},
}

@inproceedings{wang_bugram_2016,
	address = {New York, NY, USA},
	series = {{ASE} 2016},
	title = {Bugram: bug detection with n-gram language models},
	isbn = {978-1-4503-3845-5},
	shorttitle = {Bugram},
	url = {https://doi.org/10.1145/2970276.2970341},
	doi = {10.1145/2970276.2970341},
	abstract = {To improve software reliability, many rule-based techniques have been proposed to infer programming rules and detect violations of these rules as bugs. These rule-based approaches often rely on the highly frequent appearances of certain patterns in a project to infer rules. It is known that if a pattern does not appear frequently enough, rules are not learned, thus missing many bugs. In this paper, we propose a new approach—Bugram—that leverages n-gram language models instead of rules to detect bugs. Bugram models program tokens sequentially, using the n-gram language model. Token sequences from the program are then assessed according to their probability in the learned model, and low probability sequences are marked as potential bugs. The assumption is that low probability token sequences in a program are unusual, which may indicate bugs, bad practices, or unusual/special uses of code of which developers may want to be aware. We evaluate Bugram in two ways. First, we apply Bugram on the latest versions of 16 open source Java projects. Results show that Bugram detects 59 bugs, 42 of which are manually verified as correct, 25 of which are true bugs and 17 are code snippets that should be refactored. Among the 25 true bugs, 23 cannot be detected by PR-Miner. We have reported these bugs to developers, 7 of which have already been confirmed by developers (4 of them have already been fixed), while the rest await confirmation. Second, we further compare Bugram with three additional graph- and rule-based bug detection tools, i.e., JADET, Tikanga, and GrouMiner. We apply Bugram on 14 Java projects evaluated in these three studies. Bugram detects 21 true bugs, at least 10 of which cannot be detected by these three tools. Our results suggest that Bugram is complementary to existing rule-based bug detection approaches.},
	urldate = {2021-02-19},
	booktitle = {Proceedings of the 31st {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Song and Chollak, Devin and Movshovitz-Attias, Dana and Tan, Lin},
	month = aug,
	year = {2016},
	keywords = {Bug Detection, N-gram Language Model, Static Code Analysis},
	pages = {708--719},
}

@misc{noauthor_detecting_nodate,
	title = {Detecting argument selection defects {\textbar} {Proceedings} of the {ACM} on {Programming} {Languages}},
	url = {https://dl.acm.org/doi/abs/10.1145/3133928},
	urldate = {2021-02-20},
}

@inproceedings{pradel_detecting_2011,
	address = {New York, NY, USA},
	series = {{ISSTA} '11},
	title = {Detecting anomalies in the order of equally-typed method arguments},
	isbn = {978-1-4503-0562-4},
	url = {https://doi.org/10.1145/2001420.2001448},
	doi = {10.1145/2001420.2001448},
	abstract = {In statically-typed programming languages, the compiler ensures that method arguments are passed in the expected order by checking the type of each argument. However, calls to methods with multiple equally-typed parameters slip through this check. The uncertainty about the correct argument order of equally-typed arguments can cause various problems, for example, if a programmer accidentally reverses two arguments. We present an automated, static program analysis that detects such problems without any input except for the source code of a program. The analysis leverages the observation that programmer-given identifier names convey information about the semantics of arguments, which can be used to assign equally-typed arguments to their expected position. We evaluate the approach with a large corpus of Java programs and show that our analysis finds relevant anomalies with a precision of 76\%.},
	urldate = {2021-02-19},
	booktitle = {Proceedings of the 2011 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Pradel, Michael and Gross, Thomas R.},
	month = jul,
	year = {2011},
	keywords = {anomaly detection, automated program analysis, maintenance, method arguments, static analysis},
	pages = {232--242},
}

@inproceedings{liu_nomen_2016-1,
	address = {New York, NY, USA},
	series = {{ICSE} '16},
	title = {Nomen est omen: exploring and exploiting similarities between argument and parameter names},
	isbn = {978-1-4503-3900-1},
	shorttitle = {Nomen est omen},
	url = {https://doi.org/10.1145/2884781.2884841},
	doi = {10.1145/2884781.2884841},
	abstract = {Programmer-provided identifier names convey information about the semantics of a program. This information can complement traditional program analyses in various software engineering tasks, such as bug finding, code completion, and documentation. Even though identifier names appear to be a rich source of information, little is known about their properties and their potential usefulness. This paper presents an empirical study of the lexical similarity between arguments and parameters of methods, which is one prominent situation where names can provide otherwise missing information. The study involves 60 real-world Java programs. We find that, for most arguments, the similarity is either very high or very low, and that short and generic names often cause low similarities. Furthermore, we show that inferring a set of low-similarity parameter names from one set of programs allows for pruning such names in another set of programs. Finally, the study shows that many arguments are more similar to the corresponding parameter than any alternative argument available in the call site's scope. As applications of our findings, we present an anomaly detection technique that identifies 144 renaming opportunities and incorrect arguments in 14 programs, and a code recommendation system that suggests correct arguments with a precision of 83\%.},
	urldate = {2021-02-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Hui and Liu, Qiurong and Staicu, Cristian-Alexandru and Pradel, Michael and Luo, Yue},
	month = may,
	year = {2016},
	keywords = {empirical study, identifier names, method arguments, name-based program analysis, static analysis},
	pages = {1063--1073},
}

@misc{noauthor_nomen_nodate,
	title = {Nomen est omen {\textbar} {Proceedings} of the 38th {International} {Conference} on {Software} {Engineering}},
	url = {https://dl.acm.org/doi/abs/10.1145/2884781.2884841},
	urldate = {2021-02-20},
}

@inproceedings{host_debugging_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Debugging {Method} {Names}},
	isbn = {978-3-642-03013-0},
	doi = {10.1007/978-3-642-03013-0_14},
	abstract = {Meaningful method names are crucial for the readability and maintainability of software. Existing naming conventions focus on syntactic details, leaving programmers with little or no support in assuring meaningful names. In this paper, we show that naming conventions can go much further: we can mechanically check whether or not a method name and implementation are likely to be good matches for each other. The vast amount of software written in Java defines an implicit convention for pairing names and implementations. We exploit this to extract rules for method names, which are used to identify “naming bugs” in well-known Java applications. We also present an approach for automatic suggestion of more suitable names in the presence of mismatch between name and implementation.},
	language = {en},
	booktitle = {{ECOOP} 2009 – {Object}-{Oriented} {Programming}},
	publisher = {Springer},
	author = {Høst, Einar W. and Østvold, Bjarte M.},
	editor = {Drossopoulou, Sophia},
	year = {2009},
	keywords = {Candidate Phrase, Java Application, Java Programmer, Rule Violation, Semantic Distance},
	pages = {294--317},
}

@inproceedings{hovemeyer_evaluating_2005,
	address = {New York, NY, USA},
	series = {{PASTE} '05},
	title = {Evaluating and tuning a static analysis to find null pointer bugs},
	isbn = {978-1-59593-239-6},
	url = {https://doi.org/10.1145/1108792.1108798},
	doi = {10.1145/1108792.1108798},
	abstract = {Using static analysis to detect memory access errors, such as null pointer dereferences, is not a new problem. However, much of the previous work has used rather sophisticated analysis techniques in order to detect such errors.In this paper we show that simple analysis techniques can be used to identify many such software defects, both in production code and in student code. In order to make our analysis both simple and effective, we use a non-standard analysis which is neither complete nor sound. However, we find that it is effective at finding an interesting class of software defects.We describe the basic analysis we perform, as well as the additional errors we can detect using techniques such as annotations and inter-procedural analysis.In studies of both production software and student projects, we find false positive rates of around 20\% or less. In the student code base, we find that our static analysis techniques are able to pinpoint 50\% to 80\% of the defects leading to a null pointer exception at runtime.},
	urldate = {2021-02-19},
	booktitle = {Proceedings of the 6th {ACM} {SIGPLAN}-{SIGSOFT} workshop on {Program} analysis for software tools and engineering},
	publisher = {Association for Computing Machinery},
	author = {Hovemeyer, David and Spacco, Jaime and Pugh, William},
	month = sep,
	year = {2005},
	keywords = {static analysis, testing},
	pages = {13--19},
}

@inproceedings{aftandilian_building_2012,
	title = {Building {Useful} {Program} {Analysis} {Tools} {Using} an {Extensible} {Java} {Compiler}},
	doi = {10.1109/SCAM.2012.28},
	abstract = {Large software companies need customized tools to manage their source code. These tools are often built in an ad-hoc fashion, using brittle technologies such as regular expressions and home-grown parsers. Changes in the language cause the tools to break. More importantly, these ad-hoc tools often do not support uncommon-but-valid code code patterns. We report our experiences building source-code analysis tools at Google on top of a third-party, open-source, extensible compiler. We describe three tools in use on our Java code base. The first, Strict Java Dependencies, enforces our dependency policy in order to reduce JAR file sizes and testing load. The second, error-prone, adds new error checks to the compilation process and automates repair of those errors at a whole-code base scale. The third, Thindex, reduces the indexing burden for a Java IDE so that it can support Google-sized projects.},
	booktitle = {2012 {IEEE} 12th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation}},
	author = {Aftandilian, E. and Sauciuc, R. and Priya, S. and Krishnan, S.},
	month = sep,
	year = {2012},
	keywords = {Buildings, Computer bugs, Google, Google-sized projects, JAR file sizes, Java, Java IDE, Java dependencies, Libraries, Maintenance engineering, Testing, Thindex, ad-hoc fashion, ad-hoc tools, brittle technologies, bug finding, compilers, customized tools, dependency checking, extensible Java compiler, indexing, program analysis tools, program compilers, program diagnostics, program testing, public domain software, software companies, source-code analysis, static analysis, testing load, third-party open-source extensible compiler},
	pages = {14--23},
}

@article{li_vuldeepecker_2018,
	title = {{VulDeePecker}: {A} {Deep} {Learning}-{Based} {System} for {Vulnerability} {Detection}},
	shorttitle = {{VulDeePecker}},
	url = {http://arxiv.org/abs/1801.01681},
	doi = {10.14722/ndss.2018.23158},
	abstract = {The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were "silently" patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.},
	urldate = {2021-02-20},
	journal = {Proceedings 2018 Network and Distributed System Security Symposium},
	author = {Li, Zhen and Zou, Deqing and Xu, Shouhuai and Ou, Xinyu and Jin, Hai and Wang, Sujuan and Deng, Zhijun and Zhong, Yuyi},
	year = {2018},
	note = {arXiv: 1801.01681},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{wang_automatically_2016,
	title = {Automatically {Learning} {Semantic} {Features} for {Defect} {Prediction}},
	doi = {10.1145/2884781.2884804},
	abstract = {Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7\% in precision, 11.5\% in recall, and 14.2\% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9\% in F1.},
	booktitle = {2016 {IEEE}/{ACM} 38th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Wang, S. and Liu, T. and Tan, L.},
	month = may,
	year = {2016},
	note = {ISSN: 1558-1225},
	keywords = {AST, Buildings, CPDP, DBN, Data models, Feature extraction, Predictive models, Semantics, Syntactics, Training, WPDP, abstract syntax trees, belief networks, cross-project defect prediction, deep belief network, deep learning, defect prediction, feature generation, neural nets, open source projects, program diagnostics, programming language semantics, programs semantic representation, public domain software, representation-learning algorithm, semantic feature learning, software defect prediction, source code, source code (software), token vectors, within-project defect prediction},
	pages = {297--308},
}

@article{wasylkowski_mining_2011,
	title = {Mining temporal specifications from object usage},
	volume = {18},
	issn = {1573-7535},
	url = {https://doi.org/10.1007/s10515-011-0084-1},
	doi = {10.1007/s10515-011-0084-1},
	abstract = {A caller must satisfy the callee’s precondition—that is, reach a state in which the callee may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We combine static analysis with model checking to mine Fair Computation Tree Logic (CTLF) formulas that describe the operations a parameter goes through: “In parseProperties(String xml), the parameter xml normally stems from getProperties().” Such operational preconditions can be learned from program code, and the code can be checked for their violations. Applied to AspectJ, our Tikanga prototype found 169 violations of operational preconditions, uncovering 7 unique defects and 27 unique code smells—with 52\% true positives in the 25\% top-ranked violations.},
	language = {en},
	number = {3},
	urldate = {2021-02-20},
	journal = {Automated Software Engineering},
	author = {Wasylkowski, Andrzej and Zeller, Andreas},
	month = dec,
	year = {2011},
	pages = {263--292},
}

@inproceedings{pradel_detecting_2011-1,
	address = {New York, NY, USA},
	series = {{ISSTA} '11},
	title = {Detecting anomalies in the order of equally-typed method arguments},
	isbn = {978-1-4503-0562-4},
	url = {https://doi.org/10.1145/2001420.2001448},
	doi = {10.1145/2001420.2001448},
	abstract = {In statically-typed programming languages, the compiler ensures that method arguments are passed in the expected order by checking the type of each argument. However, calls to methods with multiple equally-typed parameters slip through this check. The uncertainty about the correct argument order of equally-typed arguments can cause various problems, for example, if a programmer accidentally reverses two arguments. We present an automated, static program analysis that detects such problems without any input except for the source code of a program. The analysis leverages the observation that programmer-given identifier names convey information about the semantics of arguments, which can be used to assign equally-typed arguments to their expected position. We evaluate the approach with a large corpus of Java programs and show that our analysis finds relevant anomalies with a precision of 76\%.},
	urldate = {2021-02-19},
	booktitle = {Proceedings of the 2011 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Pradel, Michael and Gross, Thomas R.},
	month = jul,
	year = {2011},
	keywords = {anomaly detection, automated program analysis, maintenance, method arguments, static analysis},
	pages = {232--242},
}

@inproceedings{liang_antminer_2016,
	address = {New York, NY, USA},
	series = {{ICSE} '16},
	title = {{AntMiner}: mining more bugs by reducing noise interference},
	isbn = {978-1-4503-3900-1},
	shorttitle = {{AntMiner}},
	url = {https://doi.org/10.1145/2884781.2884870},
	doi = {10.1145/2884781.2884870},
	abstract = {Detecting bugs with code mining has proven to be an effective approach. However, the existing methods suffer from reporting serious false positives and false negatives. In this paper, we developed an approach called AntMiner to improve the precision of code mining by carefully preprocessing the source code. Specifically, we employ the program slicing technique to decompose the original source repository into independent sub-repositories, taking critical operations (automatically extracted from source code) as slicing criteria. In this way, the statements irrelevant to a critical operation are excluded from the corresponding sub-repository. Besides, various semantics-equivalent representations are normalized into a canonical form. Eventually, the mining process can be performed on a refined code database, and false positives and false negatives can be significantly pruned. We have implemented AntMiner and applied it to detect bugs in the Linux kernel. It reported 52 violations that have been either confirmed as real bugs by the kernel development community or fixed in new kernel versions. Among them, 41 cannot be detected by a widely used representative analysis tool Coverity. Besides, the result of a comparative analysis shows that our approach can effectively improve the precision of code mining and detect subtle bugs that have previously been missed.},
	urldate = {2021-02-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Liang, Bin and Bian, Pan and Zhang, Yan and Shi, Wenchang and You, Wei and Cai, Yan},
	month = may,
	year = {2016},
	keywords = {bug detection, code mining, program slicing},
	pages = {333--344},
}

@article{ammons_mining_2002,
	title = {Mining specifications},
	volume = {37},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/565816.503275},
	doi = {10.1145/565816.503275},
	abstract = {Program verification is a promising approach to improving program quality, because it can search all possible program executions for specific errors. However, the need to formally describe correct behavior or errors is a major barrier to the widespread adoption of program verification, since programmers historically have been reluctant to write formal specifications. Automating the process of formulating specifications would remove a barrier to program verification and enhance its practicality.This paper describes specification mining, a machine learning approach to discovering formal specifications of the protocols that code must obey when interacting with an application program interface or abstract data type. Starting from the assumption that a working program is well enough debugged to reveal strong hints of correct protocols, our tool infers a specification by observing program execution and concisely summarizing the frequent interaction patterns as state machines that capture both temporal and data dependences. These state machines can be examined by a programmer, to refine the specification and identify errors, and can be utilized by automatic verification tools, to find bugs.Our preliminary experience with the mining tool has been promising. We were able to learn specifications that not only captured the correct protocol, but also discovered serious bugs.},
	number = {1},
	urldate = {2021-02-20},
	journal = {ACM SIGPLAN Notices},
	author = {Ammons, Glenn and Bodík, Rastislav and Larus, James R.},
	month = jan,
	year = {2002},
	pages = {4--16},
}

@inproceedings{gutmann_noise-contrastive_2010,
	title = {Noise-contrastive estimation: {A} new estimation principle for unnormalized statistical models},
	shorttitle = {Noise-contrastive estimation},
	url = {http://proceedings.mlr.press/v9/gutmann10a.html},
	abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially gene...},
	language = {en},
	urldate = {2021-02-20},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Gutmann, Michael and Hyvärinen, Aapo},
	month = mar,
	year = {2010},
	note = {ISSN: 1938-7228},
	pages = {297--304},
}

@article{jia_analysis_2011,
	title = {An {Analysis} and {Survey} of the {Development} of {Mutation} {Testing}},
	volume = {37},
	issn = {1939-3520},
	doi = {10.1109/TSE.2010.62},
	abstract = {Mutation Testing is a fault-based software testing technique that has been widely studied for over three decades. The literature on Mutation Testing has contributed a set of approaches, tools, developments, and empirical results. This paper provides a comprehensive analysis and survey of Mutation Testing. The paper also presents the results of several development trend analyses. These analyses provide evidence that Mutation Testing techniques and tools are reaching a state of maturity and applicability, while the topic of Mutation Testing itself is the subject of increasing interest.},
	number = {5},
	journal = {IEEE Transactions on Software Engineering},
	author = {Jia, Y. and Harman, M.},
	month = sep,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {Automata, Books, Computer languages, Educational institutions, Fault detection, Genetic mutations, History, Java, Mutation testing, Programming profession, Software testing, comprehensive analysis, development trend analysis, empirical results, fault diagnosis, fault-based software testing technique, mutation testing development, mutation testing technique, mutation testing tool, program testing, survey.},
	pages = {649--678},
}

@inproceedings{vasilescu_recovering_2017,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2017},
	title = {Recovering clear, natural identifiers from obfuscated {JS} names},
	isbn = {978-1-4503-5105-8},
	url = {https://doi.org/10.1145/3106237.3106289},
	doi = {10.1145/3106237.3106289},
	abstract = {Well-chosen variable names are critical to source code readability, reusability, and maintainability. Unfortunately, in deployed JavaScript code (which is ubiquitous on the web) the identifier names are frequently minified and overloaded. This is done both for efficiency and also to protect potentially proprietary intellectual property. In this paper, we describe an approach based on statistical machine translation (SMT) that recovers some of the original names from the JavaScript programs minified by the very popular UglifyJS. This simple tool, Autonym, performs comparably to the best currently available deobfuscator for JavaScript, JSNice, which uses sophisticated static analysis. In fact, Autonym is quite complementary to JSNice, performing well when it does not, and vice versa. We also introduce a new tool, JSNaughty, which blends Autonym and JSNice, and significantly outperforms both at identifier name recovery, while remaining just as easy to use as JSNice. JSNaughty is available online at http://jsnaughty.org.},
	urldate = {2021-02-19},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Vasilescu, Bogdan and Casalnuovo, Casey and Devanbu, Premkumar},
	month = aug,
	year = {2017},
	keywords = {Deobfuscation, JavaScript, Statistical Machine Translation},
	pages = {683--693},
}

@misc{noauthor_code_2016,
	title = {Code of {Ethics}},
	url = {https://ethics.acm.org/code-of-ethics/},
	abstract = {ACM Code of Ethics and Professional Conduct Adopted by ACM Council 6/22/18. Preamble Computing professionals’ actions change the world. To act responsibly, they should reflect upon the wider …},
	language = {en-US},
	urldate = {2021-02-20},
	journal = {ACM Ethics},
	month = jun,
	year = {2016},
}

@article{lipton_social_1979,
	title = {Social {Processes} and {Proofs} of {Theorems} and {Programs}},
	volume = {22},
	abstract = {It is argued that formal verifications of programs, no matter how obtained, will not play the same key role in the development of computer science and software engineering as proofs do in mathematics. Furthermore the absence of continuity, the inevitability of change, and the complexity of specification of significantly many real programs make the formal verification process difficult to justify and manage. It is felt that ease of formal verification should not dominate program language design.},
	language = {en},
	number = {5},
	author = {Lipton, Richard J and Perlis, Alan J},
	year = {1979},
	pages = {10},
}

@article{raychev_predicting_2015,
	title = {Predicting {Program} {Properties} from "{Big} {Code}"},
	volume = {50},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2775051.2677009},
	doi = {10.1145/2775051.2677009},
	abstract = {We present a new approach for predicting program properties from massive codebases (aka "Big Code"). Our approach first learns a probabilistic model from existing data and then uses this model to predict properties of new, unseen programs. The key idea of our work is to transform the input program into a representation which allows us to phrase the problem of inferring program properties as structured prediction in machine learning. This formulation enables us to leverage powerful probabilistic graphical models such as conditional random fields (CRFs) in order to perform joint prediction of program properties. As an example of our approach, we built a scalable prediction engine called JSNice for solving two kinds of problems in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNice predicts correct names for 63\% of name identifiers and its type annotation predictions are correct in 81\% of the cases. In the first week since its release, JSNice was used by more than 30,000 developers and in only few months has become a popular tool in the JavaScript developer community. By formulating the problem of inferring program properties as structured prediction and showing how to perform both learning and inference in this context, our work opens up new possibilities for attacking a wide range of difficult problems in the context of "Big Code" including invariant generation, decompilation, synthesis and others.},
	number = {1},
	urldate = {2021-02-20},
	journal = {ACM SIGPLAN Notices},
	author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
	month = jan,
	year = {2015},
	keywords = {big code, closure compiler, conditional random fields, javascript, names, program properties, structured prediction, types},
	pages = {111--124},
}

@article{mikolov_efficient_2013-1,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2021-02-18},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv: 1301.3781
version: 3},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{monperrus_detecting_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Detecting {Missing} {Method} {Calls} in {Object}-{Oriented} {Software}},
	isbn = {978-3-642-14107-2},
	doi = {10.1007/978-3-642-14107-2_2},
	abstract = {When using object-oriented frameworks it is easy to overlook certain important method calls that are required at particular places in code. In this paper, we provide a comprehensive set of empirical facts on this problem, starting from traces of missing method calls in a bug repository. We propose a new system, which automatically detects them during both software development and quality assurance phases. The evaluation shows that it has a low false positive rate ({\textless}5\%) and that it is able to find missing method calls in the source code of the Eclipse IDE.},
	language = {en},
	booktitle = {{ECOOP} 2010 – {Object}-{Oriented} {Programming}},
	publisher = {Springer},
	author = {Monperrus, Martin and Bruch, Marcel and Mezini, Mira},
	editor = {D’Hondt, Theo},
	year = {2010},
	keywords = {Deviant Code, Method Call, Parent Widget, Real Software, Syntactic Pattern},
	pages = {2--25},
}

@article{martin_are_2017,
	title = {Are {Psychology} {Journals} {Anti}-replication? {A} {Snapshot} of {Editorial} {Practices}},
	volume = {8},
	issn = {1664-1078},
	shorttitle = {Are {Psychology} {Journals} {Anti}-replication?},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00523/full},
	doi = {10.3389/fpsyg.2017.00523},
	abstract = {Recent research in psychology has highlighted a number of replication problems in the discipline, with publication bias – the preference for publishing original and positive results, and a resistance to publishing negative results and replications- identified as one reason for replication failure. However, little empirical research exists to demonstrate that journals explicitly refuse to publish replications. We reviewed the instructions to authors and the published aims of 1151 psychology journals and examined whether they indicated that replications were permitted and accepted. We also examined whether journal practices differed across branches of the discipline, and whether editorial practices differed between low and high impact journals. Thirty three journals (3\%) stated in their aims or instructions to authors that they accepted replications. There was no difference between high and low impact journals. The implications of these findings for psychology are discussed.},
	language = {English},
	urldate = {2021-02-18},
	journal = {Frontiers in Psychology},
	author = {Martin, G. N. and Clarke, Richard M.},
	year = {2017},
	note = {Publisher: Frontiers},
	keywords = {JOURNAL EDITORIAL PRACTICES, Psychology, Publication Bias, Replication, p-hacking},
}

@misc{noauthor_artifact_nodate,
	title = {Artifact {Review} and {Badging} – {Version} 1.0 (not current)},
	url = {https://www.acm.org/publications/policies/artifact-review-badging},
	abstract = {Result and Artifact Review and Badging},
	language = {en},
	urldate = {2021-02-17},
}

@article{baker_1500_2016,
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	url = {http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970},
	doi = {10.1038/533452a},
	abstract = {Survey sheds light on the ‘crisis’ rocking research.},
	language = {en},
	number = {7604},
	urldate = {2021-02-17},
	journal = {Nature News},
	author = {Baker, Monya},
	month = may,
	year = {2016},
	note = {Section: News Feature},
	pages = {452},
}

@article{sohn_deep_2021,
	title = {Deep belief network based intrusion detection techniques: {A} survey},
	volume = {167},
	issn = {09574174},
	shorttitle = {Deep belief network based intrusion detection techniques},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420309088},
	doi = {10.1016/j.eswa.2020.114170},
	abstract = {With the recent growth in the number of IoT devices, the amount of personal, sensitive, and important data flowing through the global network have grown rapidly. Additionally, the malicious attempt to access important information or damage the network have also become more complex and advanced. Thus, cybersecurity has become an important issue for the evolution toward future networks that can react and counter such threats. Intrusion detection is an important part of the cybersecurity technology with the goal of monitoring and analyzing network traffic from various resources and detect malicious activities. In recent years, deep learning base deep neural network (DNN) techniques have been utilized as the key solution to detect malicious attacks and among many DNNs, deep belief network (DBN) has been the most influential technique. There have been many attempts to survey wide range of machine learning and deep learning technique based intrusion detection research works, including DBN, but failed to provide a complete review of all the aspects related to the DBN based intrusion detection models. Unlike previous survey papers, we first provide basic concepts on data set, performance metric, and restricted Boltzmann machines, to help understand the basic DBN based intrusion detection model. Finally, a complete review and analysis on the previously published works on DBN based IDS models is provided.},
	language = {en},
	urldate = {2021-02-11},
	journal = {Expert Systems with Applications},
	author = {Sohn, Insoo},
	month = apr,
	year = {2021},
	pages = {114170},
}

@misc{noauthor_smt-lib-benchmarks_nodate,
	title = {{SMT}-{LIB}-benchmarks},
	url = {https://clc-gitlab.cs.uiowa.edu:2443/SMT-LIB-benchmarks},
	abstract = {Official SMT-LIB repository Non-incremental benchmarks},
	language = {en},
	urldate = {2021-02-11},
	journal = {GitLab},
}

@book{cohen_best_nodate,
	title = {Best {Kept} {Secrets} of {Peer} {Code} {Review}},
	language = {en},
	author = {Cohen, Jason and Teleki, Steven and Brown, Eric and DuRette, Brandon and Brown, Steven and Fuller, Brandon},
}

@article{jones_short_nodate,
	title = {A {Short} {History} of the {Cost} {Per} {Defect} {Metric}},
	abstract = {The oldest metric for software quality economic study is that of “cost per defect.” While there may be earlier uses, the metric was certainly used within IBM by the late 1960’s for software; and probably as early as 1950’s for hardware.},
	language = {en},
	author = {Jones, Capers},
	pages = {24},
}

@inproceedings{dorin_using_2020,
	title = {Using {Machine} {Learning} {Image} {Recognition} for {Code} {Reviews}},
	url = {https://aircconline.com/csit/papers/vol10/csit101514.pdf},
	doi = {10.5121/csit.2020.101514},
	abstract = {It is commonly understood that code reviews are a cost-effective way of finding faults early in the development cycle. However, many modern software developers are too busy to do them. Skipping code reviews means a loss of opportunity to detect expensive faults prior to software release. Software engineers can be pushed in many directions and reviewing code is very often considered an undesirable task, especially when time is wasted reviewing programs that are not ready. In this study, we wish to ascertain the potential for using machine learning and image recognition to detect immature software source code prior to a review. We show that it is possible to use machine learning to detect software problems visually and allow code reviews to focus on application details. The results are promising and are an indication that further research could be valuable.},
	language = {en},
	urldate = {2021-02-10},
	booktitle = {Computer {Science} \& {Information} {Technology} ({CS} \& {IT})},
	publisher = {AIRCC Publishing Corporation},
	author = {Dorin, Michael and Le, Trang and Kolakaluri, Rajkumar and Montenegro, Sergio},
	month = nov,
	year = {2020},
	pages = {159--167},
}

@book{winters_software_2020,
	edition = {1st edition},
	title = {Software {Engineering} at {Google}: {Lessons} {Learned} from {Programming} {Over} {Time}},
	isbn = {978-1-4920-8279-8},
	shorttitle = {Software {Engineering} at {Google}},
	abstract = {Today, software engineers need to know not only how to program effectively but also how to develop proper engineering practices to make their codebase sustainable and healthy. This book emphasizes this difference between programming and software engineering.How can software engineers manage a living codebase that evolves and responds to changing requirements and demands over the length of its life? Based on their experience at Google, software engineers Titus Winters and Hyrum Wright, along with technical writer Tom Manshreck, present a candid and insightful look at how some of the world’s leading practitioners construct and maintain software. This book covers Google’s unique engineering culture, processes, and tools and how these aspects contribute to the effectiveness of an engineering organization.You’ll explore three fundamental principles that software organizations should keep in mind when designing, architecting, writing, and maintaining code:How time affects the sustainability of software and how to make your code resilient over timeHow scale affects the viability of software practices within an engineering organizationWhat trade-offs a typical engineer needs to make when evaluating design and development decisions},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Winters, Titus and Manshreck, Tom and Wright, Hyrum},
	month = mar,
	year = {2020},
}

@article{zimmermann_mining_2005,
	title = {Mining version histories to guide software changes},
	volume = {31},
	issn = {0098-5589},
	url = {http://ieeexplore.ieee.org/document/1463228/},
	doi = {10.1109/TSE.2005.72},
	abstract = {We apply data mining to version histories in order to guide programmers along related changes: “Programmers who changed these functions also changed....” Given a set of existing changes, the mined association rules 1) suggest and predict likely further changes, 2) show up item coupling that is undetectable by program analysis, and 3) can prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict further locations to be changed; the best predictive power is obtained for changes to existing software. In our evaluation based on the history of eight popular open source projects, ROSE’s topmost three suggestions contained a correct location with a likelihood of more than 70 percent.},
	language = {en},
	number = {6},
	urldate = {2021-02-10},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zimmermann, T. and Zeller, A. and Weissgerber, P. and Diehl, S.},
	month = jun,
	year = {2005},
	pages = {429--445},
}

@article{flanagan_extended_nodate,
	title = {Extended {Static} {Checking} for {Java}},
	abstract = {Software development and maintenance are costly endeavors. The cost can be reduced if more software defects are detected earlier in the development cycle. This paper introduces the Extended Static Checker for Java (ESC/Java), an experimental compile-time program checker that ﬁnds common programming errors. The checker is powered by veriﬁcation-condition generation and automatic theoremproving techniques. It provides programmers with a simple annotation language with which programmer design decisions can be expressed formally. ESC/Java examines the annotated software and warns of inconsistencies between the design decisions recorded in the annotations and the actual code, and also warns of potential runtime errors in the code. This paper gives an overview of the checker architecture and annotation language and describes our experience applying the checker to tens of thousands of lines of Java programs.},
	language = {en},
	author = {Flanagan, Cormac and Nelson, Greg and Leino, K Rustan M and Saxe, James B and Lillibridge, Mark and Stata, Raymie},
	pages = {12},
}

@article{gupta_intelligent_2018,
	title = {Intelligent code reviews using deep learning},
	abstract = {Peer code review is a best practice in Software Engineering where source code is reviewed manually by one or more peers(reviewers) of the code author. It is widely acceptable both in industry and open-source software (OSS) systems as a process for early detection and reduction of software defects. A larger chunk of reviews given during peer reviews are related to common issues such as coding style, documentations, and best practices. This makes the code review process less effective as reviewers focus less on finding important defects. Hence, there is a need to automatically find such common issues and help reviewers perform focused code reviews. Some of this is solved by rule based systems called linters but they are rigid and needs a lot of manual effort to adapt them for a new issue.},
	language = {en},
	author = {Gupta, Anshul and Sundaresan, Neel},
	year = {2018},
	pages = {9},
}

@inproceedings{chatley_diggit_2018,
	address = {Campobasso},
	title = {Diggit: {Automated} code review via software repository mining},
	isbn = {978-1-5386-4969-5},
	shorttitle = {Diggit},
	url = {http://ieeexplore.ieee.org/document/8330261/},
	doi = {10.1109/SANER.2018.8330261},
	abstract = {We present Diggit, a tool to automatically generate code review comments, offering design guidance on prospective changes, based on insights gained from mining historical changes in source code repositories. We describe how the tool was built and tuned for use in practice as we integrated Diggit into the working processes of an industrial development team. We focus on the developer experience, the constraints that had to be met in adapting academic research to produce a tool that was useful to developers, and the effectiveness of the results in practice.},
	language = {en},
	urldate = {2021-02-10},
	booktitle = {2018 {IEEE} 25th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	publisher = {IEEE},
	author = {Chatley, Robert and Jones, Lawrence},
	month = mar,
	year = {2018},
	pages = {567--571},
}

@article{rice_detecting_2017,
	title = {Detecting argument selection defects},
	volume = {1},
	issn = {2475-1421, 2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3133928},
	doi = {10.1145/3133928},
	language = {en},
	number = {OOPSLA},
	urldate = {2021-02-10},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Rice, Andrew and Aftandilian, Edward and Jaspan, Ciera and Johnston, Emily and Pradel, Michael and Arroyo-Paredes, Yulissa},
	month = oct,
	year = {2017},
	pages = {1--22},
}

@article{axelsson_detecting_nodate,
	title = {Detecting {Defects} with an {Interactive} {Code} {Review} {Tool} {Based} on {Visualisation} and {Machine} {Learning}},
	abstract = {Code review is often suggested as a means of improving code quality. Since humans are poor at repetitive tasks, some form of tool support is valuable. To that end we developed a prototype tool to illustrate the novel idea of applying machine learning (based on Normalised Compression Distance) to the problem of static analysis of source code. Since this tool learns by example, it is trivially programmer adaptable. As machine learning algorithms are notoriously difﬁcult to understand operationally (they are opaque) we applied information visualisation to the results of the learner. In order to validate the approach we applied the prototype to source code from the open-source project Samba and from an industrial, telecom software system. Our results showed that the tool did indeed correctly ﬁnd and classify problematic sections of code based on training examples.},
	language = {en},
	author = {Axelsson, Stefan and Baca, Dejan and Feldt, Robert and Sidlauskas, Darius and Kacan, Denis},
	pages = {6},
}

@article{pradel_deepbugs_2018,
	title = {{DeepBugs}: a learning approach to name-based bug detection},
	volume = {2},
	issn = {2475-1421, 2475-1421},
	shorttitle = {{DeepBugs}},
	url = {https://dl.acm.org/doi/10.1145/3276517},
	doi = {10.1145/3276517},
	language = {en},
	number = {OOPSLA},
	urldate = {2021-02-10},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Pradel, Michael and Sen, Koushik},
	month = oct,
	year = {2018},
	pages = {1--25},
}

@incollection{yang_deepreview_2019,
	address = {Cham},
	title = {{DeepReview}: {Automatic} {Code} {Review} {Using} {Deep} {Multi}-instance {Learning}},
	volume = {11440},
	isbn = {978-3-030-16144-6 978-3-030-16145-3},
	shorttitle = {{DeepReview}},
	url = {http://link.springer.com/10.1007/978-3-030-16145-3_25},
	abstract = {Code review, an inspection of code changes in order to identify and ﬁx defects before integration, is essential in Software Quality Assurance (SQA). Code review is a time-consuming task since the reviewers need to understand, analysis and provide comments manually. To alleviate the burden of reviewers, automatic code review is needed. However, this task has not been well studied before. To bridge this research gap, in this paper, we formalize automatic code review as a multi-instance learning task that each change consisting of multiple hunks is regarded as a bag, and each hunk is described as an instance. We propose a novel deep learning model named DeepReview based on Convolutional Neural Network (CNN), which is an end-to-end model that learns feature representation to predict whether one change is approved or rejected. Experimental results on open source projects show that DeepReview is eﬀective in automatic code review tasks. In terms of F1 score and AUC, DeepReview outperforms the performance of traditional single-instance based model TFIDF-SVM and the state-of-the-art deep feature based model Deeper.},
	language = {en},
	urldate = {2021-02-10},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer International Publishing},
	author = {Li, Heng-Yi and Shi, Shu-Ting and Thung, Ferdian and Huo, Xuan and Xu, Bowen and Li, Ming and Lo, David},
	editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
	year = {2019},
	doi = {10.1007/978-3-030-16145-3_25},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {318--330},
}

@inproceedings{lal_code_2017,
	address = {Coimbatore, India},
	title = {Code review analysis of software system using machine learning techniques},
	isbn = {978-1-5090-2717-0},
	url = {http://ieeexplore.ieee.org/document/7855962/},
	doi = {10.1109/ISCO.2017.7855962},
	language = {en},
	urldate = {2021-02-10},
	booktitle = {2017 11th {International} {Conference} on {Intelligent} {Systems} and {Control} ({ISCO})},
	publisher = {IEEE},
	author = {Lal, Harsh and Pahwa, Gaurav},
	month = jan,
	year = {2017},
	pages = {8--13},
}

@article{ferenc_deep_2020,
	title = {Deep learning in static, metric-based bug prediction},
	volume = {6},
	issn = {25900056},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2590005620300060},
	doi = {10.1016/j.array.2020.100021},
	abstract = {Our increasing reliance on software products and the amount of money we spend on creating and maintaining them makes it crucial to ﬁnd bugs as early and as easily as possible. At the same time, it is not enough to know that we should be paying more attention to bugs; ﬁnding them must become a quick and seamless process in order to be actually used by developers. Our proposal is to revitalize static source code metrics – among the most easily calculable, while still meaningful predictors – and combine them with deep learning – among the most promising and generalizable prediction techniques – to ﬂag suspicious code segments at the class level. In this paper, we show a detailed methodology of how we adapted deep neural networks to bug prediction, applied them to a large bug dataset (containing 8780 bugged and 38,838 not bugged Java classes), and compared them to multiple “traditional” algorithms. We demonstrate that deep learning with static metrics can indeed boost prediction accuracies. Our best model has an F-measure of 53.59\%, which increases to 55.27\% for the best ensemble model containing a deep learning component. Additionally, another experiment suggests that these values could improve even further with more data points. We also open-source our experimental Python framework to help other researchers replicate our ﬁndings.},
	language = {en},
	urldate = {2021-02-10},
	journal = {Array},
	author = {Ferenc, Rudolf and Bán, Dénes and Grósz, Tamás and Gyimóthy, Tibor},
	month = jul,
	year = {2020},
	pages = {100021},
}

@inproceedings{ayinala_code_2020,
	address = {Madrid, Spain},
	title = {Code {Inspection} {Support} for {Recurring} {Changes} with {Deep} {Learning} in {Evolving} {Software}},
	isbn = {978-1-72817-303-0},
	url = {https://ieeexplore.ieee.org/document/9202813/},
	doi = {10.1109/COMPSAC48688.2020.0-149},
	abstract = {Developers often make recurring changes, similar but different changes across multiple locations. They inspect such code changes per source ﬁle (i.e., a diff patch) during code reviews; however, diff patches represent low-level code modiﬁcation without summarizing recurring changes, leading to tedious and error-prone code inspection. To address this problem, we propose a novel code review approach, Recurring Code Changes Inspection with Deep Learning (RIDL) that leverages change patterns of an edit script by learning code clones, identical or nearly similar code fragments. To train a classiﬁer, RIDL learns 13,940 clones with four different clone types (e.g., Type-1, Type2, Type-3, and Type-4 clones) from a clone database mined from 25,000 subject programs. Our approach then leverages the classiﬁer to (1) interactively summarize recurring changes and (2) detect change mistakes, potential anomalies in a given codebase. In the evaluation, after 2 hours of training, RIDL analyzes code changes in four open source projects. It summarizes recurring changes with 95.1\% accuracy and detects change anomalies with 93.1\% accuracy. Our results show that RIDL should help developers effectively inspect recurring changes during code reviews.},
	language = {en},
	urldate = {2021-02-10},
	booktitle = {2020 {IEEE} 44th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	publisher = {IEEE},
	author = {Ayinala, Krishna Teja and Cheng, Kwok Sun and Oh, Kwangsung and Song, Teukseob and Song, Myoungkyu},
	month = jul,
	year = {2020},
	pages = {931--942},
}

@inproceedings{mcgrath_bifrost_2017,
	address = {Québec City QC Canada},
	title = {Bifröst: {Visualizing} and {Checking} {Behavior} of {Embedded} {Systems} across {Hardware} and {Software}},
	isbn = {978-1-4503-4981-9},
	shorttitle = {Bifröst},
	url = {https://dl.acm.org/doi/10.1145/3126594.3126658},
	doi = {10.1145/3126594.3126658},
	abstract = {The Maker movement has encouraged more people to start working with electronics and embedded processors. A key challenge in developing and debugging custom embedded systems is understanding their behavior, particularly at the boundary between hardware and software. Existing tools such as step debuggers and logic analyzers only focus on software or hardware, respectively. This paper presents a new development environment designed to illuminate the boundary between embedded code and circuits. Bifröst automatically instruments and captures the progress of the user’s code, variable values, and the electrical and bus activity occurring at the interface between the processor and the circuit it operates in. This data is displayed in a linked visualization that allows navigation through time and program execution, enabling comparisons between variables in code and signals in circuits. Automatic checks can detect low-level hardware conﬁguration and protocol issues, while user-authored checks can test particular application semantics. In an exploratory study with ten participants, we investigated how Bifröst inﬂuences debugging workﬂows.},
	language = {en},
	urldate = {2021-02-10},
	booktitle = {Proceedings of the 30th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {McGrath, Will and Drew, Daniel and Warner, Jeremy and Kazemitabaar, Majeed and Karchemsky, Mitchell and Mellis, David and Hartmann, Björn},
	month = oct,
	year = {2017},
	pages = {299--310},
}

@inproceedings{beyer_animated_2006,
	address = {Benevento},
	title = {Animated {Visualization} of {Software} {History} using {Evolution} {Storyboards}},
	isbn = {978-0-7695-2719-2},
	url = {https://ieeexplore.ieee.org/document/4023990/},
	doi = {10.1109/WCRE.2006.14},
	abstract = {The understanding of the structure of a software system can be improved by analyzing the system’s evolution during development. Visualizations of software history that provide only static views do not capture the dynamic nature of software evolution. We present a new visualization technique, the Evolution Storyboard, which provides dynamic views of the evolution of a software’s structure. An evolution storyboard consists of a sequence of animated panels, which highlight the structural changes in the system; one panel for each considered time period. Using storyboards, engineers can spot good design, signs of structural decay, or the spread of cross cutting concerns in the code. We implemented our concepts in a tool, which automatically extracts software dependency graphs from version control repositories and computes storyboards based on panels for different time periods. For applying our approach in practice, we provide a step by step guide that others can follow along the storyboard visualizations, in order to study the evolution of large systems. We have applied our method to several large open source software systems. In this paper, we demonstrate that our method provides additional information (compared to static views) on the ArgoUML project, an open source UML modeling tool.},
	language = {en},
	urldate = {2021-02-10},
	booktitle = {2006 13th {Working} {Conference} on {Reverse} {Engineering}},
	publisher = {IEEE},
	author = {Beyer, Dirk and Hassan, Ahmed E.},
	month = oct,
	year = {2006},
	pages = {199--210},
}

@inproceedings{xiaomeng_survey_2018,
	address = {Shanghai, China},
	title = {A {Survey} on {Source} {Code} {Review} {Using} {Machine} {Learning}},
	isbn = {978-1-5386-6259-5},
	url = {https://ieeexplore.ieee.org/document/8614720/},
	doi = {10.1109/ICISE.2018.00018},
	abstract = {Source code review constrains software system security sufficiently. Scalability and precision are of importance for the deployment of code review tools. However, traditional tools can only detect some security flaws automatically with high false positive and false negative by tedious reviewing largescale source code. Various flaws and vulnerabilities show specific characteristic in source code. Machine learning systems founded feature matrixes of source code as input, including variables, functions and files, generating ad-hoc label by distinguish or generation methodologies to review source code automatically and intelligently. Source code, whatever the programming language, is text information in nature. Both secure and vulnerable feature can be curved from source code. Fortunately, a variety of machine learning approaches have been developed to learn and detect flaws and vulnerabilities in intelligent source code security review. Combination of code semantic and syntactic feature contribute to the optimation of false positive and false negative during source code review. In this paper, we give the review of literature related to intelligent source code security review using machine learning methods. It illustrate the primary evidence of approaching ML in source code security review. We believe machine learning and its branches will become out-standing in source code review.},
	language = {en},
	urldate = {2021-02-10},
	booktitle = {2018 3rd {International} {Conference} on {Information} {Systems} {Engineering} ({ICISE})},
	publisher = {IEEE},
	author = {Xiaomeng, Wang and Tao, Zhang and Wei, Xin and Changyu, Hou},
	month = may,
	year = {2018},
	pages = {56--60},
}

@inproceedings{madera_case_2017,
	title = {A case study on machine learning model for code review expert system in software engineering},
	url = {https://fedcsis.org/proceedings/2017/drp/536.html},
	doi = {10.15439/2017F536},
	abstract = {Code review is a key tool for quality assurance in software development. It is intended to find coding mistakes overlooked during development phase and lower risk of bugs in final product. In large and complex projects accurate code review is a challenging task. As code review depends on individual reviewer predisposition there is certain margin of source code changes that is not checked as it should. In this paper we propose machine learning approach for pointing project artifacts that are significantly at risk of failure. Planning and adjusting quality assurance (QA) activities could strongly benefit from accurate estimation of software areas endangered by defects. Extended code review could be directed there. The proposed approach has been evaluated for feasibility on large medical software project. Significant work was done to extract features from heterogeneous production data, leading to good predictive model. Our preliminary research results were considered worthy of implementation in the company where the research has been conducted, thus opening the opportunities for the continuation of the studies.},
	language = {en},
	urldate = {2021-02-10},
	author = {Madera, Michał and Tomoń, Rafał},
	month = sep,
	year = {2017},
	pages = {1357--1363},
}
