
@article{rice_detecting_2017,
	title = {Detecting argument selection defects},
	volume = {1},
	url = {https://doi.org/10.1145/3133928},
	doi = {10.1145/3133928},
	abstract = {Identifier names are often used by developers to convey additional information about the meaning of a program over and above the semantics of the programming language itself. We present an algorithm that uses this information to detect argument selection defects, in which the programmer has chosen the wrong argument to a method call in Java programs. We evaluate our algorithm at Google on 200 million lines of internal code and 10 million lines of predominantly open-source external code and find defects even in large, mature projects such as OpenJDK, ASM, and the MySQL JDBC. The precision and recall of the algorithm vary depending on a sensitivity threshold. Higher thresholds increase precision, giving a true positive rate of 85\%, reporting 459 true positives and 78 false positives. Lower thresholds increase recall but lower the true positive rate, reporting 2,060 true positives and 1,207 false positives. We show that this is an order of magnitude improvement on previous approaches. By analyzing the defects found, we are able to quantify best practice advice for API design and show that the probability of an argument selection defect increases markedly when methods have more than five arguments.},
	number = {OOPSLA},
	urldate = {2021-02-09},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Rice, Andrew and Aftandilian, Edward and Jaspan, Ciera and Johnston, Emily and Pradel, Michael and Arroyo-Paredes, Yulissa},
	month = oct,
	year = {2017},
	keywords = {empirical study, method arguments, name-based program analysis, static analysis},
	pages = {104:1--104:22},
}

@article{pradel_deepbugs_2018,
	title = {{DeepBugs}: a learning approach to name-based bug detection},
	volume = {2},
	shorttitle = {{DeepBugs}},
	url = {https://doi.org/10.1145/3276517},
	doi = {10.1145/3276517},
	abstract = {Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents DeepBugs, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 JavaScript files yields bug detectors that have a high accuracy (between 89\% and 95\%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68\% true positive rate) in real-world code.},
	number = {OOPSLA},
	urldate = {2021-01-25},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Pradel, Michael and Sen, Koushik},
	month = oct,
	year = {2018},
	keywords = {Bug detection, JavaScript, Machine learning, Name-based program analysis, Natural language},
	pages = {147:1--147:25},
}

@inproceedings{zhang_towards_2019,
	title = {Towards {Adversarially} {Robust} {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Towards_Adversarially_Robust_Object_Detection_ICCV_2019_paper.html},
	urldate = {2021-01-19},
	author = {Zhang, Haichao and Wang, Jianyu},
	year = {2019},
	pages = {421--430},
}

@article{yin_defense_2020,
	title = {Defense against adversarial attacks by low-level image transformations},
	volume = {35},
	copyright = {© 2020 Wiley Periodicals LLC},
	issn = {1098-111X},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/int.22258},
	doi = {https://doi.org/10.1002/int.22258},
	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples, which can fool classifiers by maliciously adding imperceptible perturbations to the original input. Currently, a large number of research on defending adversarial examples pay little attention to the real-world applications, either with high computational complexity or poor defensive effects. Motivated by this observation, we develop an efficient preprocessing module to defend adversarial attacks. Specifically, before an adversarial example is fed into the model, we perform two low-level image transformations, WebP compression and flip operation, on the picture. Then we can get a de-perturbed sample that can be correctly classified by DNNs. WebP compression is utilized to remove the small adversarial noises. Due to the introduction of loop filtering, there will be no square effect like JPEG compression, so the visual quality of the denoised image is higher. And flip operation, which flips the image once along one side of the image, destroys the specific structure of adversarial perturbations. By taking class activation mapping to localize the discriminative image regions, we show that flipping image may mitigate adversarial effects. Extensive experiments demonstrate that the proposed scheme outperforms the state-of-the-art defense methods. It can effectively defend adversarial attacks while ensuring only slight accuracy drops on normal images.},
	language = {en},
	number = {10},
	urldate = {2021-01-19},
	journal = {International Journal of Intelligent Systems},
	author = {Yin, Zhaoxia and Wang, Hua and Wang, Jie and Tang, Jin and Wang, Wenzhong},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.22258},
	keywords = {WebP compression, adversarial examples, deep neural networks, flip operation, image transformations},
	pages = {1453--1466},
}

@inproceedings{tan_mnasnet_2019,
	title = {{MnasNet}: {Platform}-{Aware} {Neural} {Architecture} {Search} for {Mobile}},
	shorttitle = {{MnasNet}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Tan_MnasNet_Platform-Aware_Neural_Architecture_Search_for_Mobile_CVPR_2019_paper.html},
	urldate = {2021-01-11},
	author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
	year = {2019},
	pages = {2820--2828},
}

@inproceedings{wu_fbnet_2019,
	title = {{FBNet}: {Hardware}-{Aware} {Efficient} {ConvNet} {Design} via {Differentiable} {Neural} {Architecture} {Search}},
	shorttitle = {{FBNet}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_FBNet_Hardware-Aware_Efficient_ConvNet_Design_via_Differentiable_Neural_Architecture_Search_CVPR_2019_paper.html},
	urldate = {2021-01-11},
	author = {Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing and Keutzer, Kurt},
	year = {2019},
	pages = {10734--10742},
}

@article{xu_pc-darts_2020,
	title = {{PC}-{DARTS}: {Partial} {Channel} {Connections} for {Memory}-{Efficient} {Architecture} {Search}},
	shorttitle = {{PC}-{DARTS}},
	url = {http://arxiv.org/abs/1907.05737},
	abstract = {Differentiable architecture search (DARTS) provided a fast solution in finding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-network and searching for an optimal architecture. In this paper, we present a novel approach, namely, Partially-Connected DARTS, by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, we perform operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of super-net caused by sampling different channels. We alleviate it using edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search. Thanks to the reduced memory cost, PC-DARTS can be trained with a larger batch size and, consequently, enjoys both faster speed and higher training stability. Experimental results demonstrate the effectiveness of the proposed method. Specifically, we achieve an error rate of 2.57\% on CIFAR10 with merely 0.1 GPU-days for architecture search, and a state-of-the-art top-1 error rate of 24.2\% on ImageNet (under the mobile setting) using 3.8 GPU-days for search. Our code has been made available at: https://github.com/yuhuixu1993/PC-DARTS.},
	urldate = {2021-01-11},
	journal = {arXiv:1907.05737 [cs]},
	author = {Xu, Yuhui and Xie, Lingxi and Zhang, Xiaopeng and Chen, Xin and Qi, Guo-Jun and Tian, Qi and Xiong, Hongkai},
	month = apr,
	year = {2020},
	note = {arXiv: 1907.05737},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{liu_darts_2019,
	title = {{DARTS}: {Differentiable} {Architecture} {Search}},
	shorttitle = {{DARTS}},
	url = {http://arxiv.org/abs/1806.09055},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
	urldate = {2021-01-11},
	journal = {arXiv:1806.09055 [cs, stat]},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	month = apr,
	year = {2019},
	note = {arXiv: 1806.09055},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zoph_learning_2018,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html},
	urldate = {2021-01-11},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	year = {2018},
	pages = {8697--8710},
}

@article{real_regularized_2019,
	title = {Regularized {Evolution} for {Image} {Classifier} {Architecture} {Search}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4405},
	doi = {10.1609/aaai.v33i01.33014780},
	language = {en},
	number = {01},
	urldate = {2021-01-11},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {4780--4789},
}

@article{zoph_neural_2017,
	title = {Neural {Architecture} {Search} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.01578},
	abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
	urldate = {2021-01-11},
	journal = {arXiv:1611.01578 [cs]},
	author = {Zoph, Barret and Le, Quoc V.},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.01578},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{li_neural_2020,
	title = {Neural {Architecture} {Search} for {Lightweight} {Non}-{Local} {Networks}},
	abstract = {Non-Local (NL) blocks have been widely studied in various vision tasks. However, it has been rarely explored to embed the NL blocks in mobile neural networks, mainly due to the following challenges: 1) NL blocks generally have heavy computation cost which makes it difficult to be applied in applications where computational resources are limited, and 2) it is an open problem to discover an optimal configuration to embed NL blocks into mobile neural networks. We propose AutoNL to overcome the above two obstacles. Firstly, we propose a Lightweight Non-Local (LightNL) block by squeezing the transformation operations and incorporating compact features. With the novel design choices, the proposed LightNL block is 400 times computationally cheaper},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Li, Y. and Jin, X. and Mei, J. and Lian, X. and Yang, L. and Xie, C. and Yu, Q. and Zhou, Y. and Bai, S. and Yuille, A. L.},
	year = {2020},
	pages = {10294--10303},
}

@inproceedings{chen_renas_2019,
	title = {{RENAS}: {Reinforced} {Evolutionary} {Neural} {Architecture} {Search}},
	shorttitle = {{RENAS}},
	doi = {10.1109/CVPR.2019.00492},
	abstract = {Neural Architecture Search (NAS) is an important yet challenging task in network design due to its high computational consumption. To address this issue, we propose the Reinforced Evolutionary Neural Architecture Search (RENAS), which is an evolutionary method with reinforced mutation for NAS. Our method integrates reinforced mutation into an evolution algorithm for neural architecture exploration, in which a mutation controller is introduced to learn the effects of slight modifications and make mutation actions. The reinforced mutation controller guides the model population to evolve efficiently. Furthermore, as child models can inherit parameters from their parents during evolution, our method requires very limited computational resources. In experiments, we conduct the proposed search method on CIFAR-10 and obtain a powerful network architecture, RENASNet. This architecture achieves a competitive result on CIFAR-10. The explored network architecture is transferable to ImageNet and achieves a new state-of-the-art accuracy, i.e., 75.7\% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test its performance on semantic segmentation with DeepLabv3 on the PASCAL VOC. RENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83\% mIOU without being pretrained on COCO.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Chen, Y. and Meng, G. and Zhang, Q. and Xiang, S. and Huang, C. and Mu, L. and Wang, X.},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Categorization, Deep Learning, ImageNet, RENAS, Recognition: Detection, Reinforced Evolutionary Neural Architecture Search, Retrieval, evolutionary computation, image segmentation, learning (artificial intelligence), mutation actions, mutation controller, neural architecture exploration, neural nets, search problems, semantic segmentation},
	pages = {4782--4791},
}

@inproceedings{hoiem_putting_2006,
	title = {Putting {Objects} in {Perspective}},
	volume = {2},
	doi = {10.1109/CVPR.2006.232},
	abstract = {Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach.},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'06)},
	author = {Hoiem, D. and Efros, A. A. and Hebert, M.},
	month = jun,
	year = {2006},
	note = {ISSN: 1063-6919},
	keywords = {Cameras, Computer vision, Context modeling, Detectors, Geometry, Layout, Object detection, Roads, Robot kinematics, Solid modeling},
	pages = {2137--2144},
}

@article{zhou_neurvps_2019,
	title = {{NeurVPS}: {Neural} {Vanishing} {Point} {Scanning} via {Conic} {Convolution}},
	volume = {32},
	shorttitle = {{NeurVPS}},
	url = {https://papers.nips.cc/paper/2019/hash/8e6b42f1644ecb1327dc03ab345e618b-Abstract.html},
	language = {en},
	urldate = {2021-01-06},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhou, Yichao and Qi, Haozhi and Huang, Jingwei and Ma, Yi},
	year = {2019},
	pages = {866--875},
}

@article{chen_binarized_2020,
	title = {Binarized {Neural} {Architecture} {Search} for {Efficient} {Object} {Recognition}},
	url = {http://arxiv.org/abs/2009.04247},
	abstract = {Traditional neural architecture search (NAS) has a significant impact in computer vision by automatically designing network architectures for various tasks. In this paper, binarized neural architecture search (BNAS), with a search space of binarized convolutions, is introduced to produce extremely compressed models to reduce huge computational cost on embedded devices for edge computing. The BNAS calculation is more challenging than NAS due to the learning inefficiency caused by optimization requirements and the huge architecture space, and the performance loss when handling the wild data in various computing applications. To address these issues, we introduce operation space reduction and channel sampling into BNAS to significantly reduce the cost of searching. This is accomplished through a performance-based strategy that is robust to wild data, which is further used to abandon less potential operations. Furthermore, we introduce the Upper Confidence Bound (UCB) to solve 1-bit BNAS. Two optimization methods for binarized neural networks are used to validate the effectiveness of our BNAS. Extensive experiments demonstrate that the proposed BNAS achieves a comparable performance to NAS on both CIFAR and ImageNet databases. An accuracy of \$96.53{\textbackslash}\%\$ vs. \$97.22{\textbackslash}\%\$ is achieved on the CIFAR-10 dataset, but with a significantly compressed model, and a \$40{\textbackslash}\%\$ faster search than the state-of-the-art PC-DARTS. On the wild face recognition task, our binarized models achieve a performance similar to their corresponding full-precision models.},
	urldate = {2021-01-05},
	journal = {arXiv:2009.04247 [cs]},
	author = {Chen, Hanlin and Zhuo, Li'an and Zhang, Baochang and Zheng, Xiawu and Liu, Jianzhuang and Ji, Rongrong and Doermann, David and Guo, Guodong},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.04247},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{loni_neuropower_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{NeuroPower}: {Designing} {Energy} {Efficient} {Convolutional} {Neural} {Network} {Architecture} for {Embedded} {Systems}},
	isbn = {978-3-030-30487-4},
	shorttitle = {{NeuroPower}},
	doi = {10.1007/978-3-030-30487-4_17},
	abstract = {Convolutional Neural Networks (CNNs) suffer from energy-hungry implementation due to their computation and memory intensive processing patterns. This problem is even more significant by the proliferation of CNNs on embedded platforms. To overcome this problem, we offer NeuroPower as an automatic framework that designs a highly optimized and energy efficient set of CNN architectures for embedded systems. NeuroPower explores and prunes the design space to find improved set of neural architectures. Toward this aim, a multi-objective optimization strategy is integrated to solve Neural Architecture Search (NAS) problem by near-optimal tuning network hyperparameters. The main objectives of the optimization algorithm are network accuracy and number of parameters in the network. The evaluation results show the effectiveness of NeuroPower on energy consumption, compacting rate and inference time compared to other cutting-edge approaches. In comparison with the best results on CIFAR-10/CIFAR-100 datasets, a generated network by NeuroPower presents up to 2.1x/1.56x compression rate, 1.59x/3.46x speedup and 1.52x/1.82x power saving while loses 2.4\%/−0.6\% accuracy, respectively.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2019: {Theoretical} {Neural} {Computation}},
	publisher = {Springer International Publishing},
	author = {Loni, Mohammad and Zoljodi, Ali and Sinaei, Sima and Daneshtalab, Masoud and Sjödin, Mikael},
	editor = {Tetko, Igor V. and Kůrková, Věra and Karpov, Pavel and Theis, Fabian},
	year = {2019},
	keywords = {Convolutional Neural Networks (CNNs), Embedded systems, Multi-objective optimization, Neural Architecture Search (NAS)},
	pages = {208--222},
}

@inproceedings{li_partial_2019,
	title = {Partial {Order} {Pruning}: {For} {Best} {Speed}/{Accuracy} {Trade}-{Off} in {Neural} {Architecture} {Search}},
	shorttitle = {Partial {Order} {Pruning}},
	doi = {10.1109/CVPR.2019.00936},
	abstract = {Achieving good speed and accuracy trade-off on a target platform is very important in deploying deep neural networks in real world scenarios. However, most existing automatic architecture search approaches only concentrate on high performance. In this work, we propose an algorithm that can offer better speed/accuracy trade-off of searched networks, which is termed "Partial Order Pruning”. It prunes the architecture search space with a partial order assumption to automatically search for the architectures with the best speed and accuracy trade-off. Our algorithm explicitly takes profile information about the inference speed on the target platform into consideration. With the proposed algorithm, we present several Dongfeng (DF) networks that provide high accuracy and fast inference speed on various application GPU platforms. By further searching decoder architectures, our DF-Seg real-time segmentation networks yield state-of-the-art speed/accuracy trade-off on both the target embedded device and the high-end GPU.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Li, X. and Zhou, Y. and Pan, Z. and Feng, J.},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {DF-Seg real-time segmentation networks, Deep Learning, Dongfeng networks, Grouping and Shape, Segmentation, architecture search space, automatic architecture search, decoder architecture searching, decoding, deep neural networks, embedded systems, fast inference speed, good speed, graphics processing units, neural architecture search, neural net architecture, partial order assumption, partial order pruning, search problems, searched networks, target embedded device, target platform, world scenarios},
	pages = {9137--9145},
}

@article{baker_designing_2017,
	title = {Designing {Neural} {Network} {Architectures} using {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.02167},
	abstract = {At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using \$Q\$-learning with an \${\textbackslash}epsilon\$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.},
	urldate = {2021-01-05},
	journal = {arXiv:1611.02167 [cs]},
	author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
	month = mar,
	year = {2017},
	note = {arXiv: 1611.02167},
	keywords = {Computer Science - Machine Learning},
}

@article{ren_comprehensive_2020,
	title = {A {Comprehensive} {Survey} of {Neural} {Architecture} {Search}: {Challenges} and {Solutions}},
	shorttitle = {A {Comprehensive} {Survey} of {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/2006.02903},
	abstract = {Deep learning has made major breakthroughs and progress in many fields. This is due to the powerful automatic representation capabilities of deep learning. It has been proved that the design of the network architecture is crucial to the feature representation of data and the final performance. In order to obtain a good feature representation of data, the researchers designed various complex network architectures. However, the design of the network architecture relies heavily on the researchers' prior knowledge and experience. Therefore, a natural idea is to reduce human intervention as much as possible and let the algorithm automatically design the architecture of the network. Thus going further to the strong intelligence. In recent years, a large number of related algorithms for {\textbackslash}textit\{Neural Architecture Search\} (NAS) have emerged. They have made various improvements to the NAS algorithm, and the related research work is complicated and rich. In order to reduce the difficulty for beginners to conduct NAS-related research, a comprehensive and systematic survey on the NAS is essential. Previously related surveys began to classify existing work mainly from the basic components of NAS: search space, search strategy and evaluation strategy. This classification method is more intuitive, but it is difficult for readers to grasp the challenges and the landmark work in the middle. Therefore, in this survey, we provide a new perspective: starting with an overview of the characteristics of the earliest NAS algorithms, summarizing the problems in these early NAS algorithms, and then giving solutions for subsequent related research work. In addition, we conducted a detailed and comprehensive analysis, comparison and summary of these works. Finally, we give possible future research directions.},
	urldate = {2021-01-05},
	journal = {arXiv:2006.02903 [cs, stat]},
	author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.02903},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{anderson_low-memory_2017,
	title = {Low-memory {GEMM}-based convolution algorithms for deep neural networks},
	url = {http://arxiv.org/abs/1709.03395},
	abstract = {Deep neural networks (DNNs) require very large amounts of computation both for training and for inference when deployed in the field. A common approach to implementing DNNs is to recast the most computationally expensive operations as general matrix multiplication (GEMM). However, as we demonstrate in this paper, there are a great many different ways to express DNN convolution operations using GEMM. Although different approaches all perform the same number of operations, the size of temporary data structures differs significantly. Convolution of an input matrix with dimensions \$C {\textbackslash}times H {\textbackslash}times W\$, requires \$O(K{\textasciicircum}2CHW)\$ additional space using the classical im2col approach. More recently memory-efficient approaches requiring just \$O(KCHW)\$ auxiliary space have been proposed. We present two novel GEMM-based algorithms that require just \$O(MHW)\$ and \$O(KW)\$ additional space respectively, where \$M\$ is the number of channels in the result of the convolution. These algorithms dramatically reduce the space overhead of DNN convolution, making it much more suitable for memory-limited embedded systems. Experimental evaluation shows that our low-memory algorithms are just as fast as the best patch-building approaches despite requiring just a fraction of the amount of additional memory. Our low-memory algorithms have excellent data locality which gives them a further edge over patch-building algorithms when multiple cores are used. As a result, our low memory algorithms often outperform the best patch-building algorithms using multiple threads.},
	urldate = {2020-11-19},
	journal = {arXiv:1709.03395 [cs]},
	author = {Anderson, Andrew and Vasudevan, Aravind and Keane, Cormac and Gregg, David},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.03395},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wang_structured_2020,
	title = {Structured {Pruning} for {Efficient} {Convolutional} {Neural} {Networks} via {Incremental} {Regularization}},
	volume = {14},
	issn = {1941-0484},
	doi = {10.1109/JSTSP.2019.2961233},
	abstract = {Modern Convolutional Neural Networks (CNNs) are usually restricted by their massive computation and high storage. Parameter pruning is a promising approach for CNN compression and acceleration by eliminating redundant model parameters with tolerable performance degradation. Despite its effectiveness, existing regularization-based parameter pruning methods usually drive weights towards zero with large and constant regularization factors, which neglects the fragility of the expressiveness of CNNs, and thus calls for a more gentle regularization scheme so that the networks can adapt during pruning. To achieve this, we propose a novel regularization-based pruning method, named IncReg, to incrementally assign different regularization factors to different weights based on their relative importance. Empirical analysis on CIFAR-10 dataset verifies the merits of IncReg. Further extensive experiments with popular CNNs on CIFAR-10 and ImageNet datasets show that IncReg achieves comparable to even better results compared with state-of-the-arts. Moreover, to resolve the problem that column pruning cannot be directly applied to off-the-shelf deep learning libraries for acceleration, we generalize IncReg from column pruning to spatial pruning, which can equip existing structured pruning methods (such as channel pruning) for further acceleration with ignorable accuracy loss. Our source codes and trained models are available at: https://github.com/mingsun-tse/caffe\_increghttps://github.com/mingsun-tse/caffe\_increg.},
	number = {4},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Wang, H. and Hu, X. and Zhang, Q. and Wang, Y. and Yu, L. and Hu, H.},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Journal of Selected Topics in Signal Processing},
	keywords = {Acceleration, Biological neural networks, CIFAR-10 dataset, CNN compression, Computational modeling, Convolutional neural network, IncReg, Incremental regularization, Kernel, Matrix decomposition, Model compression, Structured pruning, Training, channel pruning, column pruning, convolutional neural nets, convolutional neural networks, deep learning, incremental regularization, learning (artificial intelligence), redundant model parameters, regularization-based parameter pruning, spatial pruning, structured pruning},
	pages = {775--788},
}

@inproceedings{gan_identifying_2019,
	title = {Identifying and {Pruning} {Redundant} {Structures} for {Deep} {Neural} {Networks}},
	doi = {10.1109/VCIP47243.2019.8966025},
	abstract = {Deep convolutional neural networks have achieved considerable success in the field of computer vision. However, it is difficult to deploy state-of-the-art models on resource-constrained platforms due to their high storage, memory bandwidth, and computational costs. In this paper, we propose a structured pruning method which employs a three-step process to reduce the resource consumption of neural networks. First, we train an initial network on the training set and evaluate it on the validation set. Next, we introduce an iterative pruning and fine-tuning algorithm to identify and prune redundant structures, which results in a pruned network with a compact architecture. Finally, we train the pruned network from scratch on both the training set and validation set to obtain the final accuracy on the test set. In the experiments, our pruning method significantly reduces the model size (by 87.2\% on CIFAR-10), saves inference time (53.3\% on CIFAR-10), and achieves better performance as compared to recent state-of-the-art methods.},
	booktitle = {2019 {IEEE} {Visual} {Communications} and {Image} {Processing} ({VCIP})},
	author = {Gan, W. and Song, L. and Chen, L. and Xie, R. and Gu, X.},
	month = dec,
	year = {2019},
	note = {ISSN: 2642-9357},
	keywords = {Computational modeling, Iterative algorithms, Network architecture, Redundancy, Sensitivity analysis, Shape, Training, computer vision, convolutional neural nets, deep convolutional neural networks, deep neural networks, fine-tuning algorithm, iterative methods, iterative pruning, memory bandwidth, pruned network, redundant structures, resource consumption, resource-constrained platforms, structured pruning method},
	pages = {1--4},
}

@article{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	urldate = {2020-11-02},
	journal = {arXiv:1704.04861 [cs]},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{real_regularized_2019-1,
	title = {Regularized {Evolution} for {Image} {Classifier} {Architecture} {Search}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/4405},
	doi = {10.1609/aaai.v33i01.33014780},
	abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— AmoebaNet-A—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9\% top-1 / 96.6\% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
	language = {en},
	number = {01},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {4780--4789},
}

@inproceedings{tan_mnasnet_2019-1,
	title = {{MnasNet}: {Platform}-{Aware} {Neural} {Architecture} {Search} for {Mobile}},
	shorttitle = {{MnasNet}},
	doi = {10.1109/CVPR.2019.00293},
	abstract = {Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2\% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8× faster than MobileNetV2 with 0.5\% higher accuracy and 2.3× faster than NASNet with 1.2\% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Tan, M. and Chen, B. and Pang, R. and Vasudevan, V. and Sandler, M. and Howard, A. and Le, Q. V.},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Categorization, Deep Learning, MnasNet, Recognition: Detection, Retrieval, automated mobile neural architecture search approach, computer vision, convolutional neural nets, convolutional neural networks, hierarchical search space, image classification, learning (artificial intelligence), mobile CNN models, mobile handsets, mobile phones, object detection, platform-aware neural architecture search},
	pages = {2815--2823},
}

@inproceedings{wu_fbnet_2019-1,
	title = {{FBNet}: {Hardware}-{Aware} {Efficient} {ConvNet} {Design} via {Differentiable} {Neural} {Architecture} {Search}},
	shorttitle = {{FBNet}},
	doi = {10.1109/CVPR.2019.01099},
	abstract = {Designing accurate and efficient ConvNets for mobile devices is challenging because the design space is combinatorially large. Due to this, previous neural architecture search (NAS) methods are computationally expensive. ConvNet architecture optimality depends on factors such as input resolution and target devices. However, existing approaches are too resource demanding for case-by-case redesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP count does not always reflect actual latency. To address these, we propose a differentiable neural architecture search (DNAS) framework that uses gradient-based methods to optimize ConvNet architectures, avoiding enumerating and training individual architectures separately as in previous methods. FBNets (Facebook-Berkeley-Nets), a family of models discovered by DNAS surpass state-of-the-art models both designed manually and generated automatically. FBNet-B achieves 74.1\% top-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8 phone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with similar accuracy. Despite higher accuracy and lower latency than MnasNet, we estimate FBNet-B's search cost is 420x smaller than MnasNet's, at only 216 GPU-hours. Searched for different resolutions and channel sizes, FBNets achieve 1.5\% to 6.4\% higher accuracy than MobileNetV2. The smallest FBNet achieves 50.2\% accuracy and 2.9 ms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized FBNet, the iPhone-X-optimized model achieves a 1.4x speedup on an iPhone X. FBNet models are open-sourced at https://github. com/facebookresearch/mobile-vision.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wu, B. and Dai, X. and Zhang, P. and Wang, Y. and Sun, F. and Wu, Y. and Tian, Y. and Vajda, P. and Jia, Y. and Keutzer, K.},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {ConvNet architecture optimality, ConvNet architectures, Deep Learning, FLOPs, Facebook-Berkeley-Nets, Samsung-optimized FBNet, convolutional neural nets, differentiable neural architecture search framework, gradient methods, gradient-based methods, graphics processing units, hardware-aware efficient ConvNet design, iPhone X. FBNet models, iPhone-X-optimized model, mobile computing, mobile devices, optimisation},
	pages = {10726--10734},
}

@inproceedings{li_mimicking_2017,
	title = {Mimicking {Very} {Efficient} {Network} for {Object} {Detection}},
	doi = {10.1109/CVPR.2017.776},
	abstract = {Current CNN based object detectors need initialization from pre-trained ImageNet classification models, which are usually time-consuming. In this paper, we present a fully convolutional feature mimic framework to train very efficient CNN based detectors, which do not need ImageNet pre-training and achieve competitive performance as the large and slow models. We add supervision from high-level features of the large networks in training to help the small network better learn object representation. More specifically, we conduct a mimic method for the features sampled from the entire feature map and use a transform layer to map features from the small network onto the same dimension of the large network. In training the small network, we optimize the similarity between features sampled from the same region on the feature maps of both networks. Extensive experiments are conducted on pedestrian and common object detection tasks using VGG, Inception and ResNet. On both Caltech and Pascal VOC, we show that the modified 2.5× accelerated Inception network achieves competitive performance as the full Inception Network. Our faster model runs at 80 FPS for a 1000×1500 large input with only a minor degradation of performance on Caltech.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Li, Q. and Jin, S. and Yan, J.},
	month = jul,
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {Acceleration, CNN based detectors, Caltech, Detectors, Feature extraction, ImageNet classification, Inception, Object detection, Pascal VOC, Proposals, ResNet, Training, VGG, convolutional feature mimic framework, image classification, image representation, neural nets, object detection, object representation},
	pages = {7341--7349},
}

@inproceedings{peng_correlation_2019,
	title = {Correlation {Congruence} for {Knowledge} {Distillation}},
	doi = {10.1109/ICCV.2019.00511},
	abstract = {Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Peng, B. and Jin, X. and Li, D. and Zhou, S. and Wu, Y. and Liu, J. and Zhang, Z. and Liu, Y.},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {CCKD, CIFAR-100, Correlation, ImageNet-1K, Kernel, Knowledge engineering, Knowledge transfer, Measurement, SOTA KD-based methods, Task analysis, Training, correlation congruence, correlation congruence for knowledge distillation, correlation methods, face recognition, image classification, instance level, instance-level information, learning (artificial intelligence), metric learning tasks, strong congruent constraint, teacher-student frameworks},
	pages = {5006--5015},
}

@inproceedings{goel_low-power_2020,
	address = {New York, NY, USA},
	series = {{ISLPED} '20},
	title = {Low-power object counting with hierarchical neural networks},
	isbn = {978-1-4503-7053-0},
	doi = {10.1145/3370748.3406569},
	abstract = {Deep Neural Networks (DNNs) achieve state-of-the-art accuracy in many computer vision tasks, such as object counting. Object counting takes two inputs: an image and an object query and reports the number of occurrences of the queried object. To achieve high accuracy, DNNs require billions of operations, making them difficult to deploy on resource-constrained, low-power devices. Prior work shows that a significant number of DNN operations are redundant and can be eliminated without affecting the accuracy. To reduce these redundancies, we propose a hierarchical DNN architecture for object counting. This architecture uses a Region Proposal Network (RPN) to propose regions-of-interest (RoIs) that may contain the queried objects. A hierarchical classifier then efficiently finds the RoIs that actually contain the queried objects. The hierarchy contains groups of visually similar object categories. Small DNNs at each node of the hierarchy classify between these groups. The RoIs are incrementally processed by the hierarchical classifier. If the object in an RoI is in the same group as the queried object, then the next DNN in the hierarchy processes the RoI further; otherwise, the RoI is discarded. By using a few small DNNs to process each image, this method reduces the memory requirement, inference time, energy consumption, and number of operations with negligible accuracy loss when compared with the existing techniques.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the {ACM}/{IEEE} {International} {Symposium} on {Low} {Power} {Electronics} and {Design}},
	publisher = {Association for Computing Machinery},
	author = {Goel, Abhinav and Tung, Caleb and Aghajanzadeh, Sara and Ghodgaonkar, Isha and Ghosh, Shreya and Thiruvathukal, George K. and Lu, Yung-Hsiang},
	month = aug,
	year = {2020},
	keywords = {low-power, neural networks, object counting},
	pages = {163--168},
}

@article{goel_modular_2020,
	title = {Modular {Neural} {Networks} for {Low}-{Power} {Image} {Classification} on {Embedded} {Devices}},
	volume = {26},
	issn = {1084-4309},
	doi = {10.1145/3408062},
	abstract = {Embedded devices are generally small, battery-powered computers with limited hardware resources. It is difficult to run deep neural networks (DNNs) on these devices, because DNNs perform millions of operations and consume significant amounts of energy. Prior research has shown that a considerable number of a DNN’s memory accesses and computation are redundant when performing tasks like image classification. To reduce this redundancy and thereby reduce the energy consumption of DNNs, we introduce the Modular Neural Network Tree architecture. Instead of using one large DNN for the classifier, this architecture uses multiple smaller DNNs (called modules) to progressively classify images into groups of categories based on a novel visual similarity metric. Once a group of categories is selected by a module, another module then continues to distinguish among the similar categories within the selected group. This process is repeated over multiple modules until we are left with a single category. The computation needed to distinguish dissimilar groups is avoided, thus reducing redundant operations, memory accesses, and energy. Experimental results using several image datasets reveal the effectiveness of our proposed solution to reduce memory requirements by 50\% to 99\%, inference time by 55\% to 95\%, energy consumption by 52\% to 94\%, and the number of operations by 15\% to 99\% when compared with existing DNN architectures, running on two different embedded systems: Raspberry Pi 3 and Raspberry Pi Zero.},
	number = {1},
	urldate = {2020-10-23},
	journal = {ACM Transactions on Design Automation of Electronic Systems},
	author = {Goel, Abhinav and Aghajanzadeh, Sara and Tung, Caleb and Chen, Shuo-Han and Thiruvathukal, George K. and Lu, Yung-Hsiang},
	month = oct,
	year = {2020},
	keywords = {Low-power, image classification},
	pages = {1:1--1:35},
}

@article{courbariaux_binarized_2016,
	title = {Binarized {Neural} {Networks}: {Training} {Deep} {Neural} {Networks} with {Weights} and {Activations} {Constrained} to +1 or -1},
	shorttitle = {Binarized {Neural} {Networks}},
	abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
	urldate = {2020-11-02},
	journal = {arXiv:1602.02830 [cs]},
	author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	month = mar,
	year = {2016},
	note = {arXiv: 1602.02830},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{ding_flightnns_2019,
	title = {{FLightNNs}: {Lightweight} {Quantized} {Deep} {Neural} {Networks} for {Fast} and {Accurate} {Inference}},
	shorttitle = {{FLightNNs}},
	abstract = {To improve the throughput and energy efficiency of Deep Neural Networks (DNNs) on customized hardware, lightweight neural networks constrain the weights of DNNs to be a limited combination (denoted as k ∈ 1, 2) of powers of 2. In such networks, the multiply-accumulate operation can be replaced with a single shift operation, or two shifts and an add operation. To provide even more design flexibility, the k for each convolutional filter can be optimally chosen instead of being fixed for every filter. In this paper, we formulate the selection of k to be differentiable, and describe model training for determining k-based weights on a per-filter basis. Over 46 FPGA-design experiments involving eight configurations and four data sets reveal that lightweight neural networks with a flexible k value (dubbed FLightNNs) fully utilize the hardware resources on Field Programmable Gate Arrays (FPGAs), our experimental results show that FLightNNs can achieve 2× speedup when compared to lightweight NNs with k = 2, with only 0.1\% accuracy degradation. Compared to a 4-bit fixed-point quantization, FLightNNs achieve higher accuracy and up to 2× inference speedup, due to their lightweight shift operations. In addition, our experiments also demonstrate that FLightNNs can achieve higher computational energy efficiency for ASIC implementation.},
	booktitle = {2019 56th {ACM}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	author = {Ding, R. and Liu, Z. and Chin, T. and Marculescu, D. and Blanton, R. D. Shawn},
	month = jun,
	year = {2019},
	note = {ISSN: 0738-100X},
	keywords = {4-bit fixed-point quantization, DNNs, Energy efficiency, FLightNNs, FPGA-design experiments, Field programmable gate arrays, Filtering algorithms, Hardware, Neural networks, Quantization (signal), Training, application specific integrated circuits, convolutional filter, field programmable gate arrays, fixed point arithmetic, inference speedup, lightweight NNs, lightweight neural networks, lightweight quantized Deep Neural Networks, lightweight shift operations, multiply-accumulate operation, neural nets, per-filter basis, single shift operation},
	pages = {1--6},
}

@inproceedings{zhang_optimizing_2015,
	address = {New York, NY, USA},
	series = {{FPGA} '15},
	title = {Optimizing {FPGA}-based {Accelerator} {Design} for {Deep} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4503-3315-3},
	doi = {10.1145/2684746.2689060},
	abstract = {Convolutional neural network (CNN) has been widely employed for image recognition because it can achieve high accuracy by emulating behavior of optic nerves in living creatures. Recently, rapid growth of modern applications based on deep learning algorithms has further improved research and implementations. Especially, various accelerators for deep CNN have been proposed based on FPGA platform because it has advantages of high performance, reconfigurability, and fast development round, etc. Although current FPGA accelerators have demonstrated better performance over generic processors, the accelerator design space has not been well exploited. One critical problem is that the computation throughput may not well match the memory bandwidth provided an FPGA platform. Consequently, existing approaches cannot achieve best performance due to under-utilization of either logic resource or memory bandwidth. At the same time, the increasing complexity and scalability of deep learning applications aggravate this problem. In order to overcome this problem, we propose an analytical design scheme using the roofline model. For any solution of a CNN design, we quantitatively analyze its computing throughput and required memory bandwidth using various optimization techniques, such as loop tiling and transformation. Then, with the help of rooine model, we can identify the solution with best performance and lowest FPGA resource requirement. As a case study, we implement a CNN accelerator on a VC707 FPGA board and compare it to previous approaches. Our implementation achieves a peak performance of 61.62 GFLOPS under 100MHz working frequency, which outperform previous approaches significantly.},
	urldate = {2019-07-07},
	booktitle = {Proceedings of the 2015 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {ACM},
	author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
	year = {2015},
	note = {event-place: Monterey, California, USA},
	keywords = {acceleration, convolutional neural network, fpga, roofline model},
	pages = {161--170},
}

@inproceedings{verelst_dynamic_2020,
	title = {Dynamic {Convolutions}: {Exploiting} {Spatial} {Sparsity} for {Faster} {Inference}},
	shorttitle = {Dynamic {Convolutions}},
	doi = {10.1109/CVPR42600.2020.00239},
	abstract = {Modern convolutional neural networks apply the same operations on every pixel in an image. However, not all image regions are equally important. To address this inefficiency, we propose a method to dynamically apply convolutions conditioned on the input image. We introduce a residual block where a small gating branch learns which spatial positions should be evaluated. These discrete gating decisions are trained end-to-end using the Gumbel-Softmax trick, in combination with a sparsity criterion. Our experiments on CIFAR, ImageNet, Food-101 and MPII show that our method has better focus on the region of interest and better accuracy than existing methods, at a lower computational complexity. Moreover, we provide an efficient CUDA implementation of our dynamic convolutions using a gather-scatter approach, achieving a significant improvement in inference speed on MobileNetV2 and ShuffleNetV2. On human pose estimation, a task that is inherently spatially sparse, the processing speed is increased by 60\% with no loss in accuracy.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Verelst, T. and Tuytelaars, T.},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {Complexity theory, Computer architecture, Graphics processing units, Gumbel-Softmax trick, Image coding, Neural networks, Task analysis, Tensile stress, convolutional neural nets, discrete gating decisions, dynamic convolutions, gating branch, image regions, inference speed, input image, modern convolutional neural networks, pose estimation, residual block, sparsity criterion, spatial sparsity},
	pages = {2317--2326},
}

@inproceedings{ren_sbnet_2018,
	title = {{SBNet}: {Sparse} {Blocks} {Network} for {Fast} {Inference}},
	shorttitle = {{SBNet}},
	doi = {10.1109/CVPR.2018.00908},
	abstract = {Conventional deep convolutional neural networks (CNNs) apply convolution operators uniformly in space across all feature maps for hundreds of layers - this incurs a high computational cost for real-time applications. For many problems such as object detection and semantic segmentation, we are able to obtain a low-cost computation mask, either from a priori problem knowledge, or from a low-resolution segmentation network. We show that such computation masks can be used to reduce computation in the high-resolution main network. Variants of sparse activation CNNs have previously been explored on small-scale tasks and showed no degradation in terms of object classification accuracy, but often measured gains in terms of theoretical FLOPs without realizing a practical speedup when compared to highly optimized dense convolution implementations. In this work, we leverage the sparsity structure of computation masks and propose a novel tiling-based sparse convolution algorithm. We verified the effectiveness of our sparse CNN on LiDAR-based 3D object detection, and we report significant wall-clock speed-ups compared to dense convolution without noticeable loss of accuracy.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ren, M. and Pokrovsky, A. and Yang, B. and Urtasun, R.},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Convolution, Kernel, Object detection, SBNet, Shape, Task analysis, Three-dimensional displays, computation masks, conventional deep convolutional neural networks, convolution, convolution operators, feature extraction, feature maps, high computational cost, high-resolution main network, highly optimized dense convolution implementations, image classification, image resolution, image segmentation, low-cost computation mask, low-resolution segmentation network, neural nets, novel tiling-based sparse convolution algorithm, object classification accuracy, object detection, optical radar, parallel processing, real-time applications, semantic segmentation, small-scale tasks, sparse activation CNN, sparse blocks network},
	pages = {8711--8720},
}

@inproceedings{aghajanzadeh_camera_2020,
	title = {Camera {Placement} {Meeting} {Restrictions} of {Computer} {Vision}},
	doi = {10.1109/ICIP40778.2020.9190851},
	abstract = {In the blooming era of smart edge devices, surveillance cameras have been deployed in many locations. Surveillance cameras are most useful when they are spaced out to maximize coverage of an area. However, deciding where to place cameras is an NP-hard problem and researchers have proposed heuristic solutions. Existing work does not consider a significant restriction of computer vision: in order to track a moving object, the object must occupy enough pixels. The number of pixels depends on many factors (How far away is the object? What is the camera resolution? What is the focal length?). In this study, we propose a camera placement method that identifies effective camera placement in arbitrary spaces and can account for different camera types as well. Our strategy represents spaces as polygons, then uses a greedy algorithm to partition the polygons and determine the cameras’ locations to provide the desired coverage. Our solution also makes it possible to perform object tracking via overlapping camera placement. Our method is evaluated against complex shapes and real-world museum floor plans, achieving up to 85\% coverage and 25\% overlap.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Aghajanzadeh, S. and Naidu, R. and Chen, S.-H. and Tung, C. and Goel, A. and Lu, Y.-H. and Thiruvathukal, G. K.},
	month = oct,
	year = {2020},
	note = {ISSN: 2381-8549},
	keywords = {Camera Placement, Cameras, Computational Geometry, Computer Vision, Computer vision, NP-hard problem, Object tracking, Security, Shape, Surveillance},
	pages = {3254--3258},
}

@inproceedings{surakitbanharn_cross-referencing_2018,
	title = {Cross-referencing social media and public surveillance camera data for disaster response},
	doi = {10.1109/THS.2018.8574200},
	abstract = {Physical media (like surveillance cameras) and social media (like Instagram and Twitter) may both be useful in attaining on-the-ground information during an emergency or disaster situation. However, the intersection and reliability of both surveillance cameras and social media during a natural disaster are not fully understood. To address this gap, we tested whether social media is of utility when physical surveillance cameras went off-line during Hurricane Irma in 2017. Specifically, we collected and compared geo-tagged Instagram and Twitter posts in the state of Florida during times and in areas where public surveillance cameras went off-line. We report social media content and frequency and content to determine the utility for emergency managers or first responders during a natural disaster.},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Technologies} for {Homeland} {Security} ({HST})},
	author = {Surakitbanharn, C. and Yau, C. and Wang, G. and Chawla, A. and Pan, Y. and Sun, Z. and Yellin, S. and Ebert, D. and Lu, Y. and Thiruvathukal, G. K.},
	month = oct,
	year = {2018},
	keywords = {Cameras, Hurricanes, Instagram, Media, Surveillance, Twitter, Videos, cameras, disasters, emergency management, natural disaster, public surveillance camera data, social media content, social networking (online)},
	pages = {1--9},
}

@inproceedings{tung_large-scale_2019,
	title = {Large-{Scale} {Object} {Detection} of {Images} from {Network} {Cameras} in {Variable} {Ambient} {Lighting} {Conditions}},
	doi = {10.1109/MIPR.2019.00080},
	abstract = {Computer vision relies on labeled datasets for training and evaluation in detecting and recognizing objects. The popular computer vision program, YOLO ("You Only Look Once"), has been shown to accurately detect objects in many major image datasets. However, the images found in those datasets, are independent of one another and cannot be used to test YOLO's consistency at detecting the same object as its environment (e.g. ambient lighting) changes. This paper describes a novel effort to evaluate YOLO's consistency for large-scale applications. It does so by working (a) at large scale and (b) by using consecutive images from a curated network of public video cameras deployed in a variety of real-world situations, including traf?c intersections, national parks, shopping malls, university campuses, etc. We speci?cally examine YOLO's ability to detect objects in different scenarios (e.g., daytime vs. night), leveraging the cameras' ability to rapidly retrieve many successive images for evaluating detection consistency. Using our camera network and advanced computing resources (supercomputers), we analyzedmorethan5millionimagescapturedby140network cameras in 24 hours. Compared with labels marked by humans (considered as "ground truth"), YOLO struggles to consistently detect the same humans and cars as their positions change from one frame to the next; it also struggles to detect objects at night time. Our ?ndings suggest that state-of-the art vision solutions should be trained by data from network camera with contextual information before they can be deployed in applications that demand high consistency on object detection.},
	booktitle = {2019 {IEEE} {Conference} on {Multimedia} {Information} {Processing} and {Retrieval} ({MIPR})},
	author = {Tung, C. and Kelleher, M. R. and Schlueter, R. J. and Xu, B. and Lu, Y. and Thiruvathukal, G. K. and Chen, Y. and Lu, Y.},
	month = mar,
	year = {2019},
	keywords = {Automobiles, Cameras, Computer Vision, Computer vision, Detectors, Lighting, Network Cameras, Object Detection, Object Recognition, Object detection, Streaming media, YOLO consistency, YOLO's ability, You Only Look Once, advanced computing resources, camera network, cameras, computer vision, computer vision program, consecutive images, curated network, detection consistency, high consistency, image datasets, labeled datasets, labels, large-scale applications, large-scale object detection, lighting, network camera, object detection, objects recognition, public video cameras, state-of-the art vision solutions, successive images, time 24.0 hour, training, variable ambient lighting conditions, video cameras},
	pages = {393--398},
}

@article{alyamkin_low-power_2019,
	title = {Low-{Power} {Computer} {Vision}: {Status}, {Challenges}, and {Opportunities}},
	volume = {9},
	issn = {2156-3365},
	shorttitle = {Low-{Power} {Computer} {Vision}},
	doi = {10.1109/JETCAS.2019.2911899},
	abstract = {Computer vision has achieved impressive progress in recent years. Meanwhile, mobile phones have become the primary computing platforms for millions of people. In addition to mobile phones, many autonomous systems rely on visual data for making decisions, and some of these systems have limited energy (such as unmanned aerial vehicles also called drones and mobile robots). These systems rely on batteries, and energy efficiency is critical. This paper serves the following two main purposes. First, examine the state of the art for low-power solutions to detect objects in images. Since 2015, the IEEE Annual International Low-Power Image Recognition Challenge (LPIRC) has been held to identify the most energy-efficient computer vision solutions. This paper summarizes the 2018 winners' solutions. Second, suggest directions for research as well as opportunities for low-power computer vision.},
	number = {2},
	journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
	author = {Alyamkin, S. and Ardi, M. and Berg, A. C. and Brighton, A. and Chen, B. and Chen, Y. and Cheng, H. and Fan, Z. and Feng, C. and Fu, B. and Gauen, K. and Goel, A. and Goncharenko, A. and Guo, X. and Ha, S. and Howard, A. and Hu, X. and Huang, Y. and Kang, D. and Kim, J. and Ko, J. G. and Kondratyev, A. and Lee, J. and Lee, S. and Lee, S. and Li, Z. and Liang, Z. and Liu, J. and Liu, X. and Lu, Y. and Lu, Y. and Malik, D. and Nguyen, H. H. and Park, E. and Repin, D. and Shen, L. and Sheng, T. and Sun, F. and Svitov, D. and Thiruvathukal, G. K. and Zhang, B. and Zhang, J. and Zhang, X. and Zhuo, S.},
	month = jun,
	year = {2019},
	note = {Conference Name: IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
	keywords = {Benchmark testing, Computer architecture, Computer vision, Hardware, Image recognition, Mobile handsets, Software, autonomous systems, batteries, computer vision, decision making, energy efficiency, energy-efficient computer vision solutions, low-power computer vision, low-power electronics, low-power solutions, machine intelligence, mobile phones, object detection, primary computing platforms, unmanned aerial vehicles, visual data},
	pages = {411--421},
}

@inproceedings{hu_crowdsourcing_2020,
	address = {New York, NY, USA},
	series = {{WWW} '20},
	title = {Crowdsourcing {Detection} of {Sampling} {Biases} in {Image} {Datasets}},
	isbn = {978-1-4503-7023-3},
	url = {http://doi.org/10.1145/3366423.3380063},
	doi = {10.1145/3366423.3380063},
	abstract = {Despite many exciting innovations in computer vision, recent studies reveal a number of risks in existing computer vision systems, suggesting results of such systems may be unfair and untrustworthy. Many of these risks can be partly attributed to the use of a training image dataset that exhibits sampling biases and thus does not accurately reflect the real visual world. Being able to detect potential sampling biases in the visual dataset prior to model development is thus essential for mitigating the fairness and trustworthy concerns in computer vision. In this paper, we propose a three-step crowdsourcing workflow to get humans into the loop for facilitating bias discovery in image datasets. Through two sets of evaluation studies, we find that the proposed workflow can effectively organize the crowd to detect sampling biases in both datasets that are artificially created with designed biases and real-world image datasets that are widely used in computer vision research and system development.},
	urldate = {2020-10-28},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {Association for Computing Machinery},
	author = {Hu, Xiao and Wang, Haobo and Vegesana, Anirudh and Dube, Somesh and Yu, Kaiwen and Kao, Gore and Chen, Shuo-Han and Lu, Yung-Hsiang and Thiruvathukal, George K. and Yin, Ming},
	month = apr,
	year = {2020},
	keywords = {crowdsourcing, image dataset, sampling bias, workflow design},
	pages = {2955--2961},
}

@article{redwood_falls_gazette_mndot_nodate,
	title = {{MnDOT} to install road weather information systems in the southwest {Minnesota}},
	url = {https://www.redwoodfallsgazette.com/news/20200923/mndot-to-install-road-weather-information-systems-in-southwest-minnesota},
	abstract = {Beginning Sept. 28, motorists traveling throughout southwest Minnesota can expect short-term lane closures as road weather information systems (RWIS)},
	language = {en},
	urldate = {2020-09-30},
	journal = {Redwood Falls Gazette},
	author = {Redwood Falls Gazette},
}

@article{leon_unbiased_2020,
	title = {Unbiased {Surveillance}: {AI} {Security} {Tech} {That} {Spots} {Guns}, {Not} {People}},
	shorttitle = {Unbiased {Surveillance}},
	url = {https://observer.com/2020/02/athena-security-surveillance-bias-gun-control/},
	abstract = {We applaud tech that can prevent yet another shooting tragedy and doesn’t employ racial basis in the equation.},
	language = {en-US},
	urldate = {2020-09-30},
	journal = {Observer},
	author = {Leon, Harmon},
	month = feb,
	year = {2020},
}

@article{terry_thinking_2020,
	title = {Thinking like a naturalist: {Enhancing} computer vision of citizen science images by harnessing contextual data},
	volume = {11},
	issn = {2041-210X},
	shorttitle = {Thinking like a naturalist},
	url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13335},
	doi = {10.1111/2041-210X.13335},
	abstract = {The accurate identification of species in images submitted by citizen scientists is currently a bottleneck for many data uses. Machine learning tools offer the potential to provide rapid, objective and scalable species identification for the benefit of many aspects of ecological science. Currently, most approaches only make use of image pixel data for classification. However, an experienced naturalist would also use a wide variety of contextual information such as the location and date of recording. Here, we examine the automated identification of ladybird (Coccinellidae) records from the British Isles submitted to the UK Ladybird Survey, a volunteer-led mass participation recording scheme. Each image is associated with metadata; a date, location and recorder ID, which can be cross-referenced with other data sources to determine local weather at the time of recording, habitat types and the experience of the observer. We built multi-input neural network models that synthesize metadata and images to identify records to species level. We show that machine learning models can effectively harness contextual information to improve the interpretation of images. Against an image-only baseline of 48.2\%, we observe a 9.1 percentage-point improvement in top-1 accuracy with a multi-input model compared to only a 3.6\% increase when using an ensemble of image and metadata models. This suggests that contextual data are being used to interpret an image, beyond just providing a prior expectation. We show that our neural network models appear to be utilizing similar pieces of evidence as human naturalists to make identifications. Metadata is a key tool for human naturalists. We show it can also be harnessed by computer vision systems. Contextualization offers considerable extra information, particularly for challenging species, even within small and relatively homogeneous areas such as the British Isles. Although complex relationships between disparate sources of information can be profitably interpreted by simple neural network architectures, there is likely considerable room for further progress. Contextualizing images has the potential to lead to a step change in the accuracy of automated identification tools, with considerable benefits for large-scale verification of submitted records.},
	language = {en},
	number = {2},
	urldate = {2020-10-26},
	journal = {Methods in Ecology and Evolution},
	author = {Terry, J. Christopher D. and Roy, Helen E. and August, Tom A.},
	year = {2020},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13335},
	keywords = {citizen science, computer vision, convolutional neural network, ladybird, machine learning, metadata, naturalists, species identification},
	pages = {303--315},
}

@article{qu_joint_2017,
	title = {Joint {Hierarchical} {Category} {Structure} {Learning} and {Large}-{Scale} {Image} {Classification}},
	volume = {26},
	issn = {1941-0042},
	doi = {10.1109/TIP.2016.2615423},
	abstract = {We investigate the scalable image classification problem with a large number of categories. Hierarchical visual data structures are helpful for improving the efficiency and performance of large-scale multi-class classification. We propose a novel image classification method based on learning hierarchical inter-class structures. Specifically, we first design a fast algorithm to compute the similarity metric between categories, based on which a visual tree is constructed by hierarchical spectral clustering. Using the learned visual tree, a test sample label is efficiently predicted by searching for the best path over the entire tree. The proposed method is extensively evaluated on the ILSVRC2010 and Caltech 256 benchmark datasets. The experimental results show that our method obtains significantly better category hierarchies than other state-of-the-art visual tree-based methods and, therefore, much more accurate classification.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {Qu, Y. and Lin, L. and Shen, F. and Lu, C. and Wu, Y. and Xie, Y. and Tao, D.},
	month = sep,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Caltech 256 benchmark dataset, Clustering algorithms, Feature extraction, Hierarchical learning, ILSVRC2010 benchmark dataset, Image representation, Measurement, N-best path, Prediction algorithms, Semantics, Visualization, deep features, hierarchical spectral clustering, hierarchical visual data structure, image classification, joint hierarchical category structure learning, large-scale image classification, large-scale multiclass classification efficiency improvement, large-scale multiclass classification performance improvement, learning (artificial intelligence), visual tree},
	pages = {4331--4346},
}

@article{panda_falcon_2017,
	title = {{FALCON}: {Feature} {Driven} {Selective} {Classification} for {Energy}-{Efficient} {Image} {Recognition}},
	volume = {36},
	issn = {1937-4151},
	shorttitle = {{FALCON}},
	doi = {10.1109/TCAD.2017.2681075},
	abstract = {Machine-learning algorithms have shown outstanding image recognition/classification performance for computer vision applications. However, the compute and energy requirement for implementing such classifier models for large-scale problems is quite high. In this paper, we propose feature driven selective classification (FALCON) inspired by the biological visual attention mechanism in the brain to optimize the energy-efficiency of machine-learning classifiers. We use the consensus in the characteristic features (color/texture) across images in a dataset to decompose the original classification problem and construct a tree of classifiers (nodes) with a generic-to-specific transition in the classification hierarchy. The initial nodes of the tree separate the instances based on feature information and selectively enable the latter nodes to perform object specific classification. The proposed methodology allows selective activation of only those branches and nodes of the classification tree that are relevant to the input while keeping the remaining nodes idle. Additionally, we propose a programmable and scalable neuromorphic engine (NeuE) that utilizes arrays of specialized neural computational elements to execute the FALCON-based classifier models for diverse datasets. The structure of FALCON facilitates the reuse of nodes while scaling up from small classification problems to larger ones thus allowing us to construct classifier implementations that are significantly more efficient. We evaluate our approach for a 12-object classification task on the Caltech101 dataset and ten-object task on CIFAR-10 dataset by constructing FALCON models on the NeuE platform in 45-nm technology. Our results demonstrate up to 3.66× improvement in energy-efficiency for no loss in output quality, and even higher improvements of up to 5.91× with 3.9\% accuracy loss compared to an optimized baseline network. In addition, FALCON shows an improvement in training time of up to 1.96× as compared to the traditional classification approach.},
	number = {12},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Panda, P. and Ankit, A. and Wijesinghe, P. and Roy, K.},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {Computational modeling, Energy efficiency, Energy-efficient classification, FALCON-based classifier models, Image color analysis, Machine learning, Neuromorphic engineering, Neuromorphics, Training data, Visualization, biological visual attention mechanism, classification hierarchy, classification tree, computer vision, computer vision applications, energy requirement, energy-efficient image recognition, feature driven selective classification, feature extraction, hierarchical feature learning, image classification, image recognition, large-scale problems, learning (artificial intelligence), machine learning, machine-learning algorithms, machine-learning classifiers, medical image processing, neural nets, neuromorphic hardware, object recognition, original classification problem, pattern classification, selective activation, specialized neural computational elements, specific classification, traditional classification approach},
	pages = {2017--2029},
}

@article{mittal_studying_2019,
	title = {Studying the plasticity in deep convolutional neural networks using random pruning},
	volume = {30},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-018-01001-9},
	doi = {10.1007/s00138-018-01001-9},
	abstract = {Recently, there has been a lot of work on pruning filters from deep convolutional neural networks (CNNs) with the intention of reducing computations. The key idea is to rank the filters based on a certain criterion (say, \$\$l\_1\$\$l1-norm, average percentage of zeros, etc.) and retain only the top-ranked filters. Once the low-scoring filters are pruned away, the remainder of the network is fine-tuned and is shown to give performance comparable to the original unpruned network. In this work, we report experiments which suggest that the comparable performance of the pruned network is not due to the specific criterion chosen, but due to the inherent plasticity of deep neural networks which allows them to recover from the loss of pruned filters once the rest of the filters are fine-tuned. Specifically, we show counterintuitive results wherein by randomly pruning 25–50\% filters from deep CNNs we are able to obtain the same performance as obtained by using state-of-the-art pruning methods. We empirically validate our claims by doing an exhaustive evaluation with VGG-16 and ResNet-50. Further, we also evaluate a real-world scenario where a CNN trained on all 1000 ImageNet classes needs to be tested on only a small set of classes at test time (say, only animals). We create a new benchmark dataset from ImageNet to evaluate such class-specific pruning and show that even here a random pruning strategy gives close to state-of-the-art performance. Lastly, unlike existing approaches which mainly focus on the task of image classification, in this work we also report results on object detection and image segmentation. We show that using a simple random pruning strategy, we can achieve significant speedup in object detection (74\% improvement in fps) while retaining the same accuracy as that of the original Faster-RCNN model. Similarly, we show that the performance of a pruned segmentation network is actually very similar to that of the original unpruned SegNet.},
	language = {en},
	number = {2},
	urldate = {2020-10-25},
	journal = {Machine Vision and Applications},
	author = {Mittal, Deepak and Bhardwaj, Shweta and Khapra, Mitesh M. and Ravindran, Balaraman},
	month = mar,
	year = {2019},
	pages = {203--216},
}

@article{zagoruyko_wide_2017,
	title = {Wide {Residual} {Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
	urldate = {2020-10-25},
	journal = {arXiv:1605.07146 [cs]},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = jun,
	year = {2017},
	note = {arXiv: 1605.07146},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{iandola_squeezenet_2016,
	title = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and {\textless}0.{5MB} model size},
	shorttitle = {{SqueezeNet}},
	url = {http://arxiv.org/abs/1602.07360},
	abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
	urldate = {2020-10-25},
	journal = {arXiv:1602.07360 [cs]},
	author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	month = nov,
	year = {2016},
	note = {arXiv: 1602.07360},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{huang_densely_2017,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html},
	urldate = {2020-10-25},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	pages = {4700--4708},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {CIFAR-10, COCO object detection dataset, COCO segmentation, Complexity theory, Degradation, ILSVRC \& COCO 2015 competitions, ILSVRC 2015 classification task, Image recognition, Image segmentation, ImageNet dataset, ImageNet localization, ImageNet test set, Neural networks, Training, VGG nets, Visualization, deep residual learning, deep residual nets, deeper neural network training, image classification, image recognition, learning (artificial intelligence), neural nets, object detection, residual function learning, residual nets, visual recognition tasks},
	pages = {770--778},
}

@inproceedings{hui_dense_2014,
	title = {Dense depth map generation using sparse depth data from normal flow},
	doi = {10.1109/ICIP.2014.7025779},
	abstract = {In this paper, we address the problem of dense depth map generation from two successive image frames in a video. We first recover the camera motion from the observable normal flow pattern using our previously proposed apparent flow constraints. Once the camera motion is estimated, sparse depth data can be directly recovered from the flow pattern. We utilize a hierarchical approach to generate an initial dense depth map from the sparse depth data. This depth map is further enhanced through the refinement of the associated optical flow field in a variational framework. Experimental results show that the proposed method can provide high-quality depth maps. We also have a faster computational time than the conventional optical flow approach.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Hui, T. and Ngan, K. N.},
	month = oct,
	year = {2014},
	note = {ISSN: 2381-8549},
	keywords = {Camera motion, Cameras, Computer vision, Equations, Integrated optics, Optical imaging, Optical signal processing, Pattern recognition, camera motion, dense depth map generation, depth map, hierarchical approach, image motion analysis, image sensors, image sequences, initial dense depth map, normal flow, observable normal flow pattern, optical flow, optical flow field, sparse depth data, variational framework},
	pages = {3837--3841},
}

@inproceedings{goel_survey_2020,
	title = {A {Survey} of {Methods} for {Low}-{Power} {Deep} {Learning} and {Computer} {Vision}},
	doi = {10.1109/WF-IoT48130.2020.9221198},
	abstract = {Deep neural networks (DNNs) are successful in many computer vision tasks. However, the most accurate DNNs require millions of parameters and operations, making them energy, computation and memory intensive. This impedes the deployment of large DNNs in low-power devices with limited compute resources. Recent research improves DNN models by reducing the memory requirement, energy consumption, and number of operations without significantly decreasing the accuracy. This paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses the methods for compacting and accelerating DNN models. The techniques can be divided into four major categories: (1) parameter quantization and pruning, (2) compressed convolutional filters and matrix factorization, (3) network architecture search, and (4) knowledge distillation. We analyze the accuracy, advantages, disadvantages, and potential solutions to the problems with the techniques in each category. We also discuss new evaluation metrics as a guideline for future research.},
	booktitle = {2020 {IEEE} 6th {World} {Forum} on {Internet} of {Things} ({WF}-{IoT})},
	author = {Goel, A. and Tung, C. and Lu, Y.-H. and Thiruvathukal, G. K.},
	month = jun,
	year = {2020},
	keywords = {computer vision, low-power, neural networks},
	pages = {1--6},
}

@article{chen_maod_2020,
	title = {{MAOD}: {An} {Efficient} {Anchor}-{Free} {Object} {Detector} {Based} on {MobileDet}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {{MAOD}},
	doi = {10.1109/ACCESS.2020.2992516},
	abstract = {For real-time object detectors, accuracy and efficiency are two important considerations. In this paper, we propose a lightweight anchor-free detector, MAOD, to better balance efficiency and accuracy. Our object detector contains three components: an efficient backbone network (MobileDet), a lightweight feature pyramid structure (L-FPN) and an anchor-free per-pixel prediction method. MobileDet and L-FPN provide more accurate and faster multi-scale feature extraction. Our anchor-free per-pixel prediction method achieves efficient classification and location regression tasks. On the benchmark MS-COCO dataset, MAOD achieves 46.1\% AP at the speed of 68 FPS with the input size 512 × 512. When the input size is 800 × 800, MAOD achieves 47.1\% AP at the speed of 43 FPS. The fast version of MAOD (320 × 320 input size) can run at 91 PS with 43.3\% AP. Compared with other state-of-the-art object detectors, our detector has similar accuracy while maintaining extremely fast inference speed. MAOD achieves an optimal efficiency-accuracy tradeoff.},
	journal = {IEEE Access},
	author = {Chen, D. and Shen, H.},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Computational modeling, Convolution, Detectors, Feature extraction, L-FPN, Lightweight real-time detector, MAOD, MS-COCO dataset, MobileDet, MobileDet backbone, Object detection, Task analysis, Tensors, anchor-free object detection, anchor-free object detector, anchor-free per-pixel prediction, backbone network, classification tasks, feature extraction, image classification, lightweight anchor-free detector, lightweight feature pyramid, lightweight feature pyramid structure, location regression tasks, mobile computing, multiscale feature extraction, neural nets, object detection, regression analysis},
	pages = {86564--86572},
}

@inproceedings{jia_caffe_2014,
	address = {New York, NY, USA},
	series = {{MM} '14},
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	isbn = {978-1-4503-3063-3},
	shorttitle = {Caffe},
	url = {http://doi.org/10.1145/2647868.2654889},
	doi = {10.1145/2647868.2654889},
	abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
	urldate = {2020-10-24},
	booktitle = {Proceedings of the 22nd {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	month = nov,
	year = {2014},
	keywords = {computer vision, machine learning, neural networks, open source, parallel computation},
	pages = {675--678},
}

@inproceedings{ladicky_pulling_2014,
	title = {Pulling {Things} out of {Perspective}},
	doi = {10.1109/CVPR.2014.19},
	abstract = {The limitations of current state-of-the-art methods for single-view depth estimation and semantic segmentations are closely tied to the property of perspective geometry, that the perceived size of the objects scales inversely with the distance. In this paper, we show that we can use this property to reduce the learning of a pixel-wise depth classifier to a much simpler classifier predicting only the likelihood of a pixel being at an arbitrarily fixed canonical depth. The likelihoods for any other depths can be obtained by applying the same classifier after appropriate image manipulations. Such transformation of the problem to the canonical depth removes the training data bias towards certain depths and the effect of perspective. The approach can be straight-forwardly generalized to multiple semantic classes, improving both depth estimation and semantic segmentation performance by directly targeting the weaknesses of independent approaches. Conditioning the semantic label on the depth provides a way to align the data to their physical scale, allowing to learn a more discriminative classifier. Conditioning depth on the semantic class helps the classifier to distinguish between ambiguities of the otherwise ill-posed problem. We tested our algorithm on the KITTI road scene dataset and NYU2 indoor dataset and obtained obtained results that significantly outperform current state-of-the-art in both single-view depth and semantic segmentation domain.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ladický, L. and Shi, J. and Pollefeys, M.},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919},
	keywords = {Computer vision, Depth Estimation, Estimation, Image segmentation, Joints, KITTI road scene dataset, NYU2 indoor dataset, Object Recognition, Semantic Segmentation, Semantics, Training, Training data, estimation theory, geometry, image classification, image manipulations, image segmentation, perspective geometry, pixel-wise depth classifier, semantic segmentations, single-view depth estimation},
	pages = {89--96},
}

@inproceedings{liu_discrete-continuous_2014,
	title = {Discrete-{Continuous} {Depth} {Estimation} from a {Single} {Image}},
	doi = {10.1109/CVPR.2014.97},
	abstract = {In this paper, we tackle the problem of estimating the depth of a scene from a single image. This is a challenging task, since a single image on its own does not provide any depth cue. To address this, we exploit the availability of a pool of images for which the depth is known. More specifically, we formulate monocular depth estimation as a discrete-continuous optimization problem, where the continuous variables encode the depth of the superpixels in the input image, and the discrete ones represent relationships between neighboring superpixels. The solution to this discrete-continuous optimization problem is then obtained by performing inference in a graphical model using particle belief propagation. The unary potentials in this graphical model are computed by making use of the images with known depth. We demonstrate the effectiveness of our model in both the indoor and outdoor scenarios. Our experimental evaluation shows that our depth estimates are more accurate than existing methods on standard datasets.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Liu, M. and Salzmann, M. and He, X.},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919},
	keywords = {Belief propagation, Computational modeling, Estimation, Graphical models, Image reconstruction, Optimization, Three-dimensional displays, discrete-continuous depth estimation, discrete-continuous optimization problem, estimation theory, graphical model, image reconstruction, image representation, monocular depth estimation, optimisation, particle belief propagation, single image reconstruction},
	pages = {716--723},
}

@inproceedings{xiao_sun3d_2013,
	title = {{SUN3D}: {A} {Database} of {Big} {Spaces} {Reconstructed} {Using} {SfM} and {Object} {Labels}},
	shorttitle = {{SUN3D}},
	doi = {10.1109/ICCV.2013.458},
	abstract = {Existing scene understanding datasets contain only a limited set of views of a place, and they lack representations of complete 3D spaces. In this paper, we introduce SUN3D, a large-scale RGB-D video database with camera pose and object labels, capturing the full 3D extent of many places. The tasks that go into constructing such a dataset are difficult in isolation – hand-labeling videos is painstaking, and structure from motion (SfM) is unreliable for large spaces. But if we combine them together, we make the dataset construction task much easier. First, we introduce an intuitive labeling tool that uses a partial reconstruction to propagate labels from one frame to another. Then we use the object labels to fix errors in the reconstruction. For this, we introduce a generalization of bundle adjustment that incorporates object-to-object correspondences. This algorithm works by constraining points for the same object from different frames to lie inside a fixed-size bounding box, parameterized by its rotation and translation. The SUN3D database, the source code for the generalized bundle adjustment, and the web-based 3D annotation tool are all available at http://sun3d.cs.princeton.edu.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Xiao, J. and Owens, A. and Torralba, A.},
	month = dec,
	year = {2013},
	note = {ISSN: 2380-7504},
	keywords = {3D space representation, Cameras, Databases, Image reconstruction, Labeling, SUN3D database, Semantics, SfM, Solid modeling, Three-dimensional displays, Web-based 3D annotation tool, big spaces, camera pose, cameras, fixed-size bounding box, generalized bundle adjustment, hand-labeling videos, image colour analysis, image reconstruction, intuitive labeling tool, large-scale RGB-D video database, motion estimation, object labels, object-to-object correspondences, partial image reconstruction, scene understanding datasets, source code, structure from motion, video databases},
	pages = {1625--1632},
}

@inproceedings{silberman_indoor_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Indoor {Segmentation} and {Support} {Inference} from {RGBD} {Images}},
	isbn = {978-3-642-33715-4},
	doi = {10.1007/978-3-642-33715-4_54},
	abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2012},
	publisher = {Springer},
	author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
	editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
	year = {2012},
	pages = {746--760},
}

@inproceedings{zhuo_indoor_2015,
	title = {Indoor {Scene} {Structure} {Analysis} for {Single} {Image} {Depth} {Estimation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Zhuo_Indoor_Scene_Structure_2015_CVPR_paper.html},
	urldate = {2020-10-24},
	author = {Zhuo, Wei and Salzmann, Mathieu and He, Xuming and Liu, Miaomiao},
	year = {2015},
	pages = {614--622},
}

@inproceedings{wang_towards_2015,
	title = {Towards {Unified} {Depth} and {Semantic} {Prediction} {From} a {Single} {Image}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Wang_Towards_Unified_Depth_2015_CVPR_paper.html},
	urldate = {2020-10-24},
	author = {Wang, Peng and Shen, Xiaohui and Lin, Zhe and Cohen, Scott and Price, Brian and Yuille, Alan L.},
	year = {2015},
	pages = {2800--2809},
}

@article{zalczer_depth_2020,
	title = {Depth {Map} {Quality} {Evaluation} for {Photographic} {Applications}},
	volume = {2020},
	doi = {10.2352/ISSN.2470-1173.2020.9.IQSP-370},
	abstract = {As depth imaging is integrated into more and more consumer devices, manufacturers have to tackle new challenges. Applica- tions such as computational bokeh and augmented reality require dense and precisely segmented depth maps to achieve good re- sults. Modern devices use a multitude
of different technologies to estimate depth maps, such as time-of-flight sensors, stereoscopic cameras, structured light sensors, phase-detect pixels or a com- bination thereof. Therefore, there is a need to evaluate the quality of the depth maps, regardless of the technology used to produce
them. The aim of our work is to propose an end-result evalua- tion method based on a single scene, using a specifically designed chart. We consider the depth maps embedded in the photographs, which are not visible to the user but are used by specialized soft- ware, in association with the
RGB pictures. Some of the aspects considered are spatial alignment between RGB and depth, depth consistency, and robustness to texture variations. This work also provides a comparison of perceptual and automatic evaluations.},
	number = {9},
	journal = {Electronic Imaging},
	author = {Zalczer, Eloi and Thomas, François-Xavier and Chanas, Laurent and Facciolo, Gabriele and Guichard, Frédéric},
	month = jan,
	year = {2020},
	keywords = {Augmented Reality, Computational Bokeh, Depth Map, Image Quality, Smartphone, Time-of-flight (ToF)},
	pages = {370--1--370--7},
}

@article{ghodgaonkar_observing_2020,
	title = {Observing {Responses} to the {COVID}-19 {Pandemic} using {Worldwide} {Network} {Cameras}},
	url = {http://arxiv.org/abs/2005.09091},
	abstract = {COVID-19 has resulted in a worldwide pandemic, leading to "lockdown" policies and social distancing. The pandemic has profoundly changed the world. Traditional methods for observing these historical events are difficult because sending reporters to areas with many infected people can put the reporters' lives in danger. New technologies are needed for safely observing responses to these policies. This paper reports using thousands of network cameras deployed worldwide for the purpose of witnessing activities in response to the policies. The network cameras can continuously provide real-time visual data (image and video) without human efforts. Thus, network cameras can be utilized to observe activities without risking the lives of reporters. This paper describes a project that uses network cameras to observe responses to governments' policies during the COVID-19 pandemic (March to April in 2020). The project discovers over 30,000 network cameras deployed in 110 countries. A set of computer tools are created to collect visual data from network cameras continuously during the pandemic. This paper describes the methods to discover network cameras on the Internet, the methods to collect and manage data, and preliminary results of data analysis. This project can be the foundation for observing the possible "second wave" in fall 2020. The data may be used for post-pandemic analysis by sociologists, public health experts, and meteorologists.},
	urldate = {2020-10-24},
	journal = {arXiv:2005.09091 [cs]},
	author = {Ghodgaonkar, Isha and Goel, Abhinav and Bordwell, Fischer and Tung, Caleb and Aghajanzadeh, Sara and Curran, Noah and Chen, Ryan and Yu, Kaiwen and Mahapatra, Sneha and Banna, Vishnu and Kao, Gore and Lee, Kate and Hu, Xiao and Eliopolous, Nick and Chinnakotla, Akhil and Rijhwani, Damini and Kim, Ashley and Chakraborty, Aditya and Ward, Mark Daniel and Lu, Yung-Hsiang and Thiruvathukal, George K.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.09091},
	keywords = {Computer Science - Computers and Society},
}

@article{ghodgaonkar_analyzing_2020,
	title = {Analyzing {Worldwide} {Social} {Distancing} through {Large}-{Scale} {Computer} {Vision}},
	url = {http://arxiv.org/abs/2008.12363},
	abstract = {In order to contain the COVID-19 pandemic, countries around the world have introduced social distancing guidelines as public health interventions to reduce the spread of the disease. However, monitoring the efficacy of these guidelines at a large scale (nationwide or worldwide) is difficult. To make matters worse, traditional observational methods such as in-person reporting is dangerous because observers may risk infection. A better solution is to observe activities through network cameras; this approach is scalable and observers can stay in safe locations. This research team has created methods that can discover thousands of network cameras worldwide, retrieve data from the cameras, analyze the data, and report the sizes of crowds as different countries issued and lifted restrictions (also called ''lockdown''). We discover 11,140 network cameras that provide real-time data and we present the results across 15 countries. We collect data from these cameras beginning April 2020 at approximately 0.5TB per week. After analyzing 10,424,459 images from still image cameras and frames extracted periodically from video, the data reveals that the residents in some countries exhibited more activity (judged by numbers of people and vehicles) after the restrictions were lifted. In other countries, the amounts of activities showed no obvious changes during the restrictions and after the restrictions were lifted. The data further reveals whether people stay ''social distancing'', at least 6 feet apart. This study discerns whether social distancing is being followed in several types of locations and geographical locations worldwide and serve as an early indicator whether another wave of infections is likely to occur soon.},
	urldate = {2020-10-24},
	journal = {arXiv:2008.12363 [cs]},
	author = {Ghodgaonkar, Isha and Chakraborty, Subhankar and Banna, Vishnu and Allcroft, Shane and Metwaly, Mohammed and Bordwell, Fischer and Kimura, Kohsuke and Zhao, Xinxin and Goel, Abhinav and Tung, Caleb and Chinnakotla, Akhil and Xue, Minghao and Lu, Yung-Hsiang and Ward, Mark Daniel and Zakharov, Wei and Ebert, David S. and Barbarash, David M. and Thiruvathukal, George K.},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.12363},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{shao_computer_2014,
	address = {Cham},
	series = {Advances in {Computer} {Vision} and {Pattern} {Recognition}},
	title = {Computer {Vision} and {Machine} {Learning} with {RGB}-{D} {Sensors}},
	isbn = {978-3-319-08650-7 978-3-319-08651-4},
	url = {http://link.springer.com/10.1007/978-3-319-08651-4},
	urldate = {2020-10-24},
	publisher = {Springer International Publishing},
	editor = {Shao, Ling and Han, Jungong and Kohli, Pushmeet and Zhang, Zhengyou},
	year = {2014},
	doi = {10.1007/978-3-319-08651-4},
}

@article{alphonse_depth_2020,
	title = {Depth {Perception} in a {Single} {RGB} {Camera} {Using} {Body} {Dimensions} and {Centroid} {Property}},
	volume = {37},
	issn = {07650019},
	url = {http://www.iieta.org/journals/ts/paper/10.18280/ts.370220},
	doi = {10.18280/ts.370220},
	number = {2},
	urldate = {2020-10-24},
	journal = {Traitement du Signal},
	author = {Alphonse, P.J.A. and Sriharsha, K.V.},
	month = apr,
	year = {2020},
	pages = {333--340},
}

@article{wadhwa_synthetic_2018,
	title = {Synthetic depth-of-field with a single-camera mobile phone},
	volume = {37},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3197517.3201329},
	doi = {10.1145/3197517.3201329},
	abstract = {Shallow depth-of-field is commonly used by photographers to isolate a subject from a distracting background. However, standard cell phone cameras cannot produce such images optically, as their short focal lengths and small apertures capture nearly all-in-focus images. We present a system to computationally synthesize shallow depth-of-field images with a single mobile camera and a single button press. If the image is of a person, we use a person segmentation network to separate the person and their accessories from the background. If available, we also use dense dual-pixel auto-focus hardware, effectively a 2-sample light field with an approximately 1 millimeter baseline, to compute a dense depth map. These two signals are combined and used to render a defocused image. Our system can process a 5.4 megapixel image in 4 seconds on a mobile phone, is fully automatic, and is robust enough to be used by non-experts. The modular nature of our system allows it to degrade naturally in the absence of a dual-pixel sensor or a human subject.},
	number = {4},
	urldate = {2020-10-24},
	journal = {ACM Transactions on Graphics},
	author = {Wadhwa, Neal and Garg, Rahul and Jacobs, David E. and Feldman, Bryan E. and Kanazawa, Nori and Carroll, Robert and Movshovitz-Attias, Yair and Barron, Jonathan T. and Pritch, Yael and Levoy, Marc},
	month = jul,
	year = {2018},
	keywords = {defocus, depth-of-field, segmentation, stereo},
	pages = {64:1--64:13},
}

@inproceedings{ros_synthia_2016,
	title = {The {SYNTHIA} {Dataset}: {A} {Large} {Collection} of {Synthetic} {Images} for {Semantic} {Segmentation} of {Urban} {Scenes}},
	shorttitle = {The {SYNTHIA} {Dataset}},
	doi = {10.1109/CVPR.2016.352},
	abstract = {Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images, thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation - in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ros, G. and Sellart, L. and Materzynska, J. and Vazquez, D. and Lopez, A. M.},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Computer vision, Context, DCNN, Image segmentation, SYNTHIA dataset, Semantics, Training, Urban areas, Visualization, autonomous driving, class annotations, deep convolutional neural networks, image annotation, image segmentation, neural nets, pixel-level annotations, semantic urban scene segmentation, synthetic image collection, vision-based semantic segmentation},
	pages = {3234--3243},
}

@misc{bowles_how_2020,
	title = {How {Athena} {Security} found a brand new market in the midst of the {COVID}-19 pandemic},
	url = {https://diginomica.com/how-athena-security-found-brand-new-market-midst-covid-19-pandemic},
	abstract = {In the face of the lingering pandemic, the key to success-even survival-for many businesses is the ability to quickly respond and adapt to new opportunities.},
	language = {en},
	urldate = {2020-09-30},
	journal = {Diginomica},
	author = {Bowles, Jerry},
	month = aug,
	year = {2020},
	note = {Section: Governing identity privacy and security},
}

@article{schneider_past_2019,
	title = {Past, present and future approaches using computer vision for animal re-identification from camera trap data},
	volume = {10},
	copyright = {© 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society},
	issn = {2041-210X},
	url = {http://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13133},
	doi = {10.1111/2041-210X.13133},
	abstract = {The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is fundamental for addressing a broad range of questions in the study of ecosystem function, community and population dynamics and behavioural ecology. Tagging animals during mark and recapture studies is the most common method for reliable animal re-ID; however, camera traps are a desirable alternative, requiring less labour, much less intrusion and prolonged and continuous monitoring into an environment. Despite these advantages, the analyses of camera traps and video for re-ID by humans are criticized for their biases related to human judgement and inconsistencies between analyses. In this review, we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses and our predictions for near future methodologies based on the rapid development of deep learning methods. For decades, ecologists with expertise in computer vision have successfully utilized feature engineering to extract meaningful features from camera trap images to improve the statistical rigor of individual comparisons and remove human bias from their camera trap analyses. Recent years have witnessed the emergence of deep learning systems which have demonstrated the accurate re-ID of humans based on image and video data with near perfect accuracy. Despite this success, ecologists have yet to utilize these approaches for animal re-ID. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to reidentify individuals that exit and re-enter the camera frame. Our expectation is that this is just the beginning of a major trend that could stand to revolutionize the analysis of camera trap data and, ultimately, our approach to animal ecology.},
	language = {en},
	number = {4},
	urldate = {2020-09-29},
	journal = {Methods in Ecology and Evolution},
	author = {Schneider, Stefan and Taylor, Graham W. and Linquist, Stefan and Kremer, Stefan C.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13133},
	keywords = {animal reidentification, camera traps, computer vision, convolutional networks, deep learning, density estimation, monitoring, object detection},
	pages = {461--470},
}

@article{carl_automated_2020,
	title = {Automated detection of {European} wild mammal species in camera trap images with an existing and pre-trained computer vision model},
	volume = {66},
	issn = {1439-0574},
	url = {https://doi.org/10.1007/s10344-020-01404-y},
	doi = {10.1007/s10344-020-01404-y},
	abstract = {The use of camera traps is a nonintrusive monitoring method to obtain valuable information about the appearance and behavior of wild animals. However, each study generates thousands of pictures and extracting information remains mostly an expensive, time-consuming manual task. Nevertheless, image recognition and analyzing technologies combined with machine learning algorithms, particularly deep learning models, improve and speed up the analysis process. Therefore, we tested the usability of a pre-trained deep learning model available on the TensorFlow hub–FasterRCNN+InceptionResNet V2 network applied to images of ten different European wild mammal species such as wild boar (Sus scrofa), roe deer (Capreolus capreolus), or red fox (Vulpes vulpes) in color as well as black and white infrared images. We found that the detection rate of the correct region of interest (region of the animal) was 94\%. The classification accuracy was 71\% for the correct species’ name as mammals and 93\% for the correct species or higher taxonomic ranks such as “carnivore” as order. In 7\% of cases, the classification was incorrect as the wrong species’ name was classified. In this technical note, we have shown the potential of an existing and pre-trained image classification model for wildlife animal detection, classification, and analysis. A specific training of the model on European wild mammal species could further increase the detection and classification accuracy of the models. Analysis of camera trap images could thus become considerably faster, less expensive, and more efficient.},
	language = {en},
	number = {4},
	urldate = {2020-09-29},
	journal = {European Journal of Wildlife Research},
	author = {Carl, Christin and Schönfeld, Fiona and Profft, Ingolf and Klamm, Alisa and Landgraf, Dirk},
	month = jul,
	year = {2020},
	pages = {62},
}

@misc{magill_methane_2020,
	title = {Methane {Detection}: {Shell} {Expands} {Drone}-{Based} {Program} in the {Permian} {Basin}},
	shorttitle = {Methane {Detection}},
	url = {https://dronelife.com/2020/09/22/methane-detection-shell-expands-drone-based-program-in-the-permian-basin/},
	abstract = {By DRONELIFE staff writer Jim Magill Following a successful two-year pilot program, international oil and gas company Shell will employ a small fleet of drones to fly over more than 500 oil and gas si},
	language = {en-US},
	urldate = {2020-09-29},
	journal = {DRONELIFE},
	author = {Magill, Jim},
	month = sep,
	year = {2020},
}

@misc{business_wire_sixgill_2020,
	title = {Sixgill and {CleanConnect}.ai {Partner} to {Shape} the {AI}-{Powered} {Future} of the {Oil} and {Gas} {Industry}},
	url = {https://venturebeat.com/2020/09/02/sixgill-and-cleanconnect-ai-partner-to-shape-the-ai-powered-future-of-the-oil-and-gas-industry/},
	abstract = {Press Release Sixgill, LLC, a leading AI IoT platform provider for unified end-to-end machine learning lifecycle management, announced a partnership with Colorado-based CleanConnect.ai to deliver a suite of AI solutions to the oil and gas industry. Sixgill's Sense...},
	language = {en-US},
	urldate = {2020-09-29},
	journal = {VentureBeat},
	author = {Business Wire},
	month = sep,
	year = {2020},
}

@misc{hart_energy_news_drager_nodate,
	title = {Dräger {Safety} {Invests} in {Kuva} {Systems} for {Methane} {Monitoring} {Solution} {\textbar} {Hart} {Energy}},
	url = {https://www.hartenergy.com/news/drager-safety-invests-kuva-systems-methane-monitoring-solution-189700},
	abstract = {Dräger Safety AG \& Co. KGaA on Sept. 15 said that after an initial investment in the company in 2018, it has now made a follow-on equity investment in Kuva Systems.},
	language = {en},
	urldate = {2020-09-29},
	author = {Hart Energy News},
	note = {Section: Energy Industry},
}

@misc{kallanish_energy_news_doe_nodate,
	title = {{DOE} awards \${24M} to battery, methane detection projects – {Kallanish} {Energy} {News}},
	url = {https://www.kallanishenergy.com/2020/09/21/doe-awards-24m-to-battery-methane-detection-projects/},
	language = {en-US},
	urldate = {2020-09-29},
	author = {Kallanish Energy News},
}

@article{ravikumar_good_2018,
	title = {“{Good} versus {Good} {Enough}?” {Empirical} {Tests} of {Methane} {Leak} {Detection} {Sensitivity} of a {Commercial} {Infrared} {Camera}},
	volume = {52},
	issn = {0013-936X},
	shorttitle = {“{Good} versus {Good} {Enough}?},
	url = {https://doi.org/10.1021/acs.est.7b04945},
	doi = {10.1021/acs.est.7b04945},
	abstract = {Methane, a key component of natural gas, is a potent greenhouse gas. A key feature of recent methane mitigation policies is the use of periodic leak detection surveys, typically done with optical gas imaging (OGI) technologies. The most common OGI technology is an infrared camera. In this work, we experimentally develop detection probability curves for OGI-based methane leak detection under different environmental and imaging conditions. Controlled single blind leak detection tests show that the median detection limit (50\% detection likelihood) for FLIR-camera based OGI technology is about 20 g CH4/h at an imaging distance of 6 m, an order of magnitude higher than previously reported estimates of 1.4 g CH4/h. Furthermore, we show that median and 90\% detection likelihood limit follows a power-law relationship with imaging distance. Finally, we demonstrate that real-world marginal effectiveness of methane mitigation through periodic surveys approaches zero as leak detection sensitivity improves. For example, a median detection limit of 100 g CH4/h is sufficient to detect the maximum amount of leakage that is possible through periodic surveys. Policy makers should take note of these limits while designing equivalence metrics for next-generation leak detection technologies that can trade sensitivity for cost without affecting mitigation priorities.},
	number = {4},
	urldate = {2020-09-29},
	journal = {Environmental Science \& Technology},
	author = {Ravikumar, Arvind P. and Wang, Jingfan and McGuire, Mike and Bell, Clay S. and Zimmerle, Daniel and Brandt, Adam R.},
	month = feb,
	year = {2018},
	note = {Publisher: American Chemical Society},
	pages = {2368--2374},
}

@misc{magill_energy_nodate,
	title = {Energy {Companies}, {Others} {Use} {Expanding} {Range} {Of} {Tools} {To} {Detect}, {Control} {Methane}},
	url = {https://www.forbes.com/sites/jimmagill/2020/09/24/energy-companies-others-use-expanding-range-of-tools-to-detect-control-methane/},
	abstract = {The development of sophisticated continuous monitoring equipment, and the resultant decrease in cost of that equipment, is giving operators the ability to survey the air quality across entire oil fields, pinpointing the locations of any leaks that do occur.},
	language = {en},
	urldate = {2020-09-29},
	journal = {Forbes},
	author = {Magill, Jim},
	note = {Section: Business},
}

@misc{us_energy_information_administration_natural_nodate,
	title = {Natural gas pipelines - {U}.{S}. {Energy} {Information} {Administration} ({EIA})},
	url = {https://www.eia.gov/energyexplained/natural-gas/natural-gas-pipelines.php},
	urldate = {2020-09-29},
	journal = {U.S. Energy Information Administration {\textbar} Independent Statistics \& Analysis},
	author = {US Energy Information Administration},
}

@misc{crowe_swris_2020,
	title = {{SwRI}’s {SLED}-{W} algorithms detect crude oil on water},
	url = {https://www.swri.org/press-release/sled-w-algorithms-machine-vision-oil-leak-detection},
	abstract = {Southwest Research Institute has developed computer-based techniques to accurately detect crude oil on water using inexpensive thermal and visible cameras. This machine learning-based solution can detect and monitor oil leaks before they become major threats to lakes, rivers and coastal areas.},
	language = {en},
	urldate = {2020-09-29},
	journal = {Southwest Research Institute},
	author = {Crowe, Robert},
	month = jun,
	year = {2020},
}

@misc{crowe_swri_2016,
	title = {{SwRI} developing methane leak detection system for {DOE}},
	url = {https://www.swri.org/press-release/swri-developing-methane-leak-detection-system-doe},
	abstract = {Technology pinpoints small leaks of greenhouse gas that typically go unnoticed},
	language = {en},
	urldate = {2020-09-29},
	journal = {Southwest Research Institute},
	author = {Crowe, Robert},
	month = oct,
	year = {2016},
}

@article{meng_video-based_2020,
	title = {Video-{Based} {Vehicle} {Counting} for {Expressway}: {A} {Novel} {Approach} {Based} on {Vehicle} {Detection} and {Correlation}-{Matched} {Tracking} {Using} {Image} {Data} from {PTZ} {Cameras}},
	volume = {2020},
	shorttitle = {Video-{Based} {Vehicle} {Counting} for {Expressway}},
	url = {https://www.hindawi.com/journals/mpe/2020/1969408/?gclid=EAIaIQobChMIwPu5t4qs3AIVAQAAAB0BAAAAEAAYACAAEgJVzfD_BwE},
	abstract = {Vehicle counting plays a significant role in vehicle behavior analysis and traffic incident detection for established video surveillance systems on expressway. Since the existing sensor method and the traditional image processing method have the problems of difficulty in installation, high cost, and low precision, a novel vehicle counting method is proposed, which realizes efficient counting based on multivehicle detection and multivehicle tracking. For multivehicle detection tasks, a construction of the new expressway dataset consists of a large number of sample images with a high resolution (1920 × 1080) captured from real-world expressway scenes (including the diversity climatic conditions and visual angles) by Pan-Tilt-Zoom (PTZ) cameras, in which vehicle categories and annotation rules are defined. Moreover, a correlation-matched algorithm for multivehicle tracking is proposed, which solves the problem of occlusion and vehicle scale change in the tracking process. Due to the discontinuity and unsmooth of the trajectories that occurred during the tracking process, we designed a trajectory optimization algorithm based on least square method. Finally, a new vehicle counting method is designed based on the tracking results, in which the driving direction information of the vehicle is added in the counting process. The experimental results show that the proposed counting method in this research can achieve more than 93\% accuracy and an average speed of 25 frames per second in expressway video sequence.},
	language = {en},
	urldate = {2020-09-29},
	journal = {Mathematical Problems in Engineering},
	author = {Meng, Qiao and Song, Huansheng and Zhang, Yu’an and Zhang, Xiangqing and Li, Gang and Yang, Yanni},
	month = mar,
	year = {2020},
	doi = {https://doi.org/10.1155/2020/1969408},
	doi = {https://doi.org/10.1155/2020/1969408},
	note = {ISSN: 1024-123X
Pages: e1969408
Publisher: Hindawi
Volume: 2020},
}

@article{yaghoobi_ershadi_vehicle_2017,
	title = {Vehicle {Tracking} and {Counting} {System} in {Dusty} {Weather} with {Vibrating} {Camera} {Conditions}},
	volume = {2017},
	url = {https://www.hindawi.com/journals/js/2017/3812301/},
	abstract = {Traffic surveillance systems are interesting to many researchers to improve the traffic control and reduce the risk caused by accidents. In this area, many published works are only concerned about vehicle detection in normal conditions. The camera may vibrate due to wind or bridge movement. Detection and tracking of vehicles are a very difficult task when we have bad weather conditions in winter (snowy, rainy, windy, etc.) or dusty weather in arid and semiarid regions or at night, among others. In this paper, we proposed a method to track and count vehicles in dusty weather with a vibrating camera. For this purpose, we used a background subtraction based strategy mixed with extra processing to segment vehicles. In this paper, the extra processing included the analysis of the headlight size, location, and area. In our work, tracking was done between consecutive frames via a particle filter to detect the vehicle and pair the headlights using the connected component analysis. So, vehicle counting was performed based on the pairing result. Our proposed method was tested on several video surveillance records in different conditions such as in dusty or foggy weather, with a vibrating camera, and on roads with medium-level traffic volumes. The results showed that the proposed method performed better than other previously published methods, including the Kalman filter or Gaussian model, in different traffic conditions.},
	language = {en},
	urldate = {2020-09-29},
	journal = {Journal of Sensors},
	author = {Yaghoobi Ershadi, Nastaran and Menéndez, José Manuel},
	month = aug,
	year = {2017},
	doi = {https://doi.org/10.1155/2017/3812301},
	doi = {https://doi.org/10.1155/2017/3812301},
	note = {ISSN: 1687-725X
Pages: e3812301
Publisher: Hindawi
Volume: 2017},
}

@inproceedings{schneider_deep_2018,
	title = {Deep {Learning} {Object} {Detection} {Methods} for {Ecological} {Camera} {Trap} {Data}},
	doi = {10.1109/CRV.2018.00052},
	abstract = {Deep learning methods for computer vision tasks show promise for automating the data analysis of camera trap images. Ecological camera traps are a common approach for monitoring an ecosystem's animal population, as they provide continual insight into an environment without being intrusive. However, the analysis of camera trap images is expensive, labour intensive, and time consuming. Recent advances in the field of deep learning for object detection show promise towards automating the analysis of camera trap images. Here, we demonstrate their capabilities by training and comparing two deep learning object detection classifiers, Faster R-CNN and YOLO v2.0, to identify, quantify, and localize animal species within camera trap images using the Reconyx Camera Trap and the self-labeled Gold Standard Snapshot Serengeti data sets. When trained on large labeled datasets, object recognition methods have shown success. We demonstrate their use, in the context of realistically sized ecological data sets, by testing if object detection methods are applicable for ecological research scenarios when utilizing transfer learning. Faster R-CNN outperformed YOLO v2.0 with average accuracies of 93.0\% and 76.7\% on the two data sets, respectively. Our findings show promising steps towards the automation of the labourious task of labeling camera trap images, which can be used to improve our understanding of the population dynamics of ecosystems across the planet.},
	booktitle = {2018 15th {Conference} on {Computer} and {Robot} {Vision} ({CRV})},
	author = {Schneider, Stefan and Taylor, Graham W. and Kremer, Stefan},
	month = may,
	year = {2018},
	keywords = {Animals, Camera Trap, Cameras, Computer vision, Convolutional Neural Network, Deep Learning, Ecology, Faster R CNN, Gold Standard Snapshot Serengeti data sets, Object Detector, Object detection, Reconyx Camera Trap, Snapshot Serengeti, Sociology, Task analysis, Transfer Learning, YOLO, YOLO v2.0, animal species, biological techniques, biology computing, camera trap images, cameras, computer vision, data analysis, deep learning object detection classifiers, ecological Camera Trap data, ecology, faster R-CNN, image classification, learning (artificial intelligence), object detection, object recognition, object recognition methods, realistically sized ecological data sets, recurrent neural nets, transfer learning, zoology},
	pages = {321--328},
}

@inproceedings{fang_recurrent_2018,
	title = {Recurrent {Autoregressive} {Networks} for {Online} {Multi}-object {Tracking}},
	doi = {10.1109/WACV.2018.00057},
	abstract = {The main challenge of online multi-object tracking is to reliably associate object trajectories with detections in each video frame based on their tracking history. In this work, we propose the Recurrent Autoregressive Network (RAN), a temporal generative modeling framework to characterize the appearance and motion dynamics of multiple objects over time. The RAN couples an external memory and an internal memory. The external memory explicitly stores previous inputs of each trajectory in a time window, while the internal memory learns to summarize long-term tracking history and associate detections by processing the external memory. We conduct experiments on the MOT 2015 and 2016 datasets to demonstrate the robustness of our tracking method in highly crowded and occluded scenes. Our method achieves top-ranked results on the two benchmarks.},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Fang, Kuan and Xiang, Yu and Li, Xiaocheng and Savarese, Silvio},
	month = mar,
	year = {2018},
	keywords = {Computational modeling, Data models, Feature extraction, MOT 2015 datasets, MOT 2016 datasets, RAN, Recurrent Autoregressive networks, Target tracking, Training, Trajectory, associate object trajectories, autoregressive processes, external memory, highly crowded scenes, highly occluded scenes, internal memory, learning (artificial intelligence), long-term tracking history, object detection, object tracking, online multiobject tracking, recurrent neural nets, temporal generative modeling framework, video frame, video signal processing},
	pages = {466--475},
}

@inproceedings{fassold_omnitrack_2019,
	title = {{OmniTrack}: {Real}-{Time} {Detection} and {Tracking} of {Objects}, {Text} and {Logos} in {Video}},
	shorttitle = {{OmniTrack}},
	doi = {10.1109/ISM46123.2019.00057},
	abstract = {The automatic detection and tracking of general objects (like persons, animals or cars), text and logos in a video is crucial for many video understanding tasks, and usually real-time processing as required. We propose OmniTrack, an efficient and robust algorithm which is able to automatically detect and track objects, text as well as brand logos in real-time. It combines a powerful deep learning based object detector (YoloV3) with high-quality optical flow methods. Based on the reference YoloV3 C++ implementation, we did some important performance optimizations which will be described.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Multimedia} ({ISM})},
	author = {Fassold, Hannes and Ghermi, Ridouane},
	month = dec,
	year = {2019},
	keywords = {OmniTrack, YoloV3 C++ implementation, automatic detection, brand logos, cars, deep learning based object detector, image sequences, learning (artificial intelligence), logos detection, object detection, object detection, tracking, YoloV3, optical flow, object tracking, objects tracking, optical flow methods, optimisation, optimizations, real-time processing, text detection, video signal processing, video understanding tasks},
	pages = {245--2451},
}

@inproceedings{figurnov_spatially_2017,
	title = {Spatially {Adaptive} {Computation} {Time} for {Residual} {Networks}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Figurnov_Spatially_Adaptive_Computation_CVPR_2017_paper.html},
	urldate = {2020-06-26},
	author = {Figurnov, Michael and Collins, Maxwell D. and Zhu, Yukun and Zhang, Li and Huang, Jonathan and Vetrov, Dmitry and Salakhutdinov, Ruslan},
	year = {2017},
	pages = {1039--1048},
}

@article{barber_quickhull_1996,
	title = {The quickhull algorithm for convex hulls},
	volume = {22},
	issn = {0098-3500},
	url = {http://doi.org/10.1145/235815.235821},
	doi = {10.1145/235815.235821},
	abstract = {The convex hull of a set of points is the smallest convex set that contains the points. This article presents a practical convex hull algorithm that combines the two-dimensional Quickhull algorithm with the general-dimension Beneath-Beyond Algorithm. It is similar to the randomized, incremental algorithms for convex hull and delaunay triangulation. We provide empirical evidence that the algorithm runs faster when the input contains nonextreme points and that it used less memory. computational geometry algorithms have traditionally assumed that input sets are well behaved. When an algorithm is implemented with floating-point arithmetic, this assumption can lead to serous errors. We briefly describe a solution to this problem when computing the convex hull in two, three, or four dimensions. The output is a set of “thick” facets that contain all possible exact convex hulls of the input. A variation is effective in five or more dimensions.},
	number = {4},
	urldate = {2020-05-27},
	journal = {ACM Transactions on Mathematical Software},
	author = {Barber, C. Bradford and Dobkin, David P. and Huhdanpaa, Hannu},
	month = dec,
	year = {1996},
	keywords = {Delaunay triangulation, Voronoi diagram, convex hull, halfspace intersection},
	pages = {469--483},
}

@article{melkman_-line_1987,
	title = {On-line construction of the convex hull of a simple polyline},
	volume = {25},
	issn = {0020-0190},
	url = {http://www.sciencedirect.com/science/article/pii/002001908790086X},
	doi = {10.1016/0020-0190(87)90086-X},
	language = {en},
	number = {1},
	urldate = {2020-05-27},
	journal = {Information Processing Letters},
	author = {Melkman, Avraham A.},
	month = apr,
	year = {1987},
	keywords = {Convex hull, analysis of algorithms, simple polygon},
	pages = {11--12},
}

@article{sklansky_finding_1982,
	title = {Finding the convex hull of a simple polygon},
	volume = {1},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/0167865582900162},
	doi = {10.1016/0167-8655(82)90016-2},
	abstract = {We describe a new algorithm for finding the convex hull of any simple polygon specified by a sequence of m vertices. An earlier convex hull finder of ours is limited to polygons which remain simple (i.e., nonselfintersecting) when locally non-convex vertices are removed. In this paper we amend our earlier algorithm so that it finds with complexity O(m) the convex hull of any simple polygon, while retaining much of the simplicity of the earlier algorithm.},
	language = {en},
	number = {2},
	urldate = {2020-05-27},
	journal = {Pattern Recognition Letters},
	author = {Sklansky, Jack},
	month = dec,
	year = {1982},
	keywords = {Pattern recognition, algorithmic complexity, computational geometry, convex hull, simple polygon},
	pages = {79--83},
}

@article{azulay_why_2019,
	title = {Why do deep convolutional networks generalize so poorly to small image transformations?},
	abstract = {Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network's prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.},
	urldate = {2020-02-17},
	journal = {Journal of Machine Learning Research},
	author = {Azulay, Aharon and Weiss, Yair},
	month = dec,
	year = {2019},
	note = {arXiv: 1805.12177},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lenc_understanding_2015,
	title = {Understanding image representations by measuring their equivariance and equivalence},
	volume = {127},
	abstract = {Despite the importance of image representations such as histograms of oriented gradients and deep Convolutional Neural Networks (CNN), our theoretical understanding of them remains limited. Aiming at filling this gap, we investigate three key mathematical properties of representations: equivariance, invariance, and equivalence. Equivariance studies how transformations of the input image are encoded by the representation, invariance being a special case where a transformation has no effect. Equivalence studies whether two representations, for example two different parametrisations of a CNN, capture the same visual information or not. A number of methods to establish these properties empirically are proposed, including introducing transformation and stitching layers in CNNs. These methods are then applied to popular representations to reveal insightful aspects of their structure, including clarifying at which layers in a CNN certain geometric invariances are achieved. While the focus of the paper is theoretical, direct applications to structured-output regression are demonstrated too.},
	urldate = {2020-02-17},
	journal = {International Journal of Computer Vision},
	author = {Lenc, Karel and Vedaldi, Andrea},
	month = jun,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2020-02-17},
	booktitle = {2015 {International} {Conference} on {Learning} {Representations}},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{ren_dn-resnet_2018,
	title = {{DN}-{ResNet}: {Efficient} {Deep} {Residual} {Network} for {Image} {Denoising}},
	shorttitle = {{DN}-{ResNet}},
	url = {http://arxiv.org/abs/1810.06766},
	abstract = {A deep learning approach to blind denoising of images without complete knowledge of the noise statistics is considered. We propose DN-ResNet, which is a deep convolutional neural network (CNN) consisting of several residual blocks (ResBlocks). With cascade training, DN-ResNet is more accurate and more computationally efficient than the state of art denoising networks. An edge-aware loss function is further utilized in training DN-ResNet, so that the denoising results have better perceptive quality compared to conventional loss function. Next, we introduce the depthwise separable DN-ResNet (DS-DN-ResNet) utilizing the proposed Depthwise Seperable ResBlock (DS-ResBlock) instead of standard ResBlock, which has much less computational cost. DS-DN-ResNet is incrementally evolved by replacing the ResBlocks in DN-ResNet by DS-ResBlocks stage by stage. As a result, high accuracy and good computational efficiency are achieved concurrently. Whereas previous state of art deep learning methods focused on denoising either Gaussian or Poisson corrupted images, we consider denoising images having the more practical Poisson with additive Gaussian noise as well. The results show that DN-ResNets are more efficient, robust, and perform better denoising than current state of art deep learning methods, as well as the popular variants of the BM3D algorithm, in cases of blind and non-blind denoising of images corrupted with Poisson, Gaussian or Poisson-Gaussian noise. Our network also works well for other image enhancement task such as compressed image restoration.},
	urldate = {2020-05-06},
	booktitle = {Asian {Conference} of {Computer} {Vision} 2018},
	author = {Ren, Haoyu and El-Khamy, Mostafa and Lee, Jungwon},
	month = oct,
	year = {2018},
	keywords = {Electrical Engineering And Systems Science - Image And Video Processing ; Computer Science - Computer Vision And Pattern Recognition},
}

@inproceedings{lakshmanan_surfi_2019,
	address = {Miami, Florida},
	series = {{WiSec} '19},
	title = {{SurFi}: detecting surveillance camera looping attacks with wi-fi channel state information},
	isbn = {978-1-4503-6726-4},
	shorttitle = {{SurFi}},
	url = {https://doi.org/10.1145/3317549.3324928},
	doi = {10.1145/3317549.3324928},
	abstract = {The proliferation of surveillance cameras has greatly improved the physical security of many security-critical properties including buildings, stores, and homes. However, recent surveillance camera looping attacks demonstrate new security threats --- adversaries can replay a seemingly benign video feed of a place of interest while trespassing or stealing valuables without getting caught. Unfortunately, such attacks are extremely difficult to detect in real-time due to cost and implementation constraints. In this paper, we propose SurFi to detect these attacks in real-time by utilizing commonly available Wi-Fi signals. In particular, we leverage that channel state information (CSI) from Wi-Fi signals also perceives human activities in the place of interest in addition to surveillance cameras. SurFi processes and correlates the live video feeds and the Wi-Fi CSI signals to detect any mismatches that would identify the presence of the surveillance camera looping attacks. SurFi does not require the deployment of additional infrastructure because Wi-Fi transceivers are easily found in the urban indoor environment. We design and implement the SurFi system and evaluate its effectiveness in detecting surveillance camera looping attacks. Our evaluation demonstrates that SurFi effectively identifies attacks with up to an attack detection accuracy of 98.8\% and 0.1\% false positive rate.},
	urldate = {2020-04-29},
	booktitle = {Proceedings of the 12th {Conference} on {Security} and {Privacy} in {Wireless} and {Mobile} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Lakshmanan, Nitya and Bang, Inkyu and Kang, Min Suk and Han, Jun and Lee, Jong Taek},
	month = may,
	year = {2019},
	keywords = {CSI, surveillance video, wi-fi},
	pages = {239--244},
}

@inproceedings{buades_non-local_2005,
	title = {A non-local algorithm for image denoising},
	volume = {2},
	doi = {10.1109/CVPR.2005.38},
	abstract = {We propose a new measure, the method noise, to evaluate and compare the performance of digital image denoising methods. We first compute and analyze this method noise for a wide class of denoising algorithms, namely the local smoothing filters. Second, we propose a new algorithm, the nonlocal means (NL-means), based on a nonlocal averaging of all pixels in the image. Finally, we present some experiments comparing the NL-means algorithm and the local smoothing filters.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Buades, A. and Coll, B. and Morel, J.-M.},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {Algorithm design and analysis, Digital images, Filtering, Image denoising, NL-means algorithm, Noise measurement, Noise reduction, Pixel, Smoothing methods, White noise, Wiener filter, digital image denoising method, image denoising, image resolution, local smoothing filters, nonlocal algorithm, smoothing methods},
	pages = {60--65 vol. 2},
}

@incollection{butterfield_gaussian_2018,
	title = {Gaussian filter},
	isbn = {978-0-19-872572-5},
	url = {http://www.oxfordreference.com/view/10.1093/acref/9780198725725.001.0001/acref-9780198725725-e-1942},
	abstract = {"Gaussian filter" published on  by Oxford University Press.},
	language = {en},
	urldate = {2020-04-20},
	booktitle = {A {Dictionary} of {Electronics} and {Electrical} {Engineering}},
	publisher = {Oxford University Press},
	author = {Butterfield, Andrew and Szymanski, John},
	month = jun,
	year = {2018},
}

@inproceedings{zhang_fast_2016,
	title = {Fast depth image denoising and enhancement using a deep convolutional network},
	doi = {10.1109/ICASSP.2016.7472127},
	abstract = {We propose a depth image denoising and enhancement framework using a light convolutional network. The network contains three layers for high dimension projection, missing data completion and image reconstruction. We jointly use both depth and visual images as inputs. For the gray image, we design a pre-processing procedure to enhance the edges and remove unnecessary detail. For the depth image, we propose a data augmentation strategy to regenerate and increase essential training data. Further, we propose a weighted loss function for network training to adaptively improve the learning efficiency. We tested our algorithm on benchmark data and obtained very promising visual and quantitative results at real-time speed.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zhang, Xin and Wu, Ruiyuan},
	month = mar,
	year = {2016},
	note = {ISSN: 2379-190X},
	keywords = {Color, Image edge detection, Image reconstruction, Image segmentation, Noise reduction, Training, Visualization, benchmark data, data augmentation, deep convolutional network, depth image denoise, depth image enhancement, dimension projection, fast depth image denoising, gray image, image denoising, image enhancement, image reconstruction, light convolutional network, missing data completion},
	pages = {2499--2503},
}

@inproceedings{shicao_luo_multistage_2015,
	title = {Multistage committees of deep feedforward convolutional sparse denoise autoencoder for object recognition},
	doi = {10.1109/CAC.2015.7382564},
	abstract = {Deep learning and unsupervised feature learning systems are known to achieve good performance in benchmarks by using extremely large architectures with many features at each layer. However, we found that the number of features' contribution to performance is very small when it is more than the threshold. Meanwhile, the size of pooling layer has an important influence on performance. In this paper, we present an unsupervised method to improve the classification result by going deep and combining multistage classifiers in a committee with a small amount of features at each layer. The network is trained layer-wise via denoise autoencoder (dA) with L-BFGS to optimize convolutional kernels and no backpropagation is used. In addition, we regularize the dA encouraging representations to fit sparse for each coding layer. We apply it on the STL-10 dataset which has very few training examples and a large amount of unlabeled data. Experimental results show that our method presents higher performance than the existing ones on the condition via individual network.},
	booktitle = {2015 {Chinese} {Automation} {Congress} ({CAC})},
	author = {Shicao Luo and Yongsheng Ding and Kuangrong Hao},
	month = nov,
	year = {2015},
	keywords = {Convolution, Encoding, Feature extraction, Kernel, L-BFGS, Noise reduction, Robustness, STL-10 dataset, Training, coding layer, convolutional kernel optimization, dA, deep feedforward convolutional sparse denoise autoencoder, deep learning, feedforward neural nets, image classification, image coding, image denoising, multistage classifiers, multistage committees, object recognition, pooling layer, sparse denoise autoencoder, unsupervised feature learning systems, unsupervised learning},
	pages = {565--570},
}

@misc{noauthor_sharpen_2018,
	title = {Sharpen {Footage} {Using} the {Unsharp} {Mask} {Effect} in {Premiere} {Pro}},
	url = {https://motionarray.com/learn/premiere-pro/unsharp-mask-premiere-pro/},
	abstract = {In this tutorial, you'll learn how to sharpen your footage properly using the unsharp mask effect in Premiere Pro CC!},
	language = {en-US},
	urldate = {2020-04-17},
	journal = {Motion Array},
	month = dec,
	year = {2018},
	note = {Library Catalog: motionarray.com
Section: Premiere Pro},
}

@article{azner_worldwide_2020,
	title = {Worldwide {Confirmed} {Coronavirus} {Cases} {Top} 2 {Million}},
	url = {https://www.nytimes.com/2020/04/15/world/coronavirus-cases-world.html},
	urldate = {2020-04-16},
	journal = {The New York Times},
	author = {Azner, Jez},
	month = apr,
	year = {2020},
}

@article{harris_white_2020,
	title = {White {House} {Announces} {New} {Social} {Distancing} {Guidelines} {Around} {Coronavirus}},
	url = {https://www.npr.org/2020/03/16/816658125/white-house-announces-new-social-distancing-guidelines-around-coronavirus},
	abstract = {The latest on the U.S. federal government's response to the coronavirus pandemic was revealed in an afternoon briefing from the White House.},
	language = {en},
	urldate = {2020-04-16},
	journal = {NPR},
	author = {Harris, Rochard},
	month = mar,
	year = {2020},
}

@article{feng_thousands_2020,
	title = {With {Thousands} {Sick}, {China} {Tries} {To} {Contain} {Coronavirus}},
	url = {https://www.npr.org/2020/01/28/800350322/chinese-officials-try-hard-to-contain-coronavirus},
	abstract = {More than a dozen cities in the Chinese province of Hubei are under official lock down to try to prevent the further outbreak of a new, deadly virus.},
	language = {en},
	urldate = {2020-04-16},
	journal = {NPR},
	author = {Feng, Emily},
	month = jan,
	year = {2020},
}

@article{nebehay_who_2020,
	title = {{WHO} says new {China} coronavirus could spread, warns hospitals worldwide},
	url = {https://www.reuters.com/article/us-china-health-pneumonia-who-idUSKBN1ZD16J},
	abstract = {There may have been limited human-to-human transmission of a new coronavirus in China within families, and it is possible there could be a wider outbreak, the World Health Organization (WHO) said on Tuesday.},
	language = {en},
	urldate = {2020-04-10},
	journal = {Reuters},
	author = {Nebehay, Stephanie},
	month = jan,
	year = {2020},
	keywords = {Airlines (TRBC), Asia / Pacific, CHINA, China (PRC), Communicable Diseases, Diplomacy / Foreign Policy, Diseases, Europe, HEALTH, Health / Medicine, Healthcare (TRBC), Infectious Diseases, Influenza, Life Sciences, Major News, PNEUMONIA, Pharmaceuticals (TRBC), Regulation, Science, Society / Social Issues, Switzerland, Thailand, Transportation (TRBC), US, WHO, World Health Organization},
}

@article{saavedra_virginia_2020,
	title = {Virginia {Governor} {Ralph} {Northam} {Announces} {Stay}-{At}-{Home} {Order} {Till} {June} 10},
	url = {https://www.dailywire.com/news/breaking-virginia-governor-ralph-northam-announces-stay-at-home-order-till-june-10},
	abstract = {Virginia\&\#8217;s Democrat Governor Ralph Northam announced on Monday that he has issued a statewide \&\#8220;stay-at-home\&\#8221; order that takes effect immediately and lasts until June 10, 2020. \&\#8220;The order directs all Virginians to stay home except in extremely limited circumstances,\&\#8221; Northam\&\#8217;s office said in a statement. \&\#8220;Individuals may leave their residence for allowable travel, including to [\&hellip;]},
	language = {en},
	urldate = {2020-04-16},
	journal = {The Daily Wire},
	author = {Saavedra, Ryan},
	month = mar,
	year = {2020},
}

@article{lovelace_jr_coronavirus_2020,
	title = {The coronavirus may be deadlier than the 1918 flu: {Here}'s how it stacks up to other pandemics},
	shorttitle = {The coronavirus may be deadlier than the 1918 flu},
	url = {https://www.cnbc.com/2020/03/26/coronavirus-may-be-deadlier-than-1918-flu-heres-how-it-stacks-up-to-other-pandemics.html},
	language = {en},
	urldate = {2020-04-16},
	journal = {CNBC},
	author = {Lovelace Jr., Berkeley},
	month = mar,
	year = {2020},
}

@article{griffiths_taiwans_2020,
	title = {Taiwan's coronavirus response is among the best globally},
	url = {https://www.cnn.com/2020/04/04/asia/taiwan-coronavirus-response-who-intl-hnk/index.html},
	abstract = {On January 25, as the world was still waking up to the potential danger of the novel coronavirus spreading rapidly out of central China, two governments recorded four new infections within their territory.},
	urldate = {2020-04-16},
	journal = {CNN},
	author = {Griffiths, James},
	month = apr,
	year = {2020},
}

@article{grant_rebecca_2020,
	title = {Rebecca {Grant}: {Coronavirus} lessons from 1918 {Spanish} flu – here's what worked to save lives},
	shorttitle = {Rebecca {Grant}},
	url = {https://www.foxnews.com/opinion/rebecca-grant-coronavirus-lessons-from-1918-spanish-flu-heres-what-worked-to-save-lives},
	abstract = {Americans fought a similar battle against the Spanish flu epidemic, which claimed 675,000 Americans and between 50 million and 100 million lives around the world.},
	language = {en-US},
	urldate = {2020-04-16},
	journal = {Fox News},
	author = {Grant, Rebecca},
	month = mar,
	year = {2020},
}

@article{barry_opinion_2020,
	chapter = {Opinion},
	title = {Opinion {\textbar} {The} {Single} {Most} {Important} {Lesson} {From} the 1918 {Influenza}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2020/03/17/opinion/coronavirus-1918-spanish-flu.html},
	abstract = {Containment — the attempt to limit spread of a virus and even eliminate it — has failed.},
	language = {en-US},
	urldate = {2020-04-16},
	journal = {The New York Times},
	author = {Barry, John M.},
	month = mar,
	year = {2020},
	keywords = {Centers for Disease Control and Prevention, Coronavirus (2019-nCoV), Epidemics, Food and Drug Administration, Influenza, Quarantines, Trump, Donald J},
}

@article{johnson_joint_2020,
	title = {Joint {Shelter} {Order} {Issued} by the {City} of {Austin}, {Travis} {County}, {Williamson} {County}},
	url = {https://thetexan.news/joint-shelter-order-issued-by-the-city-of-austin-travis-county-williamson-county/},
	abstract = {Local governments in Texas are more frequently issuing shelter orders to prevent the spread of coronavirus.},
	language = {en-US},
	urldate = {2020-04-16},
	journal = {The Texan},
	author = {Johnson, Brad},
	month = mar,
	year = {2020},
}

@article{wallace_italy_2020,
	title = {Italy faces first day of nationwide coronavirus lockdown, {Europe} to discuss spread of epidemic},
	url = {https://www.foxnews.com/world/italy-coronavirus-lockdown-europe-spread},
	urldate = {2020-04-16},
	journal = {Fox News},
	author = {Wallace, Danielle},
	month = mar,
	year = {2020},
}

@article{bois_illinois_2020,
	title = {Illinois {Issues} {Stay}-{At}-{Home} {Order} {Following} {California} {And} {New} {York}},
	url = {https://www.dailywire.com/news/illinois-issues-stay-at-home-order-following-california-and-new-york},
	abstract = {Shutdown fever has gripped America amidst the COVID-19 pandemic, as the states of California, New York, and now Illinois have all issued a stay-at-home order in an effort to \&\#8220;flatten the curve\&\#8221; to prevent a healthcare system collapse.  On Friday, just hours after New York’s Democratic Gov. Andrew Cuomo issued his stay-at-home order for the [\&hellip;]},
	language = {en},
	urldate = {2020-04-16},
	journal = {The Daily Wire},
	author = {Bois, Paul},
	month = mar,
	year = {2020},
}

@article{saavedra_global_2020,
	title = {Global {Emergency} {Declared} {Over} {Coronavirus}; {First} {Transmission} {Confirmed} {In} {U}.{S}.},
	url = {https://www.dailywire.com/news/breaking-global-emergency-declared-over-coronavirus-first-transmission-confirmed-in-u-s},
	abstract = {The World Health Organization (WHO) declared a global emergency over the spread of the deadly coronavirus epidemic, as thousands of new cases have been reported and the death toll closes in on 200 people. \&\#8220;The decision reversed the organization’s decision just a week ago to hold off such a declaration,\&\#8221; The New York Times reported. [\&hellip;]},
	language = {en},
	urldate = {2020-04-16},
	journal = {The Daily Wire},
	author = {Saavedra, Ryan},
	month = jan,
	year = {2020},
}

@article{guilford_second_2020,
	chapter = {Economy},
	title = {A {Second} {Round} of {Coronavirus} {Layoffs} {Has} {Begun}. {Few} {Are} {Safe}.},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/a-second-round-of-coronavirus-layoffs-has-begun-no-one-is-safe-11586872387},
	abstract = {People who thought their jobs were secure, including white-collar professionals, increasingly face unemployment as the pandemic causes economic pain across industries.},
	language = {en-US},
	urldate = {2020-04-16},
	journal = {Wall Street Journal},
	author = {Guilford, Eric},
	month = apr,
	year = {2020},
	keywords = {capacity, corporate, epidemics, facilities, facility closures, general news, health, industrial news, infectious diseases, labor, lay-offs, leder, medical conditions, novel coronaviruses, outbreaks, personnel, political, redundancies, respiratory tract diseases},
}

@article{lucas_5_2020,
	chapter = {International},
	title = {5 {Things} to {Know} {About} {South} {Korea}’s {Response} to {Coronavirus}},
	url = {https://www.dailysignal.com/2020/03/27/5-things-americans-should-know-about-south-koreas-handling-of-coronavirus/},
	abstract = {Along with South Korea’s effective testing for COVID-19 came surveillance and data mining without court warrants.},
	urldate = {2020-04-16},
	journal = {The Daily Signal},
	author = {Lucas, Fred},
	month = mar,
	year = {2020},
}

@misc{noauthor_taiwans_nodate,
	title = {Taiwan's coronavirus response is among the best globally - {CNN}},
	url = {https://www.cnn.com/2020/04/04/asia/taiwan-coronavirus-response-who-intl-hnk/index.html},
	urldate = {2020-04-16},
}

@article{ma_chinas_2020,
	title = {China’s first confirmed {Covid}-19 case traced back to {November} 17},
	url = {https://www.scmp.com/news/china/society/article/3074991/coronavirus-chinas-first-confirmed-covid-19-case-traced-back},
	abstract = {Government records suggest first person infected with new disease may have been a Hubei resident aged 55, but ‘patient zero’ has yet to be confirmed.},
	language = {en},
	urldate = {2020-04-10},
	journal = {South China Morning Post},
	author = {Ma, Josephine},
	month = mar,
	year = {2020},
}

@article{schow_heres_2020,
	title = {Here’s {A} {Timeline} {Of} {The} {Coronavirus} {Outbreak} {And} {China}’s {Coverup}},
	url = {https://www.dailywire.com/news/heres-a-timeline-of-the-coronavirus-outbreak-and-chinas-coverup},
	urldate = {2020-04-10},
	journal = {The Daily Wire},
	author = {Schow, Ashe},
	month = mar,
	year = {2020},
}

@article{taylor_timeline_2020,
	chapter = {World},
	title = {A {Timeline} of the {Coronavirus} {Pandemic}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/article/coronavirus-timeline.html},
	abstract = {The outbreak of the virus, which began in Wuhan, China, has sickened more than a million people. At least 75,000 people have died.},
	language = {en-US},
	urldate = {2020-04-10},
	journal = {The New York Times},
	author = {Taylor, Derrick Bryson},
	month = apr,
	year = {2020},
	keywords = {China, Coronavirus (2019-nCoV), Cruises, Deaths (Fatalities), Hubei Province (China), Infections, Li Wenliang, Princess Diamond, Quarantines, Respiratory Diseases, SARS (Severe Acute Respiratory Syndrome), Travel Warnings, Viruses, World Health Organization, Wuhan (China)},
}

@misc{bhattacharyya_understanding_2019,
	title = {Understanding {Support} {Vector} {Machine}: {Part} 2: {Kernel} {Trick}; {Mercer}’s {Theorem}},
	shorttitle = {Understanding {Support} {Vector} {Machine}},
	url = {https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d},
	abstract = {Why Kernel ?},
	language = {en},
	urldate = {2020-02-18},
	journal = {Medium},
	author = {Bhattacharyya, Saptashwa},
	month = dec,
	year = {2019},
}

@article{su_one_2019,
	title = {One {Pixel} {Attack} for {Fooling} {Deep} {Neural} {Networks}},
	volume = {23},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2019.2890858},
	abstract = {Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97\% of the natural images in Kaggle CIFAR-10 test dataset and 16.04\% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03\% and 22.91\% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
	number = {5},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
	month = oct,
	year = {2019},
	keywords = {Additives, CIFAR-10 dataset, Convolutional neural network, DNN, ILSVRC 2012, Image color analysis, Image recognition, ImageNet test images, Neural networks, Perturbation methods, Robustness, adversarial information, adversarial machine learning, deep neural networks, differential evolution, differential evolution (DE), evolutionary computation, extreme limited scenario, feature extraction, image classification, image recognition, information security, input vector, learning (artificial intelligence), low dimension attacks, low-cost adversarial attacks, natural images, neural nets, object recognition, one-pixel adversarial perturbations, pixel attack},
	pages = {828--841},
}

@inproceedings{du_towards_2018,
	address = {Toronto, Canada},
	series = {{AISec} '18},
	title = {Towards {Query} {Efficient} {Black}-box {Attacks}: {An} {Input}-free {Perspective}},
	isbn = {978-1-4503-6004-3},
	shorttitle = {Towards {Query} {Efficient} {Black}-box {Attacks}},
	url = {http://doi.org/10.1145/3270101.3270106},
	doi = {10.1145/3270101.3270106},
	abstract = {Recent studies have highlighted that deep neural networks (DNNs) are vulnerable to adversarial attacks, even in a black-box scenario. However, most of the existing black-box attack algorithms need to make a huge amount of queries to perform attacks, which is not practical in the real world. We note one of the main reasons for the massive queries is that the adversarial example is required to be visually similar to the original image, but in many cases, how adversarial examples look like does not matter much. It inspires us to introduce a new attack called input-free attack, under which an adversary can choose an arbitrary image to start with and is allowed to add perceptible perturbations on it. Following this approach, we propose two techniques to significantly reduce the query complexity. First, we initialize an adversarial example with a gray color image on which every pixel has roughly the same importance for the target model. Then we shrink the dimension of the attack space by perturbing a small region and tiling it to cover the input image. To make our algorithm more effective, we stabilize a projected gradient ascent algorithm with momentum, and also propose a heuristic approach for region size selection. Recent studies have highlighted that deep neural networks (DNNs) are vulnerable to adversarial attacks, even in a black-box scenario. However, most of the existing black-box attack algorithms need to make a huge amount of queries to perform attacks, which is not practical in the real world. We note one of the main reasons for the massive queries is that the adversarial example is required to be visually similar to the original image, but in many cases, how adversarial examples look like does not matter much. It inspires us to introduce a new attack called input-free attack, under which an adversary can choose an arbitrary image to start with and is allowed to add perceptible perturbations on it. Following this approach, we propose two techniques to significantly reduce the query complexity. First, we initialize an adversarial example with a gray color image on which every pixel has roughly the same importance for the target model. Then we shrink the dimension of the attack space by perturbing a small region and tiling it to cover the input image. To make our algorithm more effective, we stabilize a projected gradient ascent algorithm with momentum, and also propose a heuristic approach for region size selection. Through extensive experiments, we show that with only 1,701 queries on average, we can perturb a gray image to any target class of ImageNet with a 100\% success rate on InceptionV3. Besides, our algorithm has successfully defeated two real-world systems, the Clarifai food detection API and the Baidu Animal Identification API.},
	urldate = {2020-02-17},
	booktitle = {Proceedings of the 11th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Du, Yali and Fang, Meng and Yi, Jinfeng and Cheng, Jun and Tao, Dacheng},
	month = jan,
	year = {2018},
	keywords = {adversarial learning, black-box attack, input-free attack, neural network, region attack},
	pages = {13--24},
}

@article{lu_see_2019,
	title = {See the {World} {Through} {Network} {Cameras}},
	volume = {52},
	issn = {1558-0814},
	doi = {10.1109/MC.2019.2906841},
	abstract = {Millions of network cameras have been deployed worldwide. Real-time data from many network cameras can offer instant views of multiple locations for many applications. We describe the real-time data available from these cameras and potential applications.},
	number = {10},
	journal = {Computer},
	author = {Lu, Yung-Hsiang and Thiruvathukal, George K. and Kaseb, Ahmed S. and Gauen, Kent and Rijhwani, Damini and Dailey, Ryan and Malik, Deeptanshu and Huang, Yutong and Aghajanzadeh, Sara and Guo, Minghao Mina},
	month = oct,
	year = {2019},
	keywords = {Cameras, IP networks, Internet, Real-time systems, Streaming media, Urban areas, Visualization, cameras, network cameras},
	pages = {30--40},
}

@inproceedings{torralba_unbiased_2011,
	title = {Unbiased look at dataset bias},
	doi = {10.1109/CVPR.2011.5995347},
	abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
	booktitle = {{CVPR} 2011},
	author = {Torralba, Antonio and Efros, Alexei A.},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6919},
	keywords = {Communities, Internet, Object recognition, Support vector machines, Testing, Training, Visualization, algorithm evaluation protocols, closed world assumption effects, contemporary object recognition, cross dataset generalization, data capture, object recognition, recognition datasets, relative data bias, sample value, visual databases},
	pages = {1521--1528},
}

@article{zhang_stability_2017,
	title = {On {The} {Stability} of {Video} {Detection} and {Tracking}},
	url = {http://arxiv.org/abs/1611.06467},
	abstract = {In this paper, we study an important yet less explored aspect in video detection and tracking -- stability. Surprisingly, there is no prior work that tried to study it. As a result, we start our work by proposing a novel evaluation metric for video detection which considers both stability and accuracy. For accuracy, we extend the existing accuracy metric mean Average Precision (mAP). For stability, we decompose it into three terms: fragment error, center position error, scale and ratio error. Each error represents one aspect of stability. Furthermore, we demonstrate that the stability metric has low correlation with accuracy metric. Thus, it indeed captures a different perspective of quality. Lastly, based on this metric, we evaluate several existing methods for video detection and show how they affect accuracy and stability. We believe our work can provide guidance and solid baselines for future researches in the related areas.},
	urldate = {2020-02-17},
	journal = {arXiv:1611.06467 [cs]},
	author = {Zhang, Hong and Wang, Naiyan},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.06467},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{oppenheim_discrete-time_1999,
	address = {Upper Saddle River, N.J.},
	edition = {2nd ed.},
	series = {Prentice-{Hall} signal processing series},
	title = {Discrete-time signal processing},
	isbn = {978-0-13-754920-7},
	abstract = {Contents: Discrete-time signals and systems -- The z-transform -- Sampling of continuous-time signals -- Transform analysis of linear time-invariant systems -- Structures for discrete-time systems -- Filter design techniques -- The discrete Fourier transform -- Computation of the discrete Fourier transform -- Fourier analysis of signals using the discrete Fourier transform -- Discrete Hilbert transforms.},
	language = {eng},
	publisher = {Prentice Hall},
	author = {Oppenheim, Alan V.},
	collaborator = {Schafer, Ronald W. and Buck, John R.},
	year = {1999},
	keywords = {Mathematics; Discrete-time systems, Signal processing},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	abstract = {Contents: Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models., Summary: "Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover.},
	language = {eng},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian},
	collaborator = {Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning; Computers and IT; Machine learning; Maschinelles Lernen; Computers and IT; Machine learning},
}

@article{connolly_study_1997,
	title = {A study of efficiency and accuracy in the transformation from {RGB} to {CIELAB} color space},
	volume = {6},
	issn = {1941-0042},
	doi = {10.1109/83.597279},
	abstract = {The perceptually uniform color space CIELAB (Commission Internationale de l'Eclairage) is useful for image analysis, particularly in applications involving color acceptability decision making; however, the transformation of an entire red-green-blue (RGB) color image is very time consuming. Various techniques are investigated for approximating the nonlinear function, and their performance in terms of speed and accuracy is assessed. This article arises from a project that aims to use video cameras to monitor the color of manufactured products.},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Connolly, C. and Fleiss, T.},
	month = jul,
	year = {1997},
	keywords = {Cameras, Commission Internationale de l'Eclairage, Decision making, Equations, Image color analysis, Manufactured products, Manufacturing processes, Monitoring, Production, RGB-CIELAB color space transformation, Table lookup, Voltage, accuracy, approximation theory, color acceptability decision making, color monitoring, efficiency, image analysis, image colour analysis, manufactured products, nonlinear function approximation, perceptually uniform color space, performance, red-green-blue color image, video cameras},
	pages = {1046--1048},
}

@article{buades_non-local_2011,
	title = {Non-{Local} {Means} {Denoising}},
	volume = {1},
	issn = {2105-1232},
	url = {http://www.ipol.im/pub/art/2011/bcm_nlm/},
	doi = {10.5201/ipol.2011.bcm_nlm},
	abstract = {We present in this paper a new denoising method called non-local means. The method is based on a simple principle: replacing the color of a pixel with an average of the colors of similar pixels. But the most similar pixels to a given pixel have no reason to be close at all. It is therefore licit to scan a vast portion of the image in search of all the pixels that really resemble the pixel one wants to denoise. The paper presents two implementations of the method and displays some results.},
	language = {en},
	urldate = {2020-02-17},
	journal = {Image Processing On Line},
	author = {Buades, Antoni and Coll, Bartomeu and Morel, Jean-Michel},
	month = sep,
	year = {2011},
	pages = {208--212},
}

@article{leal-taixe_motchallenge_2015,
	title = {{MOTChallenge} 2015: {Towards} a {Benchmark} for {Multi}-{Target} {Tracking}},
	shorttitle = {{MOTChallenge} 2015},
	url = {http://arxiv.org/abs/1504.01942},
	abstract = {In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset, targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and finally creating a unified evaluation system. With MOTChallenge we aim to pave the way toward a unified evaluation framework for a more meaningful quantification of multi-target tracking.},
	urldate = {2020-02-17},
	journal = {arXiv:1504.01942 [cs]},
	author = {Leal-Taixé, Laura and Milan, Anton and Reid, Ian and Roth, Stefan and Schindler, Konrad},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.01942},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	doi = {10.1109/ICCV.2015.123},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {ISSN: 2380-7504},
	keywords = {Adaptation models, Biological neural networks, Computational modeling, Gaussian distribution, ILSVRC 2014 winner, ImageNet 2012 classification dataset, ImageNet classification, PReLU, Testing, Training, human-level performance, image classification, model fitting, network architectures, neural nets, overfitting risk, parametric rectified linear unit, rectified activation units, rectifier neural networks, rectifier nonlinearities, robust initialization method, state-of-the-art neural networks},
	pages = {1026--1034},
}

@inproceedings{lin_focal_2017,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://openaccess.thecvf.com/content_iccv_2017/html/Lin_Focal_Loss_for_ICCV_2017_paper.html},
	urldate = {2020-02-17},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
	year = {2017},
	pages = {2980--2988},
}

@incollection{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf},
	urldate = {2020-02-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {91--99},
}

@inproceedings{liu_ssd_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	isbn = {978-3-319-46448-0},
	shorttitle = {{SSD}},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300300×300300 {\textbackslash}times 300 input, SSD achieves 74.3 \% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512×512512×512512 {\textbackslash}times 512 input, SSD achieves 76.9 \% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Convolutional neural network, Real-time object detection},
	pages = {21--37},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	urldate = {2020-02-17},
	journal = {arXiv:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhang_making_2019,
	title = {Making {Convolutional} {Networks} {Shift}-{Invariant} {Again}},
	url = {http://proceedings.mlr.press/v97/zhang19a.html},
	abstract = {Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, stride...},
	language = {en},
	urldate = {2020-02-17},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Zhang, Richard},
	month = may,
	year = {2019},
	pages = {7324--7334},
}

@article{paszke_enet_2016,
	title = {{ENet}: {A} {Deep} {Neural} {Network} {Architecture} for {Real}-{Time} {Semantic} {Segmentation}},
	shorttitle = {{ENet}},
	url = {http://arxiv.org/abs/1606.02147},
	abstract = {The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18\${\textbackslash}times\$ faster, requires 75\${\textbackslash}times\$ less FLOPs, has 79\${\textbackslash}times\$ less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.},
	urldate = {2020-01-15},
	journal = {arXiv:1606.02147 [cs]},
	author = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.02147},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{romera_erfnet_2018,
	title = {{ERFNet}: {Efficient} {Residual} {Factorized} {ConvNet} for {Real}-{Time} {Semantic} {Segmentation}},
	volume = {19},
	issn = {1558-0016},
	shorttitle = {{ERFNet}},
	doi = {10.1109/TITS.2017.2750080},
	abstract = {Semantic segmentation is a challenging task that addresses most of the perception needs of intelligent vehicles (IVs) in an unified way. Deep neural networks excel at this task, as they can be trained end-to-end to accurately classify multiple object categories in an image at pixel level. However, a good tradeoff between high quality and computational resources is yet not present in the state-of-the-art semantic segmentation approaches, limiting their application in real vehicles. In this paper, we propose a deep architecture that is able to run in real time while providing accurate semantic segmentation. The core of our architecture is a novel layer that uses residual connections and factorized convolutions in order to remain efficient while retaining remarkable accuracy. Our approach is able to run at over 83 FPS in a single Titan X, and 7 FPS in a Jetson TX1 (embedded device). A comprehensive set of experiments on the publicly available Cityscapes data set demonstrates that our system achieves an accuracy that is similar to the state of the art, while being orders of magnitude faster to compute than other architectures that achieve top precision. The resulting tradeoff makes our model an ideal approach for scene understanding in IV applications. The code is publicly available at: https://github.com/Eromera/erfnet.},
	number = {1},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Romera, Eduardo and Álvarez, José M. and Bergasa, Luis M. and Arroyo, Roberto},
	month = jan,
	year = {2018},
	keywords = {Cityscapes data, Computer architecture, ERFNet, Image segmentation, Intelligent vehicles, Jetson TX1, Kernel, Real-time systems, Semantics, Two dimensional displays, convolution, deep learning, efficient residual factorized ConvNet, image classification, image resolution, image segmentation, intelligent transportation systems, intelligent vehicles, neural nets, neural networks, real-time, residual layers, scene understanding, semantic segmentation},
	pages = {263--272},
}

@inproceedings{yang_unifying_2018,
	title = {Unifying terrain awareness through real-time semantic segmentation},
	doi = {10.1109/IVS.2018.8500506},
	abstract = {Active research on computer vision accelerates the progress in autonomous driving. Following this trend, we aim to leverage the recently emerged methods for Intelligent Vehicles (IV), and transfer them to develop navigation assistive technologies for the Visually Impaired (VI). This topic grows notoriously challenging as it requires to detect a variety of scenes towards higher level of assistance. Computer vision based techniques with monocular detectors or depth sensors sprung up within years of research. These separate approaches achieved remarkable results with relatively low processing time, and improved the mobility of visually impaired people to a large extent. However, running all detectors jointly increases the latency and burdens the computational resources. In this paper, we put forward to seize pixel-wise semantic segmentation to cover the perception needs of navigational assistance in a unified way. This is critical not only for the terrain awareness regarding traversable areas, sidewalks, stairs and water hazards, but also for the avoidance of short-range obstacles, fast-approaching pedestrians and vehicles. At the heart of our proposal is a combination of efficient residual factorized network (ERFNet), pyramid scene parsing network (PSPNet) and 3D point cloud based segmentation. This approach proves to be with qualified accuracy and speed for real-world applications by a comprehensive set of experiments on a wearable navigation system.},
	booktitle = {2018 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Yang, Kailun and Bergasa, Luis M. and Romera, Eduardo and Cheng, Ruiqi and Chen, Tianxue and Wang, Kaiwei},
	month = jun,
	year = {2018},
	note = {ISSN: 1931-0587},
	keywords = {3D point cloud based segmentation, Convolution, Decoding, Image segmentation, Navigation, Real-time systems, Semantics, Task analysis, active research, assisted living, autonomous driving, collision avoidance, computational resources, computer graphics, computer vision, depth sensors, handicapped aids, image motion analysis, image segmentation, intelligent vehicles, monocular detectors, navigation assistive technologies, navigational assistance, object detection, pedestrians, pixel-wise semantic segmentation, pyramid scene parsing network, real-time semantic segmentation, short-range obstacles, traffic engineering computing, unifying terrain awareness, visually impaired people, wearable navigation system},
	pages = {1033--1038},
}

@inproceedings{liu_terrain_2017,
	title = {Terrain recognition for outdoor mobile robots},
	doi = {10.1109/CAC.2017.8243527},
	abstract = {Recent work in terrain recognition for outdoor mobile robots mainly focused on several typical pure terrain sample classification, and only one terrain feature is extracted for terrain sample description. In this paper, a segmentation scheme for complex terrain samples is designed for the terrain recognition process. The segmentation scheme is achieved using the graph segmentation followed by the watershed segmentation. The terrain image area is identified on the basis of segmentation. For terrain recognition, several global-based features are extracted from the typical terrain samples: Color Histogram, LBP (Local Binary Pattern), CEDD (Color and Edge Directivity Descriptor), with an ELM (Extreme Learning Machine) classifier to implement the classification experiment. We evaluate the performance of the three features and the principle of extracted feature, and we combined color histogram features with LBP features in series method, and the fused feature are proved to be robust to terrain samples with multi-illumination and jitter scenarios with higher classification accuracy.},
	booktitle = {2017 {Chinese} {Automation} {Congress} ({CAC})},
	author = {Liu, Fushuai and Ma, Xin and Li, Xue and Song, Rui and Tian, Guohui and Li, Yibin},
	month = oct,
	year = {2017},
	note = {ISSN: null},
	keywords = {Classification algorithms, Feature extraction, Histograms, Image color analysis, Image segmentation, Mobile robots, color and edge directivity descriptor, color histogram, computer vision, edge detection, extreme learning machine classifier, feature extraction, feature fusion, graph colouring, graph segmentation scheme, image classification, image colour analysis, image segmentation, learning (artificial intelligence), local binary pattern, mobile robots, outdoor mobile robots, robot vision, terrain image area, terrain recognition, terrain recognition process, terrain sample classification, terrain sample description, watershed segmentation},
	pages = {4257--4262},
}

@inproceedings{da_silva_vieira_ground_2019,
	title = {Ground {Segmentation} {From} {Outdoor} {Environments} in {Rural} {Areas}},
	doi = {10.1109/CCECE.2019.8861556},
	abstract = {The understanding of the complexity of outdoor environments is an essential issue for the development of efficient processes of autonomous mobility, especially in areas with uneven illumination and without a well-defined road. In this context, the detection of ground and obstacles plays a relevant role in giving the first impressions of the external surroundings to a machine. Furthermore, it can guide independent movements and decisions. In this study, we introduce a segmentation method that detects ground and non-ground points of complex scenes under different exposures to illumination, textures, and shading. We prepared a dataset with images collected from some environments in which trees are prominent obstacles. The proposed method uses contrast templates, statistical measures, and morphological operators to reach the ground segmentation. Experiments showed satisfactory results in which trees were well detected and the ground was efficiently segmented with the maintenance of the structure of the image.},
	booktitle = {2019 {IEEE} {Canadian} {Conference} of {Electrical} and {Computer} {Engineering} ({CCECE})},
	author = {da Silva Vieira, Gabriel and Soares, Fabrizzio A.A.M.N. and Santos, Samuel A. and Laureano, Gustavo T. and Lima, Junio Cesar de and Costa, Ronaldo M. and Félix, Juliana Paula and Nascimento, Thamer H.},
	month = may,
	year = {2019},
	note = {ISSN: 0840-7789},
	keywords = {Feature extraction, Image color analysis, Image segmentation, Roads, Task analysis, Training, Vegetation, autonomous mobility, computer vision., contrast templates, feature engineering, ground detection, ground segmentation, image segmentation, image structure, morphological operators, object detection, outdoor environments, segmentation method, statistical analysis, statistical measures, tree trunk detection},
	pages = {1--4},
}

@article{nguyen_high-throughput_2019,
	title = {A {High}-{Throughput} and {Power}-{Efficient} {FPGA} {Implementation} of {YOLO} {CNN} for {Object} {Detection}},
	volume = {27},
	issn = {1557-9999},
	doi = {10.1109/TVLSI.2019.2905242},
	abstract = {Convolutional neural networks (CNNs) require numerous computations and external memory accesses. Frequent accesses to off-chip memory cause slow processing and large power dissipation. For real-time object detection with high throughput and power efficiency, this paper presents a Tera-OPS streaming hardware accelerator implementing a you-only-look-once (YOLO) CNN. The parameters of the YOLO CNN are retrained and quantized with the PASCAL VOC data set using binary weight and flexible low-bit activation. The binary weight enables storing the entire network model in block RAMs of a field-programmable gate array (FPGA) to reduce off-chip accesses aggressively and, thereby, achieve significant performance enhancement. In the proposed design, all convolutional layers are fully pipelined for enhanced hardware utilization. The input image is delivered to the accelerator line-by-line. Similarly, the output from the previous layer is transmitted to the next layer line-by-line. The intermediate data are fully reused across layers, thereby eliminating external memory accesses. The decreased dynamic random access memory (DRAM) accesses reduce DRAM power consumption. Furthermore, as the convolutional layers are fully parameterized, it is easy to scale up the network. In this streaming design, each convolution layer is mapped to a dedicated hardware block. Therefore, it outperforms the “one-size-fits-all” designs in both performance and power efficiency. This CNN implemented using VC707 FPGA achieves a throughput of 1.877 tera operations per second (TOPS) at 200 MHz with batch processing while consuming 18.29 W of on-chip power, which shows the best power efficiency compared with the previous research. As for object detection accuracy, it achieves a mean average precision (mAP) of 64.16\% for the PASCAL VOC 2007 data set that is only 2.63\% lower than the mAP of the same YOLO network with full precision.},
	number = {8},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	author = {Nguyen, Duy Thanh and Nguyen, Tuan Nghia and Kim, Hyun and Lee, Hyuk-Jae},
	month = aug,
	year = {2019},
	keywords = {Binary weight, Convolution, DRAM chips, DRAM power consumption, Field programmable gate arrays, Hardware, Object detection, PASCAL VOC 2007 data, PASCAL VOC data, Quantization (signal), Random access memory, Tera-OPS streaming hardware accelerator, Throughput, VC707 FPGA, YOLO CNN, YOLO network, accelerator line-by-line, binary weight, convolution layer, convolutional layers, convolutional neural nets, convolutional neural networks, dedicated hardware block, dynamic random access memory accesses, enhanced hardware utilization, entire network model, external memory accesses, field programmable gate arrays, field-programmable gate array, frequent accesses, high throughput, high-throughput, layer line-by-line, low-bit activation, low-precision quantization, numerous computations, object detection, object detection accuracy, off-chip accesses, off-chip memory cause slow processing, on-chip power, power dissipation, power efficiency, power-efficient FPGA implementation, random-access storage, real-time object detection, significant performance enhancement, streaming architecture, streaming design, you-only-look-once (YOLO)},
	pages = {1861--1873},
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2020-01-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
}

@inproceedings{mottaghi_role_2014,
	title = {The {Role} of {Context} for {Object} {Detection} and {Semantic} {Segmentation} in the {Wild}},
	doi = {10.1109/CVPR.2014.119},
	abstract = {In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 detection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmentation and object detection. Our analysis shows that nearest neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of existing contextual models for detection is rather modest. In order to push forward the performance in this difficult scenario, we propose a novel deformable part-based model, which exploits both local context around each candidate detection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919},
	keywords = {Buildings, Context, Context modeling, Image segmentation, Object detection, PASCAL VOC 2010 detection challenge, PASCAL imagery, Semantics, Vegetation, contextual reasoning, deformable part-based model, image segmentation, inference mechanisms, nearest neighbor based approach, object detection, semantic category, semantic segmentation},
	pages = {891--898},
}

@article{everingham_pascal_2015,
	title = {The {Pascal} {Visual} {Object} {Classes} {Challenge}: {A} {Retrospective}},
	volume = {111},
	issn = {1573-1405},
	shorttitle = {The {Pascal} {Visual} {Object} {Classes} {Challenge}},
	url = {https://doi.org/10.1007/s11263-014-0733-5},
	doi = {10.1007/s11263-014-0733-5},
	abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community’s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
	language = {en},
	number = {1},
	urldate = {2019-12-13},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Eslami, S. M. Ali and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jan,
	year = {2015},
	pages = {98--136},
}

@article{he_mask_2018,
	title = {Mask {R}-{CNN}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2844175},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, {\textbackslash}eg, allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
	year = {2018},
	keywords = {Convolutional Neural Network, Feature extraction, Image segmentation, Instance Segmentation, Object Detection, Object detection, Pose Estimation, Proposals, Quantization (signal), Semantics, Task analysis},
	pages = {1--1},
}

@article{shelhamer_fully_2017,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	volume = {39},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2016.2572683},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	month = apr,
	year = {2017},
	keywords = {Computer architecture, Convolution, Convolutional Networks, Deep Learning, Fuses, Image segmentation, NYUDv2, PASCAL VOC, PASCAL-Context, Proposals, SIFT Flow, Semantic Segmentation, Semantics, Training, Transfer Learning, coarse layer, contemporary classification networks, correspondingly-sized output, feedforward neural nets, fine layer, fully convolutional networks, image classification, image representation, image resolution, image segmentation, learned representations, learning (artificial intelligence), semantic segmentation, spatially dense prediction tasks, transforms, visual models},
	pages = {640--651},
}

@article{chen_deeplab:_2018,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	volume = {40},
	issn = {1939-3539},
	shorttitle = {{DeepLab}},
	doi = {10.1109/TPAMI.2017.2699184},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = apr,
	year = {2018},
	keywords = {Computational modeling, Context, Convolution, Convolutional neural networks, Deep Convolutional Neural Networks, Deep Learning, DeepLab, Image resolution, Image segmentation, Neural networks, PASCAL VOC-2012 semantic image segmentation task, Semantics, atrous convolution, atrous spatial pyramid pooling, conditional random fields, convolution, deep convolutional nets, feature extraction, feedforward neural nets, fully connected Conditional Random Field, highlight convolution, image context, image segmentation, learning (artificial intelligence), probabilistic graphical models, random processes, semantic image segmentation, semantic segmentation},
	pages = {834--848},
}

@article{ma_adaptive_2015,
	title = {Adaptive {Multiobjective} {Memetic} {Fuzzy} {Clustering} {Algorithm} for {Remote} {Sensing} {Imagery}},
	volume = {53},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2015.2393357},
	abstract = {Due to the intrinsic complexity of remote sensing images and the lack of prior knowledge, clustering for remote sensing images has always been one of the most challenging tasks in remote sensing image processing. Recently, clustering methods for remote sensing images have often been transformed into multiobjective optimization problems, making them more suitable for complex remote sensing image clustering. However, the performance of the multiobjective clustering methods is often influenced by their optimization capability. To resolve this problem, this paper proposes an adaptive multiobjective memetic fuzzy clustering algorithm (AFCMOMA) for remote sensing imagery. In AFCMOMA, a multiobjective memetic clustering framework is devised to optimize the two objective functions, i.e., Jm and the Xie-Beni (XB) index. One challenging task for memetic algorithms is how to balance the local and global search capabilities. In AFCMOMA, an adaptive strategy is used, which can adaptively achieve a balance between them, based on the statistical characteristic of the objective function values. In addition, in the multiobjective memetic framework, in order to acquire more individuals with high quality, a new population update strategy is devised, in which the updated population is composed of individuals generated in both the local and global searches. Finally, to evaluate the proposed AFCMOMA algorithm, experiments using three remote sensing images were conducted, which confirmed the effectiveness of the proposed algorithm.},
	number = {8},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Ma, Ailong and Zhong, Yanfei and Zhang, Liangpei},
	month = aug,
	year = {2015},
	keywords = {AFCMOMA algorithm, Clustering algorithms, Fuzzy clustering, Linear programming, Memetics, Optimization, Remote sensing, Sociology, Statistics, adaptive multiobjective memetic fuzzy clustering algorithm, fuzzy systems, global search capabilities, memetic algorithm, multiobjective, optimisation, remote sensing, remote sensing imagery},
	pages = {4202--4217},
}

@article{zou_deep_2015,
	title = {Deep {Learning} {Based} {Feature} {Selection} for {Remote} {Sensing} {Scene} {Classification}},
	volume = {12},
	issn = {1558-0571},
	doi = {10.1109/LGRS.2015.2475299},
	abstract = {With the popular use of high-resolution satellite images, more and more research efforts have been placed on remote sensing scene classification/recognition. In scene classification, effective feature selection can significantly boost the final performance. In this letter, a novel deep-learning-based feature-selection method is proposed, which formulates the feature-selection problem as a feature reconstruction problem. Note that the popular deep-learning technique, i.e., the deep belief network (DBN), achieves feature abstraction by minimizing the reconstruction error over the whole feature set, and features with smaller reconstruction errors would hold more feature intrinsics for image representation. Therefore, the proposed method selects features that are more reconstructible as the discriminative features. Specifically, an iterative algorithm is developed to adapt the DBN to produce the inquired reconstruction weights. In the experiments, 2800 remote sensing scene images of seven categories are collected for performance evaluation. Experimental results demonstrate the effectiveness of the proposed method.},
	number = {11},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Zou, Qin and Ni, Lihao and Zhang, Tong and Wang, Qian},
	month = nov,
	year = {2015},
	keywords = {Deep belief network (DBN), Feature extraction, Image reconstruction, Machine learning, Remote sensing, Satellites, Testing, Training, belief networks, deep belief network, deep learning based feature selection, feature abstraction, feature learning, feature reconstruction problem, feature selection, geophysical image processing, high-resolution satellite images, image classification, image representation, iterative algorithm, iterative deep learning, iterative methods, remote sensing, remote sensing scene classification, remote sensing scene images, remote sensing scene recognition, scene recognition, scene understanding},
	pages = {2321--2325},
}

@inproceedings{sevak_survey_2017,
	title = {Survey on semantic image segmentation techniques},
	doi = {10.1109/ISS1.2017.8389420},
	abstract = {Semantic image segmentation is a vast area of interest for computer vision and machine learning researchers. Many vision applications need accurate and efficient image segmentation and segment classification mechanisms for assessing the visual contents and perform the real-time decision making. The application area includes remote sensing, autonomous driving, indoor navigation, video surveillance and virtual or augmented reality systems etc. The segmentation and classification of objects generate the specific performance parameters for various applications which require detailed domain analysis. There are broad range of applications where remote sensing image scene classification play an important role and has been receiving remarkable attention. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This survey paper provides a review of different traditional methods of image segmentation and classification. By comparing these methods with semantic image segmentation using deep learning it is assumed to show the far better result.},
	booktitle = {2017 {International} {Conference} on {Intelligent} {Sustainable} {Systems} ({ICISS})},
	author = {Sevak, Jay S and Kapadia, Aerika D. and Chavda, Jaiminkumar B. and Shah, Arpita and Rahevar, Mrugendrasinh},
	month = dec,
	year = {2017},
	note = {ISSN: null},
	keywords = {Clustering algorithms, Computer vision, Histograms, Image color analysis, Image segmentation, Remote sensing, Semantic segmentation, Semantics, augmented reality, augmented reality systems, computer vision, decision making, deep learning, feature and preprocessing method, feature extraction, geophysical image processing, image classification, image segmentation, learning (artificial intelligence), machine learning researchers, random decision forest, real-time decision making, remote sensing, remote sensing image scene classification, segmentation pipeline, semantic image segmentation techniques, semantic segmentation, specific performance parameters, unsupervised segmentation, video surveillance},
	pages = {306--313},
}

@inproceedings{munaja_road_2016,
	title = {Road detection analysis based on corner adjacent features},
	doi = {10.1109/ICACSIS.2016.7892513},
	abstract = {This article discusses a new method in detecting the road by using corner adjacent features. Corner is a vertices obtained from every part of vehicle moving from one point to the other, which will be the basic for the road boundary calculation process. The adoption of Lukas Kanade and Melkman algorithm [13] proofs to improve system responsiveness towards the motion of moving object. It is proven that from object reading in the late afternoon with minimum light available, the system able to record corner for 1-minute duration and revealing road boundary masking, in an acceptable level of precision. Nevertheless, system still needs improvement on road condition consisting of trees or any other obstacles producing shadows on the road area, causing the incorrect reading. It is concluded that the proposed system is effectively and efficiently able to read road boundary condition by using a single camera.},
	booktitle = {2016 {International} {Conference} on {Advanced} {Computer} {Science} and {Information} {Systems} ({ICACSIS})},
	author = {Munaja, M.D. Enjat and Widyantoro, Dwi H. and Munir, Rinaldi},
	month = oct,
	year = {2016},
	note = {ISSN: null},
	keywords = {Cameras, Color, Corner Adjacent Features, Feature extraction, Global Positioning System, Image Processing, Lukas Kanade, Mathematical model, Melkman algorithm, Motion Detection, Road Detection, Roads, Shape, corner adjacent features, feature extraction, image sensors, object detection, object reading, read road boundary condition, record corner, road boundary calculation process, road condition, road detection analysis, road traffic, single camera, traffic engineering computing, vehicle moving},
	pages = {1--5},
}

@incollection{suh_motion-based_2016,
	address = {Cham},
	series = {Human–{Computer} {Interaction} {Series}},
	title = {Motion-{Based} {Learning}},
	isbn = {978-3-319-19947-4},
	url = {https://doi.org/10.1007/978-3-319-19947-4_7},
	abstract = {In this Chapter, we introduce several learning approaches to generate non-preprogrammed motions for a virtual human. Motion primitives and their causalities should first be learned from a task, which consists of a cascade of sub-tasks. Using programming by demonstration (PbD), it is now common for a virtual human to learn motion primitives and their causalities from a human demonstration. Typically, a virtual human can swiftly and effortlessly acquire a human demonstration from a PbD. To generate non-preprogrammed motions, a virtual human should possess the abilities to: (i) segment a whole movement into meaning segments; (ii) learn motion primitives for their adaptation in a changing environment; (iii) represent a combination of a motion primitive and its causalities (a motion tuple) by considering reusability; and finally, (iv) swiftly and reasonably select a dependable motion primitive in accordance with current and goal situations. In this chapter, we review the state of the art and several solution approaches including their limitations. We then discuss future avenues to target motion tuples in terms of the generation of non-preprogrammed motions for a virtual human.},
	language = {en},
	urldate = {2019-12-05},
	booktitle = {Context {Aware} {Human}-{Robot} and {Human}-{Agent} {Interaction}},
	publisher = {Springer International Publishing},
	author = {Suh, Il Hong and Lee, Sang Hyoung},
	editor = {Magnenat-Thalmann, Nadia and Yuan, Junsong and Thalmann, Daniel and You, Bum-Jae},
	year = {2016},
	doi = {10.1007/978-3-319-19947-4_7},
	keywords = {Bayesian Network, Fine Movement, Gaussian Mixture Model, Motion Trajectory, Virtual Human},
	pages = {151--173},
}

@article{jain_unsupervised_1991,
	title = {Unsupervised texture segmentation using {Gabor} filters},
	volume = {24},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/003132039190143S},
	doi = {10.1016/0031-3203(91)90143-S},
	abstract = {This paper presents a texture segmentation algorithm inspired by the multi-channel filtering theory for visual information processing in the early stages of human visual system. The channels are characterized by a bank of Gabor filters that nearly uniformly covers the spatial-frequency domain, and a systematic filter selection scheme is proposed, which is based on reconstruction of the input image from the filtered images. Texture features are obtained by subjecting each (selected) filtered image to a nonlinear transformation and computing a measure of “energy” in a window around each pixel. A square-error clustering algorithm is then used to integrate the feature images and produce a segmentation. A simple procedure to incorporate spatial information in the clustering process is proposed. A relative index is used to estimate the “true” number of texture categories.},
	language = {en},
	number = {12},
	urldate = {2019-12-04},
	journal = {Pattern Recognition},
	author = {Jain, Anil K. and Farrokhnia, Farshid},
	month = jan,
	year = {1991},
	keywords = {Clustering, Clustering index, Gabor filters, Multi-channel filtering, Texture segmentation, Wavelet transform},
	pages = {1167--1186},
}

@article{john_reliable_2015,
	series = {Second {International} {Symposium} on {Computer} {Vision} and the {Internet} ({VisionNet}’15)},
	title = {A {Reliable} {Method} for {Detecting} {Road} {Regions} from a {Single} {Image} {Based} on {Color} {Distribution} and {Vanishing} {Point} {Location}},
	volume = {58},
	issn = {1877-0509},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050915021134},
	doi = {10.1016/j.procs.2015.08.002},
	abstract = {Numerous advanced driver assistance systems (ADAS) are gaining popularity even in the mid to low end segment cars. Vision based technologies assisted by the use of cameras cater to a lot of these ADAS systems. These systems enhance the safety of the driver, passenger and pedestrians on the road. In a typical image taken from an on-board camera of a car, it is the road region that occupies most of the pixels. Therefore, an approach for detecting and eliminating road regions from an optical image is proposed which in turn speeds up computations for further object/ROI detection. Our proposed road detection algorithm works in two stages: i) vanishing point detection; and ii) road region identification. Experimental results show that this approach performs better with real-road images of varying texture, colour and shapes.},
	language = {en},
	urldate = {2019-12-04},
	journal = {Procedia Computer Science},
	author = {John, Neethu and Anusha, B. and Kutty, Krishnan},
	month = jan,
	year = {2015},
	keywords = {Blob detection, likelihood estimation, road detection, texture-orientation, vanishing point estimation.},
	pages = {2--9},
}

@article{melkman_-line_1987,
	title = {On-line construction of the convex hull of a simple polyline},
	volume = {25},
	issn = {0020-0190},
	url = {http://www.sciencedirect.com/science/article/pii/002001908790086X},
	doi = {10.1016/0020-0190(87)90086-X},
	language = {en},
	number = {1},
	urldate = {2019-11-19},
	journal = {Information Processing Letters},
	author = {Melkman, Avraham A.},
	month = apr,
	year = {1987},
	keywords = {Convex hull, analysis of algorithms, simple polygon},
	pages = {11--12},
}

@inproceedings{khan_high_2011,
	title = {High resolution visual terrain classification for outdoor robots},
	doi = {10.1109/ICCVW.2011.6130362},
	abstract = {In this paper we investigate SURF features for visual terrain classification for outdoor mobile robots. The image is divided into a grid and SURF features are calculated on the intersections of this grid. These features are then used to train a classifier that can differentiate between different terrain classes. Images of five different terrain types are taken using a single camera mounted on a mobile outdoor robot. We further introduce another descriptor, which is a modified form of the dense Daisy descriptor. Random forests are used for classification on each descriptor. Classification results of SURF and Daisy descriptors are compared with the results from traditional texture descriptors like LBP, LTP and LATP. It is shown that SURF features perform better than other descriptors at higher resolutions. Daisy features, although not better than SURF features, also perform better than the three texture descriptors at high resolution.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCV} {Workshops})},
	author = {Khan, Yasir Niaz and Komma, Philippe and Zell, Andreas},
	month = nov,
	year = {2011},
	keywords = {Accuracy, Cameras, Image color analysis, Rain, Robot vision systems, SURF feature, Vegetation, dense Daisy descriptor, feature extraction, image classification, mobile robot, mobile robots, outdoor robot, random forests, robot vision, speeded up robust feature, visual terrain classification},
	pages = {1014--1021},
}

@inproceedings{jacobs_consistent_2007,
	title = {Consistent {Temporal} {Variations} in {Many} {Outdoor} {Scenes}},
	doi = {10.1109/CVPR.2007.383258},
	abstract = {This paper details an empirical study of large image sets taken by static cameras. These images have consistent correlations over the entire image and over time scales of days to months. Simple second-order statistics of such image sets show vastly more structure than exists in generic natural images or video from moving cameras. Using a slight variant to PCA, we can decompose all cameras into comparable components and annotate images with respect to surface orientation, weather, and seasonal change. Experiments are based on a data set from 538 cameras across the United States which have collected more than 17 million images over the the last 6 months.},
	booktitle = {2007 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Jacobs, Nathan and Roman, Nathaniel and Pless, Robert},
	month = jun,
	year = {2007},
	note = {ISSN: 1063-6919},
	keywords = {Cameras, Color, Computer science, Higher order statistics, Independent component analysis, Jacobian matrices, Layout, Principal component analysis, Statistical distributions, Video sharing, geophysics computing, image annotation, image colour analysis, image resolution, image sets, outdoor scenes, principal component analysis, second-order statistics, set theory, static cameras, surface orientation, weather forecasting},
	pages = {1--6},
}

@article{wired_magazine_crazy_2019,
	title = {The {Crazy} {Hacks} {One} {Woman} {Used} to {Make} {Money} on {Mechanical} {Turk}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/the-crazy-hacks-one-woman-used-to-make-money-on-mechanical-turk/},
	abstract = {A new book describes how a Mechanical Turker used alarms and shortcuts to make \$20 an hour, at the cost of her health.},
	language = {en},
	urldate = {2019-09-23},
	journal = {Wired},
	author = {Wired Magazine},
	year = {2019},
	keywords = {amazon, gig economy, mechanical turk, uber},
}

@inproceedings{moro_wisenet:_2012,
	address = {New York, NY, USA},
	series = {{CIKM} '12},
	title = {{WiSeNet}: {Building} a {Wikipedia}-based {Semantic} {Network} with {Ontologized} {Relations}},
	isbn = {978-1-4503-1156-4},
	shorttitle = {{WiSeNet}},
	url = {http://doi.acm.org/10.1145/2396761.2398495},
	doi = {10.1145/2396761.2398495},
	abstract = {In this paper we present an approach for building a Wikipedia-based semantic network by integrating Open Information Extraction with Knowledge Acquisition techniques. Our algorithm extracts relation instances from Wikipedia page bodies and ontologizes them by, first, creating sets of synonymous relational phrases, called relation synsets, second, assigning semantic classes to the arguments of these relation synsets and, third, disambiguating the initial relation instances with relation synsets. As a result we obtain WiSeNet, a Wikipedia-based Semantic Network with Wikipedia pages as concepts and labeled, ontologized relations between them.},
	urldate = {2019-11-08},
	booktitle = {Proceedings of the 21st {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Moro, Andrea and Navigli, Roberto},
	year = {2012},
	note = {event-place: Maui, Hawaii, USA},
	keywords = {information extraction, knowledge acquisition, relation ontologization, semantic network},
	pages = {1672--1676},
}

@misc{escarlate_advanced_2017,
	title = {Advanced {Lane} {Detection} for {Autonomous} {Cars}},
	url = {https://medium.com/@cacheop/advanced-lane-detection-for-autonomous-cars-bff5390a360f},
	abstract = {Computer vision techniques to implement an improved lane finding algorithm.},
	language = {en},
	urldate = {2019-11-08},
	journal = {Medium},
	author = {Escarlate, Alberto},
	month = sep,
	year = {2017},
}

@article{wei-ying_ma_edgeflow:_2000,
	title = {{EdgeFlow}: a technique for boundary detection and image segmentation},
	volume = {9},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{EdgeFlow}},
	doi = {10.1109/83.855433},
	abstract = {A novel boundary detection scheme based on "edge flow" is proposed in this paper. This scheme utilizes a predictive coding model to identify the direction of change in color and texture at each image location at a given scale, and constructs an edge flow vector. By propagating the edge flow vectors, the boundaries can be detected at image locations which encounter two opposite directions of flow in the stable state. A user defined image scale is the only significant control parameter that is needed by the algorithm. The scheme facilitates integration of color and texture into a single framework for boundary detection. Segmentation results on a large and diverse collections of natural images are provided, demonstrating the usefulness of this method to content based image retrieval.},
	number = {8},
	journal = {IEEE Transactions on Image Processing},
	author = {Wei-Ying Ma and Manjunath, B.S.},
	month = aug,
	year = {2000},
	keywords = {Application software, Computer vision, Content based retrieval, EdgeFlow, Filtering, Gabor filters, Image edge detection, Image retrieval, Image segmentation, Predictive coding, Predictive models, boundary detection, color, content based image retrieval, content-based retrieval, control parameter, edge detection, edge flow, image colour analysis, image location, image segmentation, image texture, predictive coding, stable state, texture, user defined image scale},
	pages = {1375--1388},
}

@misc{noauthor_scrapy_nodate,
	title = {Scrapy {\textbar} {A} {Fast} and {Powerful} {Scraping} and {Web} {Crawling} {Framework}},
	url = {https://scrapy.org/},
	urldate = {2019-11-08},
}

@inproceedings{jain_radial_2019,
	title = {Radial {Loss} for {Learning} {Fine}-grained {Video} {Similarity} {Metric}},
	doi = {10.1109/ICASSP.2019.8683003},
	abstract = {In this paper, we propose the Radial Loss which utilizes category and sub-category labels to learn an order-preserving fine-grained video similarity metric. We propose an end-to-end quadlet-based Convolutional Neural Network (CNN) combined with Long Short-term Memory (LSTM) Unit to model video similarities by learning the pairwise distance relationships between samples in a quadlet generated using the category and sub-category labels. We showcase two novel applications of learning a video similarity metric - (i) fine-grained video retrieval, (ii) fine-grained event detection, along with simultaneous shot boundary detection, and correspondingly show promising results against those of the baselines on two new fine-grained video datasets.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Jain, Abhinav and Agarwal, Prerna and Mujumdar, Shashank and Gupta, Nitin and Mehta, Sameep and Chattopadhyay, Chiranjoy},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X, 1520-6149},
	keywords = {CNN, CNN-LSTM, Charge coupled devices, Critical Event Detection, Encoding, Event detection, Fine-grained Video Similarity, Measurement, Radial Loss, Sports, Task analysis, Training, convolutional neural nets, data analysis, end-to-end quadlet-based convolutional neural network, fine-grained event detection, fine-grained video datasets, fine-grained video retrieval, gradient methods, learning (artificial intelligence), long short-term memory unit, order-preserving fine-grained video similarity metric, pairwise distance relationships, radial loss, sub-category labels, video retrieval, video signal processing, video similarities},
	pages = {1652--1656},
}

@inproceedings{dailey_creating_2017,
	title = {Creating the world's largest real-time camera network},
	doi = {10.2352/ISSN.2470-1173.2017.10.IMAWM-160},
	booktitle = {{IS} and {T} {International} {Symposium} on {Electronic} {Imaging} {Science} and {Technology}},
	publisher = {Society for Imaging Science and Technology},
	author = {Dailey, R. and Kaseb, A. S. and Brown, C. and Jenkins, S. and Yellin, S. and Pan, F. and Lu, Y.-H.},
	year = {2017},
	pages = {5--12},
}

@article{miller_wordnet:_1995,
	title = {{WordNet}: {A} {Lexical} {Database} for {English}},
	volume = {38},
	issn = {0001-0782},
	shorttitle = {{WordNet}},
	url = {http://doi.acm.org/10.1145/219717.219748},
	doi = {10.1145/219717.219748},
	abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
	number = {11},
	urldate = {2019-11-08},
	journal = {Commun. ACM},
	author = {Miller, George A.},
	month = nov,
	year = {1995},
	pages = {39--41},
}

@inproceedings{cantoni_vanishing_2001,
	title = {Vanishing point detection: representation analysis and new approaches},
	shorttitle = {Vanishing point detection},
	doi = {10.1109/ICIAP.2001.956990},
	abstract = {We introduce two different representation approaches and propose two techniques to estimate the position of vanishing points in an image, one bused on a probabilistic strategy and the other focused on a deterministic analysis. Unlike most of the methods so far developed, which exploit the Gaussian sphere, the new techniques operate in the (/spl rho/, /spl theta/) polar parameter space and in the (x, y) image plane coordinate space. Both the solutions are described and compared, through the discussion of the results obtained from their application to real images.},
	booktitle = {Proceedings 11th {International} {Conference} on {Image} {Analysis} and {Processing}},
	author = {Cantoni, V. and Lombardi, L. and Porta, M. and Sicard, N.},
	month = sep,
	year = {2001},
	keywords = {Gaussian processes, Hough transforms, Image converters, Image segmentation, Layout, Proposals, Quantization, Voting, deterministic analysis, image plane coordinate space, image representation, parameter estimation, polar parameter space, position estimation, probabilistic strategy, probes, representation analysis, vanishing point detection},
	pages = {90--94},
}

@patent{hough_method_1962,
	title = {Method and means for recognizing complex patterns},
	url = {https://patents.google.com/patent/US3069654A/en},
	nationality = {US},
	assignee = {Paul V C Hough},
	number = {US3069654A},
	urldate = {2019-11-05},
	author = {Hough, Paul V. C.},
	month = dec,
	year = {1962},
	keywords = {framelet, line, microsecond, pulse, segment},
}

@inproceedings{han_geometric_2011,
	title = {Geometric and texture cue based depth-map estimation for {2D} to {3D} image conversion},
	doi = {10.1109/ICCE.2011.5722790},
	abstract = {We present a novel framework for estimating depth-map in a single 2D image. The proposed framework employs both vanishing point and superpixels as geometric and texture cues, respectively. The combination of both cues can generate a feasible perceptual depth-map, which will be applied to 2D to 3D image conversion.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Consumer} {Electronics} ({ICCE})},
	author = {Han, Kyuseo and Hong, Kihyun},
	month = jan,
	year = {2011},
	note = {ISSN: 2158-4001, 2158-3994, 2158-3994, 2158-3994},
	keywords = {2D image, 3D image conversion, Computer vision, Estimation, Geometry, Image segmentation, Semantics, Simulation, Three dimensional displays, depth-map estimation, geometric cue, image convertors, image texture, texture cue, three-dimensional displays},
	pages = {651--652},
}

@inproceedings{jung_depth_2010,
	title = {Depth map estimation from single-view image using object classification based on {Bayesian} learning},
	doi = {10.1109/3DTV.2010.5506603},
	abstract = {Generation of three-dimensional (3D) scenes from two-dimensional (2D) images is an important step for a successful introduction to 3D multimedia services. Among the relevant problems, depth estimation from a single-view image is probably the most difficult and challenging task. In this paper, we propose a new depth estimation method using object classification based on the Bayesian learning algorithm. Using training data of six attributes, we categorize objects in the single-view image into four different types. According to the type, we assign a relative depth value to each object and generate a simple 3D model. Experimental results show that the proposed method estimates depth information properly and generates a good 3D model.},
	booktitle = {2010 {3DTV}-{Conference}: {The} {True} {Vision} - {Capture}, {Transmission} and {Display} of {3D} {Video}},
	author = {Jung, Jae-Il and Ho, Yo-Sung},
	month = jun,
	year = {2010},
	note = {ISSN: 2161-2021, 2161-203X},
	keywords = {2D images, 2D-to-3D conversion, 3D model, 3D multimedia services, 3D scene generation, Bayesian learning algorithm, Bayesian methods, Cameras, Depth estimation, Focusing, Image converters, Image edge detection, Image generation, Image processing, Layout, Monocular depth cues, Pixel, Single-view image, Training data, depth map estimation method, estimation theory, image classification, learning (artificial intelligence), multimedia communication, object classification, single-view image, telecommunication computing, three-dimensional scene generation, two-dimensional images},
	pages = {1--4},
}

@inproceedings{galleguillos_object_2008,
	title = {Object categorization using co-occurrence, location and appearance},
	doi = {10.1109/CVPR.2008.4587799},
	abstract = {In this work we introduce a novel approach to object categorization that incorporates two types of context-co-occurrence and relative location - with local appearance-based features. Our approach, named CoLA (for co-occurrence, location and appearance), uses a conditional random field (CRF) to maximize object label agreement according to both semantic and spatial relevance. We model relative location between objects using simple pairwise features. By vector quantizing this feature space, we learn a small set of prototypical spatial relationships directly from the data. We evaluate our results on two challenging datasets: PASCAL 2007 and MSRC. The results show that combining co-occurrence and spatial context improves accuracy in as many as half of the categories compared to using co-occurrence alone.},
	booktitle = {2008 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Galleguillos, Carolina and Rabinovich, Andrew and Belongie, Serge},
	month = jun,
	year = {2008},
	note = {ISSN: 1063-6919},
	keywords = {CRF, CoLA, Computer science, Computer vision, Context modeling, Face detection, Image segmentation, Layout, Lighting, Object recognition, PASCAL 2007, Prototypes, Psychology, conditional random field, cooccurrence location appearance, image segmentation, local appearance-based features, object categorization, object label agreement, pairwise features, prototypical spatial relationships, semantic-spatial relevance, vector quantisation},
	pages = {1--8},
}

@techreport{fischler_recognizing_1989,
	title = {Recognizing {Objects} in a {Natural} {Environment}: {A} {Contextual} {Vision} {System} ({CVS})},
	shorttitle = {Recognizing {Objects} in a {Natural} {Environment}},
	url = {https://apps.dtic.mil/docs/citations/ADA460946},
	abstract = {Existing machine vision techniques are not competent to reliably recognize objects in unconstrained views of natural scenes. In this paper we identify a number of weaknesses in current recognition systems, including an inability to solve the partitioning problem or to effectively use context and other types of knowledge beyond that of immediate object appearance. We propose specific mechanisms for dealing with some of these problems and describe the design of a vision system that incorporates these new mechanisms. The system has been partially implemented and we include some experimental results indicative of its operation and performance.},
	language = {en},
	number = {SRI-TN-463},
	urldate = {2019-11-04},
	institution = {SRI INTERNATIONAL MENLO PARK CA ARTIFICIAL INTELLIGENCE CENTER},
	author = {Fischler, Martin A. and Strat, Thomas M.},
	month = mar,
	year = {1989},
}

@article{peer_reputation_2014,
	title = {Reputation as a sufficient condition for data quality on {Amazon} {Mechanical} {Turk}},
	volume = {46},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-013-0434-y},
	doi = {10.3758/s13428-013-0434-y},
	abstract = {Data quality is one of the major concerns of using crowdsourcing websites such as Amazon Mechanical Turk (MTurk) to recruit participants for online behavioral studies. We compared two methods for ensuring data quality on MTurk: attention check questions (ACQs) and restricting participation to MTurk workers with high reputation (above 95\% approval ratings). In Experiment 1, we found that high-reputation workers rarely failed ACQs and provided higher-quality data than did low-reputation workers; ACQs improved data quality only for low-reputation workers, and only in some cases. Experiment 2 corroborated these findings and also showed that more productive high-reputation workers produce the highest-quality data. We concluded that sampling high-reputation workers can ensure high-quality data without having to resort to using ACQs, which may lead to selection bias if participants who fail ACQs are excluded post-hoc.},
	language = {en},
	number = {4},
	urldate = {2019-11-03},
	journal = {Behavior Research Methods},
	author = {Peer, Eyal and Vosgerau, Joachim and Acquisti, Alessandro},
	month = dec,
	year = {2014},
	keywords = {Amazon Mechanical Turk, Data quality, Online research, Reputation},
	pages = {1023--1031},
}

@article{dawid_maximum_1979,
	title = {Maximum {Likelihood} {Estimation} of {Observer} {Error}-{Rates} {Using} the {EM} {Algorithm}},
	volume = {28},
	copyright = {© 1979 The Authors},
	issn = {1467-9876},
	url = {http://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2346806},
	doi = {10.2307/2346806},
	abstract = {In compiling a patient record many facets are subject to errors of measurement. A model is presented which allows individual error-rates to be estimated for polytomous facets even when the patient's “true” response is not available. The EM algorithm is shown to provide a slow but sure way of obtaining maximum likelihood estimates of the parameters of interest. Some preliminary experience is reported and the limitations of the method are described.},
	language = {en},
	number = {1},
	urldate = {2019-11-03},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Dawid, A. P. and Skene, A. M.},
	year = {1979},
	keywords = {em algorithm, latent class model, medical example, observer variation},
	pages = {20--28},
}

@misc{lionbridge_ai_image_nodate,
	title = {Image {Annotation}},
	url = {https://lionbridge.ai/services/image-annotation/},
	abstract = {Power your computer vision models with high-quality image data, meticulously tagged by our expert annotators.},
	language = {en},
	urldate = {2019-11-01},
	journal = {Lionbridge AI},
	author = {LionBridge AI},
}

@misc{amazon.com_amazon_nodate,
	title = {Amazon {Mechanical} {Turk}},
	url = {https://www.mturk.com/},
	urldate = {2019-11-01},
	author = {Amazon.com},
}

@inproceedings{ipeirotis_quality_2010,
	address = {New York, NY, USA},
	series = {{HCOMP} '10},
	title = {Quality {Management} on {Amazon} {Mechanical} {Turk}},
	isbn = {978-1-4503-0222-7},
	url = {http://doi.acm.org/10.1145/1837885.1837906},
	doi = {10.1145/1837885.1837906},
	abstract = {Crowdsourcing services, such as Amazon Mechanical Turk, allow for easy distribution of small tasks to a large number of workers. Unfortunately, since manually verifying the quality of the submitted results is hard, malicious workers often take advantage of the verification difficulty and submit answers of low quality. Currently, most requesters rely on redundancy to identify the correct answers. However, redundancy is not a panacea. Massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. Therefore, we need techniques that will accurately estimate the quality of the workers, allowing for the rejection and blocking of the low-performing workers and spammers. However, existing techniques cannot separate the true (unrecoverable) error rate from the (recoverable) biases that some workers exhibit. This lack of separation leads to incorrect assessments of a worker's quality. We present algorithms that improve the existing state-of-the-art techniques, enabling the separation of bias and error. Our algorithm generates a scalar score representing the inherent quality of each worker. We illustrate how to incorporate cost-sensitive classification errors in the overall framework and how to seamlessly integrate unsupervised and supervised techniques for inferring the quality of the workers. We present experimental results demonstrating the performance of the proposed algorithm under a variety of settings.},
	urldate = {2019-11-01},
	booktitle = {Proceedings of the {ACM} {SIGKDD} {Workshop} on {Human} {Computation}},
	publisher = {ACM},
	author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
	year = {2010},
	note = {event-place: Washington DC},
	pages = {64--67},
}

@misc{darrenl_tzutalin/labelimg_2019,
	title = {tzutalin/{labelImg}},
	copyright = {MIT},
	url = {https://github.com/tzutalin/labelImg},
	abstract = {:metal: LabelImg is a graphical image annotation tool and label object bounding boxes in images},
	urldate = {2019-11-01},
	author = {darrenl},
	month = nov,
	year = {2019},
	note = {original-date: 2015-09-17T01:33:59Z},
	keywords = {annotations, deep-learning, detection, image-classification, imagenet, python2, python3, recognition, tools},
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	volume = {1},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Dalal, N. and Triggs, B.},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {High performance computing, Histograms, Humans, Image databases, Image edge detection, Object detection, Object recognition, Robustness, Support vector machines, Testing, coarse spatial binning, contrast normalization, edge based descriptors, feature extraction, fine orientation binning, fine-scale gradients, gradient based descriptors, gradient methods, histograms of oriented gradients, human detection, linear SVM, object detection, object recognition, overlapping descriptor, pedestrian database, robust visual object recognition, support vector machines},
	pages = {886--893 vol. 1},
}

@article{everingham_pascal_2010,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	volume = {88},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
	language = {en},
	number = {2},
	urldate = {2019-11-01},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2010},
	keywords = {Benchmark, Database, Object detection, Object recognition},
	pages = {303--338},
}

@inproceedings{lin_microsoft_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	isbn = {978-3-319-10602-1},
	shorttitle = {Microsoft {COCO}},
	doi = {10.1007/978-3-319-10602-1_48},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Common Object, Object Category, Object Detection, Object Instance, Scene Understanding},
	pages = {740--755},
}

@inproceedings{deng_imagenet:_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, ImageNet database, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine, computer vision, image resolution, image retrieval, large-scale hierarchical image database, large-scale ontology, multimedia computing, multimedia data, ontologies (artificial intelligence), subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
}

@inproceedings{xiao_sun_2010,
	title = {{SUN} database: {Large}-scale scene recognition from abbey to zoo},
	shorttitle = {{SUN} database},
	doi = {10.1109/CVPR.2010.5539970},
	abstract = {Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Xiao, Jianxiong and Hays, James and Ehinger, Krista A. and Oliva, Aude and Torralba, Antonio},
	month = jun,
	year = {2010},
	note = {ISSN: 1063-6919, 1063-6919},
	keywords = {Anthropometry, Bridges, Computer vision, Humans, Image databases, Large-scale systems, Layout, Legged locomotion, SUN database, Spatial databases, Sun, abbey, computer vision, finer-grained scene representation, human factors, human scene classification performance, image classification, large-scale scene recognition, object categorization, object recognition, scene categorization, scene category, scene understanding database, scene understanding research, state-of-the-art algorithms, visual databases, zoo},
	pages = {3485--3492},
}

@misc{u.s._national_library_of_medicine_unified_2019,
	type = {List of {Links}},
	title = {Unified {Medical} {Language} {System} ({UMLS})},
	copyright = {Public Domain},
	url = {https://www.nlm.nih.gov/research/umls/index.html},
	abstract = {The UMLS integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and interoperable biomedical information systems and services, including electronic health records.},
	language = {eng},
	urldate = {2019-11-01},
	journal = {Unified Medical Language System (UMLS)},
	author = {U.S. National Library of Medicine},
	month = may,
	year = {2019},
}

@inproceedings{quanyou_zhao_face_2011,
	title = {A face detection method based on corner verifying},
	doi = {10.1109/CSSS.2011.5974784},
	abstract = {Face detection is the first step for automatic face recognition system and many surveillance systems, it has been widely applied in many fields. A face detection method based on corner verifying is presented. Firstly, most of the background area is quickly filtered out by skin color detection. Then faces are detected by AdaBoost face detection algorithm combination with skin color detection, but there are some non-faces to be detected. Finally, the faces are verified by corner distribution of human face after Harris corner detection to further reduce the error rate. Experimental results show that this method can efficiently and accurately detect the faces with multi-feature fusion.},
	booktitle = {2011 {International} {Conference} on {Computer} {Science} and {Service} {System} ({CSSS})},
	author = {{Quanyou Zhao} and {Shujun Zhang}},
	month = jun,
	year = {2011},
	keywords = {AdaBoost, AdaBoost face detection algorithm, Color, Face, Face detection, Harris, Harris corner detection, Humans, IEEE Press, Object detection, Skin, Skin color detection, automatic face recognition system, background filtering, corner verification, edge detection, error rate reduction, face recognition, human face corner distribution, image colour analysis, image fusion, multifeature fusion, skin color detection, surveillance, surveillance system},
	pages = {2854--2857},
}

@inproceedings{dang_review_2017,
	title = {Review and comparison of face detection algorithms},
	doi = {10.1109/CONFLUENCE.2017.7943228},
	abstract = {With the tremendous increase in video and image database there is a great need of automatic understanding and examination of data by the intelligent systems as manually it is becoming out of reach. Narrowing it down to one specific domain, one of the most specific objects that can be traced in the images are people i.e. faces. Face detection is becoming a challenge by its increasing use in number of applications. It is the first step for face recognition, face analysis and detection of other features of face. In this paper, various face detection algorithms are discussed and analyzed like Viola-Jones, SMQT features \& SNOW Classifier, Neural Network-Based Face Detection and Support Vector Machine-Based face detection. All these face detection methods are compared based on the precision and recall value calculated using a DetEval Software which deals with precised values of the bounding boxes around the faces to give accurate results.},
	booktitle = {2017 7th {International} {Conference} on {Cloud} {Computing}, {Data} {Science} {Engineering} - {Confluence}},
	author = {Dang, K. and Sharma, S.},
	month = jan,
	year = {2017},
	keywords = {DetEval software, Face, Face detection, Feature extraction, Neural Network-Based Face Detection, Neural networks, Object detection, Precision, Recall, SMQT Features and SNOW Classifier, SMQT features \& SNOW classifier, Snow, Support Vector Machines-Based face detection, Support vector machines, Viola-Jones algorithm, Viola-Jones face detector, bounding boxes, face analysis, face detection algorithms, face recognition, image classification, image database, intelligent systems, neural network-based face detection, support vector machine-based face detection, support vector machines, video database, visual databases},
	pages = {629--633},
}

@inproceedings{zhang_computer_2010,
	title = {Computer vision vs. human vision},
	doi = {10.1109/COGINF.2010.5599750},
	abstract = {In object recognition (classification), it was known that the human brain processes visual information in semantic space mainly, that is, extracting the semantically meaningful features such as line-segments, boundaries, shape and so on. But by recent information processing techniques, these kinds of features cannot be detected by computers robustly so that in computer vision it's still difficult to process visual information as humans do. Computers have to process visual information in data space formed by the robustly detectable but less meaningful features such as colors, textures etc. Therefore, the processing methodology in computers is quite different from that in human beings. In the talk, we will address the main principle of the image recognition (classification) approach in computer vision, its seedtime, main results and the difficulty faced recently. From digital cameras, there is a huge amount of 2D-image data. In computer object recognition (or classification), the data should be transformed into an object-invariant inner representation. In order to solve the problem, we need two key techniques, i.e., a robust detector and an invariant descriptor. People have attempted to solve the two key techniques for a long time but so far they didn't find any efficient solution. Human visual performances are still superior to that of computer vision greatly in many aspects. So as a future direction, computer vision should learn some things from neuroscience and brain science. We will discuss what computer vision can learn from human vision and how it will be affected by the new interdisciplinary research. We may still face many difficulties in the future.},
	booktitle = {9th {IEEE} {International} {Conference} on {Cognitive} {Informatics} ({ICCI}'10)},
	author = {Zhang, B.},
	month = jul,
	year = {2010},
	keywords = {Computer vision, Computers, Feature extraction, Humans, Object recognition, Robustness, Visualization, computer vision, data space, descriptor, detector, feature, feature extraction, human brain, human vision, image classification, information processing technique, object recognition, semantic space, visual information processing},
	pages = {3--3},
}

@inproceedings{zhao_pyramid_2017,
	title = {Pyramid {Scene} {Parsing} {Network}},
	doi = {10.1109/CVPR.2017.660},
	abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhao, H. and Shi, J. and Qi, X. and Wang, X. and Jia, J.},
	month = jul,
	year = {2017},
	keywords = {Automobiles, Convolution, Feature extraction, Image segmentation, ImageNet scene parsing challenge, Neural networks, PSPNet, Semantics, feature extraction, global context information, global prior representation, image classification, image representation, image segmentation, learning (artificial intelligence), object detection, pyramid pooling module, pyramid scene parsing network, scene parsing task, unrestricted open vocabulary},
	pages = {6230--6239},
}

@inproceedings{cordts_cityscapes_2016,
	title = {The {Cityscapes} {Dataset} for {Semantic} {Urban} {Scene} {Understanding}},
	doi = {10.1109/CVPR.2016.350},
	abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Cordts, M. and Omran, M. and Ramos, S. and Rehfeld, T. and Enzweiler, M. and Benenson, R. and Franke, U. and Roth, S. and Schiele, B.},
	month = jun,
	year = {2016},
	keywords = {Benchmark testing, Cityscapes dataset, Complexity theory, Semantics, Training, Urban areas, Vehicles, Visualization, computer vision, image sequences, object detection, semantic urban scene understanding, stereo image processing, stereo video sequence, video signal processing},
	pages = {3213--3223},
}

@inproceedings{kumar_efficiently_2010,
	title = {Efficiently selecting regions for scene understanding},
	doi = {10.1109/CVPR.2010.5540072},
	abstract = {Recent advances in scene understanding and related tasks have highlighted the importance of using regions to reason about high-level scene structure. Typically, the regions are selected beforehand and then an energy function is defined over them. This two step process suffers from the following deficiencies: (i) the regions may not match the boundaries of the scene entities, thereby introducing errors; and (ii) as the regions are obtained without any knowledge of the energy function, they may not be suitable for the task at hand. We address these problems by designing an efficient approach for obtaining the best set of regions in terms of the energy function itself. Each iteration of our algorithm selects regions from a large dictionary by solving an accurate linear programming relaxation via dual decomposition. The dictionary of regions is constructed by merging and intersecting segments obtained from multiple bottom-up over-segmentations. To demonstrate the usefulness of our algorithm, we consider the task of scene segmentation and show significant improvements over state of the art methods.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Kumar, M. P. and Koller, D.},
	month = jun,
	year = {2010},
	keywords = {Computer science, Dictionaries, Feature extraction, Image reconstruction, Image segmentation, Layout, Linear programming, Merging, Pixel, Tires, dual decomposition, energy function, high-level scene structure, image segmentation, linear programming, linear programming relaxation, scene segmentation, scene understanding},
	pages = {3217--3224},
}

@inproceedings{li_towards_2009,
	title = {Towards total scene understanding: {Classification}, annotation and segmentation in an automatic framework},
	shorttitle = {Towards total scene understanding},
	doi = {10.1109/CVPR.2009.5206718},
	abstract = {Given an image, we propose a hierarchical generative model that classifies the overall scene, recognizes and segments each object component, as well as annotates the image with a list of tags. To our knowledge, this is the first model that performs all three tasks in one coherent framework. For instance, a scene of a `polo game' consists of several visual objects such as `human', `horse', `grass', etc. In addition, it can be further annotated with a list of more abstract (e.g. `dusk') or visually less salient (e.g. `saddle') tags. Our generative model jointly explains images through a visual model and a textual model. Visually relevant objects are represented by regions and patches, while visually irrelevant textual annotations are influenced directly by the overall scene class. We propose a fully automatic learning framework that is able to learn robust scene models from noisy Web data such as images and user tags from Flickr.com. We demonstrate the effectiveness of our framework by automatically classifying, annotating and segmenting images from eight classes depicting sport scenes. In all three tasks, our model significantly outperforms state-of-the-art algorithms.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, L. and Socher, R. and Fei-Fei, L.},
	month = jun,
	year = {2009},
	keywords = {Layout, automatic framework, image annotation, image classification, image segmentation, object component recognition, object recognition, polo game, sport scene, textual model, total scene understanding, visual model, visual object},
	pages = {2036--2043},
}

@misc{noauthor_efficiently_nodate,
	title = {Efficiently selecting regions for scene understanding - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.ezproxy.lib.purdue.edu/abstract/document/5540072},
	urldate = {2019-10-03},
}

@misc{noauthor_efficiently_nodate-1,
	title = {Efficiently selecting regions for scene understanding - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.ezproxy.lib.purdue.edu/abstract/document/5540072},
	urldate = {2019-10-03},
}

@inproceedings{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	doi = {10.1109/CVPR.2016.91},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Redmon, J. and Divvala, S. and Girshick, R. and Farhadi, A.},
	month = jun,
	year = {2016},
	keywords = {Computer architecture, DPM, Microprocessors, Neural networks, Object detection, Pipelines, R-CNN, Real-time systems, Training, YOLO model, bounding boxes, class probabilities, detection performance, detection pipeline, image classification, image representation, natural images, neural nets, neural network, object classifiers, object detection, object representation, unified real-time object detection, you only look once},
	pages = {779--788},
}

@misc{madbsekache_computer_nodate,
	title = {Computer {Vision} {Annotation} {Tool}: {A} {Universal} {Approach} to {Data} {Annotation}},
	shorttitle = {Computer {Vision} {Annotation} {Tool}},
	url = {https://software.intel.com/en-us/articles/computer-vision-annotation-tool-a-universal-approach-to-data-annotation},
	abstract = {At Intel, one of the projects we’re undertaking research on is developing computer vision algorithms based on deep neural networks (DNNs) and how to streamline the process. As with any other DNN, for model training data scientists need annotated data. Of course, there is plenty of data available on the Internet, but there are some roadblocks to utilizing. On one hand, data scientists are asked to apply AI to more and more new tasks without appropriate annotated data for those tasks.},
	language = {en},
	urldate = {2019-09-25},
	author = {mad{\textbackslash}bsekache},
}

@inproceedings{hyundo_kim_semi-autonomous_2006,
	title = {Semi-autonomous {Learning} of {Objects}},
	doi = {10.1109/CVPRW.2006.193},
	abstract = {This paper presents a robotic vision system that can be taught to recognize novel objects in a semi-autonomous manner that does not require manual labeling or segmentation of any individual training images. Instead, unfamiliar objects are simply shown to the system in varying poses and scales against cluttered background and the system automatically detects, tracks, segments, and builds representations for these objects. We demonstrate the feasibility of our approach by training the system to recognize one hundred household objects, which are presented to the system for about a minute each. Our method resembles the way that biological organisms learn to recognize objects and it paves the way for a wealth of applications in robotics and other fields.},
	booktitle = {2006 {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop} ({CVPRW}'06)},
	author = {{Hyundo Kim} and Murphy-Chutorian, E. and Triesch, J.},
	month = jun,
	year = {2006},
	keywords = {Biological systems, Computer vision, Humans, Image recognition, Image segmentation, Labeling, Machine vision, Robot vision systems, Robotics and automation, Training data},
	pages = {145--145},
}

@article{afifi_afif4:_2019,
	title = {{AFIF4}: {Deep} gender classification based on {AdaBoost}-based fusion of isolated facial features and foggy faces},
	volume = {62},
	issn = {1047-3203},
	shorttitle = {{AFIF4}},
	url = {http://www.sciencedirect.com/science/article/pii/S1047320319301567},
	doi = {10.1016/j.jvcir.2019.05.001},
	abstract = {Gender classification aims at recognizing a person’s gender. Despite the high accuracy achieved by state-of-the-art methods for this task, there is still room for improvement in generalized and unrestricted datasets. In this paper, we advocate a new strategy inspired by the behavior of humans in gender recognition. Instead of dealing with the face image as a sole feature, we rely on the combination of isolated facial components and a contextual feature which we call the foggy face. Then, we use these features to train deep convolutional neural networks followed by an AdaBoost-based score fusion to infer the final gender class. We evaluate our method on four challenging datasets to demonstrate its efficacy in achieving better or on-par accuracy with state-of-the-art methods. In addition, we present a new face dataset that intensifies the challenges of occluded faces and illumination changes, which we believe to be a much-needed resource for gender classification research.},
	urldate = {2019-09-23},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Afifi, Mahmoud and Abdelhamed, Abdelrahman},
	month = jul,
	year = {2019},
	keywords = {Deep convolutional neural networks, Face image dataset, Gender classification},
	pages = {77--86},
}

@misc{semuels_internet_2018,
	title = {The {Internet} {Is} {Enabling} a {New} {Kind} of {Poorly} {Paid} {Hell}},
	url = {https://www.theatlantic.com/business/archive/2018/01/amazon-mechanical-turk/551192/},
	abstract = {For some Americans, sub-minimum-wage online tasks are the only work available.},
	language = {en-US},
	urldate = {2019-09-23},
	journal = {The Atlantic},
	author = {Semuels, Alana},
	month = jan,
	year = {2018},
}

@inproceedings{difallah_demographics_2018,
	address = {New York, NY, USA},
	series = {{WSDM} '18},
	title = {Demographics and {Dynamics} of {Mechanical} {Turk} {Workers}},
	isbn = {978-1-4503-5581-0},
	url = {http://doi.acm.org/10.1145/3159652.3159661},
	doi = {10.1145/3159652.3159661},
	abstract = {We present an analysis of the population dynamics and demographics of Amazon Mechanical Turk workers based on the results of the survey that we conducted over a period of 28 months, with more than 85K responses from 40K unique participants. The demographics survey is ongoing (as of November 2017), and the results are available at http://demographics.mturk-tracker.com: we provide an API for researchers to download the survey data. We use techniques from the field of ecology, in particular, the capture-recapture technique, to understand the size and dynamics of the underlying population. We also demonstrate how to model and account for the inherent selection biases in such surveys. Our results indicate that there are more than 100K workers available in Amazon»s crowdsourcing platform, the participation of the workers in the platform follows a heavy-tailed distribution, and at any given time there are more than 2K active workers. We also show that the half-life of a worker on the platform is around 12-18 months and that the rate of arrival of new workers balances the rate of departures, keeping the overall worker population relatively stable. Finally, we demonstrate how we can estimate the biases of different demographics to participate in the survey tasks, and show how to correct such biases. Our methodology is generic and can be applied to any platform where we are interested in understanding the dynamics and demographics of the underlying user population.},
	urldate = {2019-09-23},
	booktitle = {Proceedings of the {Eleventh} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Difallah, Djellel and Filatova, Elena and Ipeirotis, Panos},
	year = {2018},
	note = {event-place: Marina Del Rey, CA, USA},
	keywords = {amazon mechanical turk, capture-recapture, crowdsourcing, demographics, dynamics, selection bias, surveys},
	pages = {135--143},
}

@inproceedings{snow_cheap_2008,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '08},
	title = {Cheap and {Fast}—but is {It} {Good}?: {Evaluating} {Non}-expert {Annotations} for {Natural} {Language} {Tasks}},
	shorttitle = {Cheap and {Fast}—but is {It} {Good}?},
	url = {http://dl.acm.org/citation.cfm?id=1613715.1613751},
	abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
	urldate = {2019-09-23},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Snow, Rion and O'Connor, Brendan and Jurafsky, Daniel and Ng, Andrew Y.},
	year = {2008},
	note = {event-place: Honolulu, Hawaii},
	pages = {254--263},
}

@inproceedings{clouse_multi-level_2014,
	title = {Multi-level scene understanding via hierarchical classification},
	doi = {10.1109/ICIP.2014.7025194},
	abstract = {In applications where the use of video surveillance is necessary and/or beneficial, it is a common goal to identify the contents of the video automatically. Of particular interest in such applications is the ability to recognize locations in the environment, where events occur, and describe the events common to those locations. This is one of the goals of scene understanding. Scene understanding is traditionally addressed from one of two separate points-of-view: the description of the underlying environment or the action taking-place throughout the scene. Each of these facets is required to address the overarching goal but, is insufficient independently to address the problem entirely. These facets are, in fact, dependent and by considering both, a more complete description becomes available. In this paper, we describe a novel, data-driven scene understanding and classification technique that captures and utilizes information about both the environment and activity within a scene.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Clouse, H. S. and Bian, X. and Gentimis, T. and Krim, H.},
	month = oct,
	year = {2014},
	keywords = {Equations, Indexes, Motion segmentation, Pattern recognition, Robustness, Training, Video sequences, data-driven scene understanding, hierarchical classification, hierarchical classification technique, image classification, information capture, multilevel model, multilevel scene understanding, natural scenes, scene understanding, supervised learning, video content, video processing, video signal processing, video surveillance},
	pages = {966--970},
}

@article{wojek_monocular_2013,
	title = {Monocular {Visual} {Scene} {Understanding}: {Understanding} {Multi}-{Object} {Traffic} {Scenes}},
	volume = {35},
	issn = {0162-8828},
	shorttitle = {Monocular {Visual} {Scene} {Understanding}},
	doi = {10.1109/TPAMI.2012.174},
	abstract = {Following recent advances in detection, context modeling, and tracking, scene understanding has been the focus of renewed interest in computer vision research. This paper presents a novel probabilistic 3D scene model that integrates state-of-the-art multiclass object detection, object tracking and scene labeling together with geometric 3D reasoning. Our model is able to represent complex object interactions such as inter-object occlusion, physical exclusion between objects, and geometric context. Inference in this model allows us to jointly recover the 3D scene context and perform 3D multi-object tracking from a mobile observer, for objects of multiple categories, using only monocular video as input. Contrary to many other approaches, our system performs explicit occlusion reasoning and is therefore capable of tracking objects that are partially occluded for extended periods of time, or objects that have never been observed to their full extent. In addition, we show that a joint scene tracklet model for the evidence collected over multiple frames substantially improves performance. The approach is evaluated for different types of challenging onboard sequences. We first show a substantial improvement to the state of the art in 3D multipeople tracking. Moreover, a similar performance gain is achieved for multiclass 3D tracking of cars and trucks on a challenging dataset.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wojek, C. and Walk, S. and Roth, S. and Schindler, K. and Schiele, B.},
	month = apr,
	year = {2013},
	keywords = {3D multiclass object tracking, 3D multipeople tracking, Algorithms, Automobiles, Cameras, Cluster Analysis, Cognition, Computational modeling, Databases, Factual, Detectors, Hidden Markov models, Human Activities, Humans, Image Processing, Computer-Assisted, MCMC, Models, Theoretical, Object detection, Pattern Recognition, Automated, Scene understanding, Solid modeling, Video Recording, Walking, automobiles, cars, complex object interaction representation, computer graphics, computer vision, context modeling, geometric 3D reasoning, image motion analysis, image representation, inference mechanism, inference mechanisms, mobile observer, monocular video, monocular visual scene understanding, multiclass object detection, multiobject traffic scene understanding, natural scenes, object detection, object recognition, object tracking, observers, occlusion reasoning, probabilistic 3D scene model, scene labeling, scene tracklets, tracking, tracking-by-detection, traffic engineering computing, trucks, video surveillance},
	pages = {882--897},
}

@article{lombardi_importance_2006,
	title = {On the {Importance} of {Being} {Contextual}},
	volume = {39},
	issn = {0018-9162},
	doi = {10.1109/MC.2006.434},
	abstract = {Unmanned vehicles are an important evolutionary step for increased safety in a range of missions from passive observation to active exploration to aggressive proaction. To achieve this goal, these vehicles must be reliably autonomous, with effective context interpretation - the continuous understanding and monitoring of the external environment being an essential step on this path. In this approach, appropriate accounting for context should increase the system's perception performance by exploiting opportunistic cues in the visual context and leveraging domain-based anticipated cues from the exosystem. Espousing an emerging probabilistic approach to context monitoring in sensory-based vehicle control systems, we account for partial certainties in the operating environment external to the vision system's field of regard. Our results point to the promise of multimodal awareness as the key to improved vehicle control performance across a broad spectrum of future mission spaces},
	number = {12},
	journal = {Computer},
	author = {Lombardi, P. and Zavidovique, B. and Talbert, M.},
	month = dec,
	year = {2006},
	keywords = {Autonomous vehicles, Bayesian methods, Cameras, Context modeling, Context monitoring, Current measurement, Hidden Markov models, Layout, Mars, Navigation, System performance, Unmanned vehicles, Visual system, domain-based anticipated cues, military vehicles, mission spaces, mobile robots, remotely operated vehicles, robot vision, sensory-based vehicle control systems, unmanned vehicles, vision system, visual context},
	pages = {57--61},
}

@inproceedings{rabinovich_scenes_2009,
	title = {Scenes vs. objects: {A} comparative study of two approaches to context based recognition},
	shorttitle = {Scenes vs. objects},
	doi = {10.1109/CVPRW.2009.5204220},
	abstract = {Contextual models play a very important role in the task of object recognition. Over the years, two kinds of contextual models have emerged: models with contextual inference based on the statistical summary of the scene (we will refer to these as scene based context models, or SBC), and models representing the context in terms of relationships among objects in the image (object based context, or OBC). In designing object recognition systems, it is necessary to understand the theoretical and practical properties of such approaches. This work provides an analysis of these models and evaluates two of their representatives using the LabelMe dataset. We demonstrate a considerable margin of improvement using the OBC style approach.},
	booktitle = {2009 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Rabinovich, A. and Belongie, S.},
	month = jun,
	year = {2009},
	keywords = {Computer vision, Context modeling, Fires, Humans, Information resources, Keyboards, LabelMe dataset, Layout, Object recognition, Refrigerators, Statistics, computer vision, computer vision community, context based recognition, object based context, object recognition, object recognition systems, scene based context models},
	pages = {92--99},
}

@inproceedings{aarthi_scene_2017,
	title = {Scene understanding — {A} survey},
	doi = {10.1109/ICCCSP.2017.7944094},
	abstract = {In recent times, scene understanding holds a great position in computer vision due to its real time perceiving, analyzing and elaborating an interpretation of dynamic scene which leads to new discoveries. A scene is a view of real world environment with multiple objects and surfaces in a meaningful way. Objects are compact and act upon whereas scene are extended in space and act within. The visual information can be given with many features such as Colors, Luminance and contours or in the form of Shapes, Parts and Textures or through semantic context. The goal of scene understanding is to make machines look like humans, to have a complete understanding of visual scenes. Scene understanding is influenced by cognitive vision with an involvement of major areas like computer vision, cognitive engineering and software engineering. Due to its enormous growth many outstanding universities like Boston University, Stafford Vision lab, Scene grammar lab, air lab, Laboratory Machine Vision and Pattern Recognition have been perseveringly working for added improvements in this area. This paper discusses an extensive survey of scene understanding with various strategies and methods.},
	booktitle = {2017 {International} {Conference} on {Computer}, {Communication} and {Signal} {Processing} ({ICCCSP})},
	author = {Aarthi, S. and Chitrakala, S.},
	month = jan,
	year = {2017},
	keywords = {Boston University, Computational modeling, Conferences, Context, Context modeling, Image identification, Laboratory Machine Vision, Pattern Recognition, Scene grammar lab, Scene understanding, Semantics, Stafford Vision lab, Two dimensional displays, Visualization, air lab, cognitive vision, computer vision, contextual scene, dynamic scene interpretation, semantic context, semantic scene, visual information, visual scene understanding},
	pages = {1--4},
}

@inproceedings{bauckhage_cognitive_2004,
	title = {A cognitive vision system for action recognition in office environments},
	volume = {2},
	doi = {10.1109/CVPR.2004.1315250},
	abstract = {The emerging cognitive vision paradigm is concerned with vision systems that evaluate, gather and integrate contextual knowledge for visual analysis. In reasoning about events and structures, cognitive vision systems should rely on multiple computations in order to perform robustly even in noisy domains. Action recognition in an unconstrained office environment thus provides an excellent testbed for research on cognitive computer vision. In this contribution, we present a system that consists of several computational modules for object and action recognition. It applies attention mechanisms, visual learning and contextual as well as probabilistic reasoning to fuse individual results and verify their consistency. Database technologies are used for information storage and an XML based communication framework integrates all modules into a consistent architecture.},
	booktitle = {Proceedings of the 2004 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, 2004. {CVPR} 2004.},
	author = {Bauckhage, C. and Hanheide, M. and Wrede, S. and Sagerer, G.},
	month = jun,
	year = {2004},
	keywords = {Computer architecture, Computer vision, Context, Fuses, Machine vision, Robustness, Testing, Visual databases, Working environment noise, XML, XML based communication framework, action recognition, attention mechanisms, cognitive computer vision, cognitive systems, cognitive vision system, computer vision, database management systems, database technologies, gesture recognition, inference mechanisms, information storage, integrate contextual knowledge, learning systems, office environments, probabilistic reasoning, visual analysis, visual learning},
	pages = {II--II},
}

@inproceedings{marroquin_know_2018,
	title = {Know {Beyond} {Seeing}: {Combining} {Computer} {Vision} with {Semantic} {Reasoning}},
	shorttitle = {Know {Beyond} {Seeing}},
	doi = {10.1109/ICSC.2018.00061},
	abstract = {To date, computer vision systems are limited to extract the digital data of what the cameras "see". However, the meaning of what they observe could be greatly enhanced by considering the environment and common-sense knowledge. A new approach to combine computer vision with semantic modeling has been developed. This approach extracts the knowledge from images and uses it to perform real-time reasoning according to the contextual information, events of interest and logic rules. The reasoning with image knowledge allows protecting the privacy of the users, to overcome some problems of computer vision such as occlusion and missed detections and to offer services such as people guidance and people counting. This approach is the first step to develop an "all-seeing" smart building that can automatically react according to its evolving information.},
	booktitle = {2018 {IEEE} 12th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Marroquin, R. and Dubois, J. and Nicolle, C.},
	month = jan,
	year = {2018},
	keywords = {Cameras, Cognition, Computer vision, Context aware system, Data mining, Ontologies, Privacy, Semantic interoperability, Semantics, Smart camera network, all-seeing smart building, common-sense knowledge, common-sense reasoning, computer vision, computer vision systems, data privacy, digital data, environment knowledge, image knowledge, knowledge acquisition, occlusion, people counting, people guidance, semantic modeling, semantic reasoning, users privacy},
	pages = {310--311},
}

@inproceedings{kuan_region_2017,
	title = {Region average pooling for context-aware object detection},
	doi = {10.1109/ICIP.2017.8296501},
	abstract = {Object detection has been a key task in computer vision with deep convolutional neural networks being a significant performer. We propose a method named Region Average Pooling that leverages object co-occurrence to improve object detection performance. Given regions of interest in an image, our method augments object detection networks with pooled contextual features from other regions of interest in the scene. We implement our scheme and evaluate it on the Pascal Visual Object Classes (VOC) 2007 and Microsoft Common Objects in Context (MS COCO) datasets. When used as part of the Faster R-CNN object detection framework with VGG-16, we show an increase in mAP from 24.2\% to 25.5\% over baseline Faster R-CNN and Global Average Pooling when testing on MS COCO.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Kuan, K. and Manek, G. and Lin, J. and Fang, Y. and Chandrasekhar, V.},
	month = sep,
	year = {2017},
	keywords = {CNN, Computer vision, Context, Faster R-CNN, Faster R-CNN object detection framework, Feature extraction, Global Average Pooling, Microsoft Common Objects in Context datasets, Object Co-occurrence, Object Detection, Object detection, Pascal Visual Object Classes 2007, Pooling, Proposals, Region Average Pooling, Task analysis, Testing, Training, computer vision, context-aware object detection, contextual features, deep convolutional neural networks, feature extraction, feedforward neural nets, image segmentation, object cooccurrence, object detection, object detection performance, region average pooling, ubiquitous computing},
	pages = {1347--1351},
}

@inproceedings{lombardi_architectural_2004,
	title = {Architectural design issues for {Bayesian} contextual vision},
	volume = {1},
	doi = {10.1109/ICPR.2004.1334302},
	abstract = {Sensor fusion technology has been so far developed for the fusion of camera and different sensors, e.g. radar, sonar, etc. The same techniques apply to integrating several vision algorithms into a multi-modular system. In this paper, we abstract our attempt on the matter and propose a uniform paradigm to integrate both "vision modules" directly observing targets (e.g. intruders in video surveillance) and "accessory modules" observing scene features that may trigger system adaptation to the current context. To be concrete, we completely develop a real example in re-designing a previous context-dependent video surveillance system.},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Pattern} {Recognition}, 2004. {ICPR} 2004.},
	author = {Lombardi, P. and Zavidovique, B.},
	month = aug,
	year = {2004},
	keywords = {Bayes methods, Bayesian contextual vision, Bayesian methods, Cameras, Computer vision, Layout, Machine vision, Radar, Sensor fusion, Sensor systems, Sonar, Video surveillance, accessory modules, architectural design, camera fusion, computer vision, context dependent video surveillance, feature extraction, multimodular system, sensor fusion, sensor fusion technology, state estimation, surveillance, video signal processing, vision modules},
	pages = {753--756 Vol.1},
}

@inproceedings{lombardi_context-dependent_2004,
	title = {A context-dependent vision system for pedestrian detection},
	doi = {10.1109/IVS.2004.1336448},
	abstract = {Robustness is a key issue in pedestrian detection for autonomous vehicles. Contextual information, if well exploited, should increase robustness and performance. Specifically, contextual knowledge allows for the integration of algorithms performing well only in specific situations, which would otherwise be excluded from a system designed for the general case. Here, we discuss using context in a vision-based system. Contextual evolution of scene parameters is represented as the hidden process of a Hidden Markov Model. Consequently, a Bayesian framework is adopted for all principal elements, including sensor models for specialised algorithms and sensors observing the current context. Our strategy allows re-use of known algorithms, at the same time enabling context-sensitive developments.},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium}, 2004},
	author = {Lombardi, P. and Zavidovique, B.},
	month = jun,
	year = {2004},
	keywords = {Algorithm design and analysis, Bayes methods, Bayesian methods, Context modeling, Hidden Markov models, Layout, Machine vision, Mobile robots, Remotely operated vehicles, Robustness, Vehicle detection, autonomous vehicles, computer vision, context dependent vision system, contextual evolution, contextual information, contextual knowledge, hidden Markov model, hidden Markov models, mobile robots, object detection, pedestrian detection, sensor model, sensors, vehicles},
	pages = {578--583},
}

@inproceedings{torralba_context-based_2003,
	title = {Context-based vision system for place and object recognition},
	doi = {10.1109/ICCV.2003.1238354},
	abstract = {While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated into a mobile system that provides realtime feedback to the user.},
	booktitle = {Proceedings {Ninth} {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {{Torralba} and {Murphy} and {Freeman} and {Rubin}},
	month = oct,
	year = {2003},
	keywords = {Artificial intelligence, Face recognition, Image recognition, Image representation, Layout, Machine vision, Mobile robots, Navigation, Object recognition, Robot kinematics, context-based vision system, image categorization, image representation, low-dimensional global image representation, mobile robots, mobile system, object detection, object recognition, place recognition, realtime feedback, robot vision},
	pages = {273--280 vol.1},
}

@inproceedings{zoev_convolutional_2019,
	title = {Convolutional neural networks of the {YOLO} class in computer vision systems for mobile robotic complexes},
	doi = {10.1109/SIBCON.2019.8729605},
	abstract = {An important scientific direction is the development and study of computer vision systems (CVS) for mobile robotic complexes. Today, developers of CVS are most often using convolutional neural networks (CNN). For increasing the speed detection of objects on images in CVS, there has been a trend of using CNN, which are hardware-implemented on field-programmable gate array (FPGAs).This article shows that the perspective for hardware implementation on the FPGA is the tiny-YOLO CNN from the YOLO class. For reduce required FPGA computing resources in this CNN, was proposed to use Inception-ResNet modules. We was found that with high detection accuracy of objects in images with minimum resources requirements provide by the tiny-YOLO-Inception-ResNet2 network architecture. It is obtained from replacing the fifth tiny-YOLO convolutional layer of the tiny-YOLO CNN with two sequential processing Inception-ResNet modules. Also results of the study of the detection accuracy of objects using the CNN for this architecture with the lack of resource-intensive operations: batch normalization and bias from calculations were given. These studies were performed for different formats of representation numbers in the FPGA.},
	booktitle = {2019 {International} {Siberian} {Conference} on {Control} and {Communications} ({SIBCON})},
	author = {Zoev, I. V. and Beresnev, A. P. and Markov, N. G.},
	month = apr,
	year = {2019},
	keywords = {CVS, Computer architecture, Convolutional neural networks, FPGA computing resources, Field programmable gate arrays, Hardware, Open area test sites, Robots, Task analysis, YOLO class, computer vision, computer vision systems, convolutional neural nets, convolutional neural networks, field programmable gate arrays, field-programmable gate array, minimum resources requirements, mobile robotic complexes, mobile robotic systems, mobile robots, object detection on images, sequential processing Inception-ResNet modules, tiny-YOLO CNN, tiny-YOLO convolutional layer, tiny-YOLO-Inception-ResNet2 network architecture},
	pages = {1--5},
}

@inproceedings{zhang_caffeine:_2016,
	title = {Caffeine: {Towards} uniformed representation and acceleration for deep convolutional neural networks},
	shorttitle = {Caffeine},
	doi = {10.1145/2966986.2967011},
	abstract = {With the recent advancement of multilayer convolutional neural networks (CNN), deep learning has achieved amazing success in many areas, especially in visual content understanding and classification. To improve the performance and energy-efficiency of the computation-demanding CNN, the FPGA-based acceleration emerges as one of the most attractive alternatives. In this paper we design and implement Caffeine, a hardware/software co-designed library to efficiently accelerate the entire CNN on FPGAs. First, we propose a uniformed convolutional matrix-multiplication representation for both computation-intensive convolutional layers and communication-intensive fully connected (FCN) layers. Second, we design Caffeine with the goal to maximize the underlying FPGA computing and bandwidth resource utilization, with a key focus on the bandwidth optimization by the memory access reorganization not studied in prior work. Moreover, we implement Caffeine in the portable high-level synthesis and provide various hardware/software definable parameters for user configurations. Finally, we also integrate Caffeine into the industry-standard software deep learning framework Caffe. We evaluate Caffeine and its integration with Caffe by implementing VGG16 and AlexNet network on multiple FPGA platforms. Caffeine achieves a peak performance of 365 GOPS on Xilinx KU060 FPGA and 636 GOPS on Virtex7 690t FPGA. This is the best published result to our best knowledge. We achieve more than 100× speedup on FCN layers over previous FPGA accelerators. An end-to-end evaluation with Caffe integration shows up to 7.3× and 43.5× performance and energy gains over Caffe on a 12-core Xeon server, and 1.5× better energy-efficiency over the GPU implementation on a medium-sized FPGA (KU060). Performance projections to a system with a high-end FPGA (Virtex7 690t) shows even higher gains.},
	booktitle = {2016 {IEEE}/{ACM} {International} {Conference} on {Computer}-{Aided} {Design} ({ICCAD})},
	author = {Zhang, C. and {Zhenman Fang} and {Peipei Zhou} and {Peichen Pan} and {Jason Cong}},
	month = nov,
	year = {2016},
	keywords = {12-core Xeon server, Acceleration, AlexNet network, Bandwidth, CNN, Caffe integration evaluation, Caffeine design, Convolution, FCN layers, FPGA accelerators, FPGA-based acceleration, Field programmable gate arrays, Graphics processing units, Kernel, Machine learning, VGG16 network, Virtex7 690t FPGA, Xilinx KU060 FPGA, bandwidth optimization, bandwidth resource utilization, circuit optimisation, communication-intensive fully connected layers, computation-intensive convolutional layers, deep convolutional neural networks, energy conservation, energy gains, energy-efficiency, feedforward neural nets, field programmable gate arrays, hardware-software co-designed library, image classification, industry-standard software deep learning framework, learning (artificial intelligence), matrix multiplication, memory access reorganization, multilayer convolutional neural networks, performance evaluation, performance improvement, portable high-level synthesis, power aware computing, resource allocation, uniformed convolutional matrix-multiplication representation, visual content classification, visual content understanding},
	pages = {1--8},
}

@inproceedings{chadjiminas_-field_2015,
	title = {In-field vulnerability analysis of hardware-accelerated computer vision applications},
	doi = {10.1109/FPL.2015.7294003},
	abstract = {In this paper, we propose an FPGA-based emulation framework that can provide dynamic vulnerability analysis for hardware-accelerated computer vision applications. The framework can be integrated alongside the targeted application, to allow for run-time, in-field, dynamically adjusted vulnerability analysis in real-world conditions, taking into consideration the non-deterministic parameters of the computer vision algorithm computations. We evaluate the proposed framework in real-time using an FPGA platform, for an obstacle avoidance (OA) computer vision application and its disparity estimation kernel to study the impact of Single-Event Upsets (SEUs).},
	booktitle = {2015 25th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	author = {Chadjiminas, I. and Kyrkou, C. and Theocharides, T. and Michael, M. K. and Ttofis, C.},
	month = sep,
	year = {2015},
	keywords = {Approximate Computing, Circuit faults, Collision avoidance, Computer Vision, Computer vision, Decoding, FPGA, FPGA platform, FPGA-based emulation framework, Field programmable gate arrays, Intermittent Faults, Kernel, Registers, Reliability and Vulnerability Analysis, SEU, Single Event Upsets, Transient Faults, collision avoidance, computer vision, field programmable gate arrays, hardware-accelerated computer vision applications, nondeterministic parameters, obstacle avoidance computer vision application, run-time in-field dynamically adjusted vulnerability analysis, single-event upsets},
	pages = {1--4},
}

@inproceedings{maggiani_reconfigurable_2013,
	title = {Reconfigurable {FPGA} architecture for computer vision applications in {Smart} {Camera} {Networks}},
	doi = {10.1109/ICDSC.2013.6778212},
	abstract = {Smart Camera Networks (SCNs) is nowadays an emerging research field which represents the natural evolution of centralized computer vision applications towards full distributed and pervasive systems. In such a scenario, one of the biggest effort is in the definition of a flexible and reconfigurable SCN node architecture able to remotely support the possibility of updating the application parameters and changing the running computer vision applications at run-time. In this respect, this paper presents a novel SCN node architecture based on a device in which a microcontroller manages all the network functionality as well as the remote configuration, while an FPGA implements all the necessary module of a full computer vision pipeline. In the paper the envisioned architecture is first detailed in general terms, then a real implementation is presented to show the feasibility and the benefits of the proposed solution. Finally, performance evaluation results prove the potential of hardware software codesign in reaching flexibility and reduced latency time.},
	booktitle = {2013 {Seventh} {International} {Conference} on {Distributed} {Smart} {Cameras} ({ICDSC})},
	author = {Maggiani, L. and Salvadori, C. and Petracca, M. and Pagano, P. and Saletti, R.},
	month = oct,
	year = {2013},
	keywords = {Computer Vision, Computer architecture, Computer vision, FPGA, Field programmable gate arrays, Hardware, Hardware Software Codesign, Hardware design languages, Pipelines, Smart Camera Network, Software, application parameters, centralized computer vision applications, computer vision, computer vision pipeline, distributed system, field programmable gate arrays, flexible SCN node architecture, hardware software codesign, hardware-software codesign, latency time, microcontroller, microcontrollers, natural evolution, network functionality, pervasive system, pipeline processing, reconfigurable FPGA architecture, reconfigurable SCN node architecture, reconfigurable architectures, remote configuration, smart camera networks, ubiquitous computing},
	pages = {1--6},
}

@inproceedings{honegger_real-time_2014,
	title = {Real-time and low latency embedded computer vision hardware based on a combination of {FPGA} and mobile {CPU}},
	doi = {10.1109/IROS.2014.6943263},
	abstract = {Recent developments in smartphones create an ideal platform for robotics and computer vision applications: they are small, powerful, embedded devices with low-power mobile CPUs. However, though the computational power of smartphones has increased substantially in recent years, they are still not capable of performing intense computer vision tasks in real time, at high frame rates and low latency. We present a combination of FPGA and mobile CPU to overcome the computational and latency limitations of mobile CPUs alone. With the FPGA as an additional layer between the image sensor and CPU, the system is capable of accelerating computer vision algorithms to real-time performance. Low latency calculation allows for direct usage within control loops of mobile robots. A stereo camera setup with disparity estimation based on the semi global matching algorithm is implemented as an accelerated example application. The system calculates dense disparity images with 752×480 pixels resolution at 60 frames per second. The overall latency of the disparity estimation is less than 2 milliseconds. The system is suitable for any mobile robot application due to its light weight and low power consumption.},
	booktitle = {2014 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Honegger, D. and Oleynikova, H. and Pollefeys, M.},
	month = sep,
	year = {2014},
	keywords = {Cameras, Estimation, FPGA, Field programmable gate arrays, Image sensors, Mobile communication, Real-time systems, Streaming media, computer vision, control engineering computing, dense disparity images, disparity estimation, embedded systems, field programmable gate arrays, image matching, image sensor, image sensors, low latency embedded computer vision hardware, low-power mobile CPUs, mobile computing, mobile robot control loops, mobile robots, real-time embedded computer vision hardware, semiglobal matching algorithm, smartphones, stereo camera setup},
	pages = {4930--4935},
}

@inproceedings{qasaimeh_analyzing_2019,
	title = {Analyzing the {Energy}-{Efficiency} of {Vision} {Kernels} on {Embedded} {CPU}, {GPU} and {FPGA} {Platforms}},
	doi = {10.1109/FCCM.2019.00077},
	abstract = {This paper presents a benchmark of the energy efficiency of a wide range of vision kernels on three commonly used hardware accelerators for embedded vision applications: ARM57 CPU, Jetson TX2 GPU and ZCU102 FPGA, using their vendor optimized vision libraries: OpenCV, VisionWorks and xfOpenCV. Our results show that the GPU achieves an energy/frame reduction ratio of 1.1-3.2× compared to CPU and FPGA for simple kernels. While for more complicated kernels, the FPGA outperforms the others with energy/frame reduction ratios of 1.2-22.3×. It is also observed that the FPGA performs increasingly better as a vision kernel's complexity grows.},
	booktitle = {2019 {IEEE} 27th {Annual} {International} {Symposium} on {Field}-{Programmable} {Custom} {Computing} {Machines} ({FCCM})},
	author = {Qasaimeh, M. and Zambreno, J. and Jones, P. H. and Denolf, K. and Lo, J. and Vissers, K.},
	month = apr,
	year = {2019},
	keywords = {ARM57 CPU, Benchmark testing, CPU, Complexity theory, FPGA, FPGA platforms, Field programmable gate arrays, GPU, Graphics processing units, Hardware, Jetson TX2 GPU, Kernel, OpenCV, Optical filters, VisionWorks, ZCU102 FPGA, accelerators, benchmark, computer vision, embedded, embedded CPU, embedded vision applications, energy conservation, energy efficiency, energy-frame reduction ratio, energy/frame, field programmable gate arrays, graphics processing units, hardware, hardware accelerators, image processing, kernles, performance, power, power aware computing, vendor optimized vision libraries, vision kernel complexity, vision kernels, vision libraries, visionWorks, xfOpenCV},
	pages = {336--336},
}

@inproceedings{nakahara_lightweight_2018,
	address = {New York, NY, USA},
	series = {{FPGA} '18},
	title = {A {Lightweight} {YOLOv2}: {A} {Binarized} {CNN} with {A} {Parallel} {Support} {Vector} {Regression} for an {FPGA}},
	isbn = {978-1-4503-5614-5},
	shorttitle = {A {Lightweight} {YOLOv2}},
	url = {http://doi.acm.org/10.1145/3174243.3174266},
	doi = {10.1145/3174243.3174266},
	abstract = {A frame object detection problem consists of two problems: one is a regression problem to spatially separated bounding boxes, the second is the associated classification of the objects within realtime frame rate. It is widely used in the embedded systems, such as robotics, autonomous driving, security, and drones - all of which require high-performance and low-power consumption. This paper implements the YOLO (You only look once) object detector on an FPGA, which is faster and has a higher accuracy. It is based on the convolutional deep neural network (CNN), and it is a dominant part both the performance and the area. However, the object detector based on the CNN consists of a bounding box prediction (regression) and a class estimation (classification). Thus, the conventional all binarized CNN fails to recognize in most cases. In the paper, we propose a lightweight YOLOv2, which consists of the binarized CNN for a feature extraction and the parallel support vector regression (SVR) for both a classification and a localization. To our knowledge, this is the first time binarized CNN»s have been successfully used in object detection. We implement a pipelined based architecture for the lightweight YOLOv2 on the Xilinx Inc. zcu102 board, which has the Xilinx Inc. Zynq Ultrascale+ MPSoC. The implemented object detector archived 40.81 frames per second (FPS). Compared with the ARM Cortex-A57, it was 177.4 times faster, it dissipated 1.1 times more power, and its performance per power efficiency was 158.9 times better. Also, compared with the nVidia Pascall embedded GPU, it was 27.5 times faster, it dissipated 1.5 times lower power, and its performance per power efficiency was 42.9 times better. Thus, our method is suitable for the frame object detector for an embedded vision system.},
	urldate = {2019-07-05},
	booktitle = {Proceedings of the 2018 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {ACM},
	author = {Nakahara, Hiroki and Yonekawa, Haruyoshi and Fujii, Tomoya and Sato, Shimpei},
	year = {2018},
	note = {event-place: Monterey, CALIFORNIA, USA},
	keywords = {binarized deep neural network, convolutional deep neural network, object detection},
	pages = {31--40},
}

@inproceedings{xu_scalable_2019,
	title = {A {Scalable} {OpenCL}-{Based} {FPGA} {Accelerator} for {YOLOv2}},
	doi = {10.1109/FCCM.2019.00058},
	abstract = {This paper implements an OpenCL-based FPGA accelerator for YOLOv2 on Arria-10 GX1150 FPGA board. The hardware architecture adopts a scalable pipeline design to support multi-resolution input image, and improves resource utilization by full 8-bit fixed-point computation and CONV+BN+Leaky-ReLU layer fusion technology. The proposed design achieves a peak throughput of 566 GOPs under 190 MHz working frequency. The accelerator could run YOLOv2 inference with 288×288 input resolution and tiny YOLOv2 with 416×416 input resolution at the speed of 35 and 71 FPS, respectively.},
	booktitle = {2019 {IEEE} 27th {Annual} {International} {Symposium} on {Field}-{Programmable} {Custom} {Computing} {Machines} ({FCCM})},
	author = {Xu, K. and Wang, X. and Wang, D.},
	month = apr,
	year = {2019},
	keywords = {8-bit fixed-point computation, Accelerator, Arria-10 GX1150 FPGA board, CONV+BN+Leaky-ReLU layer fusion technology, Computer architecture, Convolution, FPGA, Field programmable gate arrays, Hardware, Kernel, Object Detection, Object detection, OpenCL, Pipelines, YOLOv2, YOLOv2 inference, field programmable gate arrays, hardware architecture, image resolution, multiresolution input image, resource utilization, scalable OpenCL-based FPGA accelerator, scalable pipeline design},
	pages = {317--317},
}

@inproceedings{aydonat_opencl_2017,
	address = {New York, NY, USA},
	series = {{FPGA} '17},
	title = {An {OpenCL}™ {Deep} {Learning} {Accelerator} on {Arria} 10},
	isbn = {978-1-4503-4354-1},
	url = {http://doi.acm.org/10.1145/3020078.3021738},
	doi = {10.1145/3020078.3021738},
	abstract = {Convolutional neural nets (CNNs) have become a practical means to perform vision tasks, particularly in the area of image classification. FPGAs are well known to be able to perform convolutions efficiently, however, most recent efforts to run CNNs on FPGAs have shown limited advantages over other devices such as GPUs. Previous approaches on FPGAs have often been memory bound due to the limited external memory bandwidth on the FPGA device. We show a novel architecture written in OpenCL(TM), which we refer to as a Deep Learning Accelerator (DLA), that maximizes data reuse and minimizes external memory bandwidth. Furthermore, we show how we can use the Winograd transform to significantly boost the performance of the FPGA. As a result, when running our DLA on Intel's Arria 10 device we can achieve a performance of 1020 img/s, or 23 img/s/W when running the AlexNet CNN benchmark. This comes to 1382 GFLOPs and is 10x faster with 8.4x more GFLOPS and 5.8x better efficiency than the state-of-the-art on FPGAs. Additionally, 23 img/s/W is competitive against the best publicly known implementation of AlexNet on nVidia's TitanX GPU.},
	urldate = {2019-07-05},
	booktitle = {Proceedings of the 2017 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {ACM},
	author = {Aydonat, Utku and O'Connell, Shane and Capalija, Davor and Ling, Andrew C. and Chiu, Gordon R.},
	year = {2017},
	note = {event-place: Monterey, California, USA},
	keywords = {convolutional neural networks, deep neural networks},
	pages = {55--64},
}

@inproceedings{vestias_parallel_2017,
	title = {Parallel dot-products for deep learning on {FPGA}},
	doi = {10.23919/FPL.2017.8056863},
	abstract = {Deep neural networks have recently shown great results in a vast set of image applications. The associated deep learning models are computationally very demanding and, therefore, several hardware solutions have been proposed to accelerate their computation. FPGAs have recently shown very good performances for these kind of applications and so it is considered a promising platform to accelerate the execution of deep learning algorithms. A common operation in these algorithms is multiply-accumulate (MACC) that is used to calculate dot-products. Since many dot products can be calculated in parallel, as long as memory bandwidth is available, it is very important to implement this operation very efficiently to increase the density of MACC units in an FPGA. In this paper, we propose an implementation of parallel MACC units in FPGA for dot-product operations with very high performance/area ratios using a mix of DSP blocks and LUTs. We consider fixed-point representations with 8 bits of size, but the method can be applied to other bit widths. The method allows us to achieve TOPs performances, even for low cost FPGAs.},
	booktitle = {2017 27th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	author = {Véstias, M. and Duarte, R. P. and Sousa, J. T. de and Neto, H.},
	month = sep,
	year = {2017},
	keywords = {Adders, Bandwidth, DSP blocks, Deep learning, Digital signal processing, FPGA, Field programmable gate arrays, LUT, Lead, Machine learning, Multiply-accumulate, TOPs performances, Table lookup, deep learning algorithms, deep learning models, deep neural networks, digital signal processing chips, dot-product operations, field programmable gate arrays, fixed point arithmetic, fixed-point representation, hardware solutions, high performance-area ratios, image applications, image processing, learning (artificial intelligence), neural nets, parallel MACC units, parallel dot-products},
	pages = {1--4},
}
