\section{Background and Related Work}
\label{sec:background_and_related_work}

% \textbf{TODO!!!! Make sure the 5 papers we asses end up discussed here too}

% Dario [X]
% https://dl.acm.org/doi/pdf/10.1145/3379597.3387491 (How Often Do Single Statement Bugs Occur)

% Caleb [X]
% https://www.sciencedirect.com/science/article/pii/S0950584917304305 (The role and value of replication in empirical software engineering results)

% Abhimanyu []
% https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.8408&rep=rep1&type=pdf (D. Hovemeyer, J. Spacco, and W. Pugh, â€œEvaluating and tuning a static analysis to find null pointer bugs")

% Jordan [X]
% https://dl.acm.org/doi/10.1145/2884781.2884841 (Nomen est omen: exploring and exploiting similarities between argument and parameter names)

% Young []
% https://dl.acm.org/doi/pdf/10.1145/3183519.3183525 (Modern Code Review: A Case Study at Google) :^} XOR https://dl.acm.org/doi/pdf/10.1145/1287624.1287634 (Context-Based Detection of Clone-Related Bugs)



%\textbf{FINAL REPORT TODO!!!! Explain what name-related bugs are, what the switched argument bug is and how it cannot be detected by conventional methods}

% done 

% \textbf{TODO!!!! Go through Prof. Davis' Background/Related Work comments to be sure we got everything}

% done

%\textbf{TODO!!!! Talk about why replication studies are important}

% wasn't this done in motivation? And then it is done a few times in ethics. Does it need to be done again here?


\subsection{Automated bug detection}
\label{subsec:deepbugs_related_work}
Automated bug detection is an active area of research. There is a wealth of statistical information in identifier names. \cite{liu_nomen_2016} DeepBugs specifically addresses name-based bug detection unlike several popular static analysis tools, such as Google Error Prone \cite{aftandilian_building_2012} and FindBugs \cite{hovemeyer_evaluating_2005} that mostly ignores identifier names. Name-based bugs are bugs that can be tracked by linguistic similarity. An example of this is the association of the variable $x$ with width and $y$ with height. In a program, this could lead to a switch-argument bug. Switch-argument bugs are part of a class of bugs called single-statement bugs, which have been reported to consist possibly as much as 33\% of bugs in some Java projects. \cite{karampatsis_how_2020}

The reason many program analysis tools ignore identifier names is because such detection is challenging. Name-based analysis must encode the meaning of identifier names, which can be based on subconscious cultural knowledge. Although lexical reasoning is often used to achieve this, existing name-based analysis relies on manually designed algorithms that use hard-coded patterns and tuned heuristics which require significant amount of time and human effort \cite{host_debugging_2009, noauthor_nomen_nodate, pradel_deepbugs_2018, noauthor_detecting_nodate}. In contrast, DeepBugs uses a deep learning approach to detect bugs automatically. This is done by training the neural network on a large corpus of code examples.

Before the identifiers can be trained on, they must first be extracted from code. Conditional random fields and statistical machine translation have been used to do this in works such as JSNice \cite{raychev_predicting_2015} and JSNaughty \cite{vasilescu_recovering_2017}. DeepBugs used Word2Vec \cite{mikolov_efficient_2013} to automatically generate vector representations of code snippets to feed into a neural network. Furthermore, in contrast to previous work, DeepBugs focused on implicitly typed languages, where information about the code semantics is conveyed through identifier names by programmers instead of types. The overall framework complements any designed bug detector that relies on processes inducing significant manual work, thereby replacing human effort by transitioning to computational means to achieve the same. 

\subsection{Similar models for analyzing and finding bugs}
\label{subsec:Models for Analyzing and Finding Bugs}

The DeepBugs model learns from positive examples and negative examples, focusing on bug patterns that can be expressed via probabilistic means. On the other hand, Bugram \cite{wang_bugram_2016} uses an n-gram model of code to detect bugs with positive examples only (i.e., anomaly detection). Specification mining \cite{ammons_mining_2002} uses mined features and specifications to detect unusual bugs in code \cite{liang_antminer_2016,pradel_deepbugs_2018,wasylkowski_mining_2011} while learning only from correct examples and flagging any inconsistencies and errors. 

While DeepBugs finds bugs and pinpoints the bug location, Wang, et al.'s approach uses a deep belief network \cite{wang_automatically_2016}  to flag buggy code files for further inspection. Ultimately, DeepBugs differs from existing name-based bug detectors in that it (1) exploits semantic similarities that may not be identified by lexical comparison of identifies, (2) improves generalizability by replacing manually tuned heuristics with a deep neural network.

\subsection{Background on replication studies}
\label{subsec:background_on_replication_studies}


According to the ACM, replication of an existing study should be performed by a ``different team, with the same experimental setup" \cite{noauthor_artifact_nodate}. And according to BSEL, complete reporting is also highly important due to possible high variance of the parameter of interest. \cite{shepperd_role_2018} More concretely, the replication study should be conducted without deploying the original authors' artifacts and should instead use independently generated artifacts. The original study is verified if those new artifacts achieve the same results as those found in the original paper. A reproduction study differs from a replication study: a reproduction study checks whether the original results can be obtained using the author's artifacts, while a replication study requires the artifacts to be generated from scratch. Our study will reuse the original authors' code data set \cite{pradel_michaelpradeldeepbugs_2021} but will implement the DeepBugs algorithms using our own source code. Therefore, our project would qualify as a replication study.

It is worth noting that replication studies remain largely unwelcome in academic venues; for example, only 3\% of 1,500 top psychology journals welcome replication work as submissions \cite{martin_are_2017}. Fortunately, in recent years, a few research communities have tried to increase incentive for replication studies. Targeted at software engineering research, the ROSE Festival exclusively features replication work \cite{noauthor_rose_nodate}. Similarly, the Association for Information Systems publishes an entire journal, \textit{Transactions on Replication Research}, dedicated to reproducibility studies \cite{noauthor_ais_nodate}.

Our replication study of DeepBugs will be unique; to our knowledge, there exists no such study for that work.