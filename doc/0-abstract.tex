\begin{abstract}
\label{sec:abstract}

INTRODUCTION: Reproducibility and replication of existing work is important in the research community. For works that are not reproduced, the validity of the work may be questioned, since the results are unvalidated. The research community has labeled the large amount of unreproduced work as the  ``reproducibility crisis''. Researchers often do not undertake replication studies because there are relatively few incentives to do so.
In this project, we replicate \textit{DeepBugs} by Pradel and Sen, a novel automated name-based bugs detection framework. We re-implement the DeepBugs algorithm from scratch and test it with the original dataset, verifying the original authors' claims.
STATE-OF-ART: The research community is encouraging more replication studies, with journals and workshops dedicated to replication work. Prior to this paper, there were no replication studies for DeepBugs. CONTRIBUTION: In this project, we perform a replication study of the DeepBugs name-based bug detection framework. This provides validation for those who wish to implement DeepBugs into their work. METHOD: We re-implement the DeepBugs artifacts for training set generation, source code vectorization, and neural network training and testing. The team uses Python, while the original work uses mostly JavaScript. RESULTS: The team successfully replicated DeepBugs by following the process outlined in the original work. The resulting reimplementation of the framework has an accuracy only 2\% lower than that of the original.
\end{abstract}