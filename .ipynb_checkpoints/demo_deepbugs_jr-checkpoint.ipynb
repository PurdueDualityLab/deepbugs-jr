{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0fc4836a1f8c77ef59b46ee91a877fd19e2c2e6498eab2bc8311adf4e0428d723",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "b484c80a5327703a6fe0da11ec022652aebbf779d66ce1c66dfcc2d4536d1019"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# How to Use DeepBugs for Yourself\n",
    "Follow along with this notebook to reproduce our replication of DeepBugs, tested on the switched-argument bug (i.e., the developer accidentally typed the arguments in reverse order.)\n",
    "\n",
    "Or, feel free to just check out the pre-saved output - things can take a while to run.\n",
    "\n",
    "You can also use the functions we provide to deploy DeepBugs in your own code!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Round up the source code\n",
    "\n",
    "Start by downloading the 150k JavaScript Dataset using the links below.\n",
    "\n",
    "* [Training Data - 10.0GB](https://1drv.ms/u/s!AvvT9f1RiwGbh6hYNoymTrzQcNA46g?e=WeJf3K)\n",
    "* [Testing Data - 4.8GB](https://1drv.ms/u/s!AvvT9f1RiwGbh6hXmjPOUS-kBARjFA?e=AJY1Xf)\n",
    "\n",
    "Save them into the `demo_data` folder."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Convert the source code ASTs to tokens\n",
    "For a given corpus of code, you should have a large list of source files, each of which is converted into an Abstract Syntax Tree (AST).\n",
    "\n",
    "In this example, we convert each AST from the 150k JavaScript Dataset into a list of tokens (e.g., \"ID:setInterval\" or \"LIT:true\"). Those lists are aggregated together into a master list of lists. This list-of-list format is important for training Word2Vec, since each list of tokens corresponds to a single source file -  tokens within a source file are closely related but tokens across source files may not be as closely related.\n",
    "\n",
    "Example:\n",
    "```\n",
    "[\n",
    "    [ # Corresponds to first source file\n",
    "        \"ID:setInterval\",\n",
    "        \"LIT:1000\",\n",
    "        \"ID:callbackFn\",\n",
    "        \"LIT:true\",\n",
    "        \"LIT:http-mode\",\n",
    "        ...\n",
    "    ],\n",
    "    [ # Corresponds to second source file\n",
    "        \"ID:fadeIn\",\n",
    "        \"LIT:300\",\n",
    "        \"ID:css\",\n",
    "        \"LIT:color:red;margin:auto\",\n",
    "        ...\n",
    "    ]\n",
    "]\n",
    "```\n",
    "### Note on using our code\n",
    "If you organize your ASTs into one file, such that each line of the file corresponds to one AST, you can just call our ready-to-go `ast_token_extractor.get_tokens_from_corpus()` function as shown below.\n",
    "\n",
    "If you need more fine-grained control, you could use `ast_token_extractor.get_tokens_from()` to extract tokens from each node in a single AST."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100000it [54:32, 30.55it/s]Extracted 100000 tokens\n",
      "A few examples...\n",
      "['ID:gTestfile', 'LIT:regress-472450-04.js', 'ID:BUGNUMBER', 'LIT:472450', 'ID:summary', 'LIT:TM: Do not assert: StackBase(fp) + blockDepth == regs.sp', 'ID:actual', 'LIT:', 'ID:expect', 'LIT:', 'ID:test', 'ID:test', 'ID:enterFunc', 'LIT:test', 'ID:printBugNumber', 'ID:BUGNUMBER', 'ID:printStatus', 'ID:summary', 'ID:jit', 'LIT:true', 'ID:__proto__', 'ID:âœ–', 'LIT:1', 'ID:f', 'ID:eval', \"LIT:for (var y = 0; y < 1; ++y) { for each (let z in [null, function(){}, null, '', null, '', null]) { let x = 1, c = []; } }\", 'ID:f', 'ID:jit', 'LIT:false', 'ID:reportCompare', 'ID:expect', 'ID:actual', 'ID:summary', 'ID:exitFunc', 'LIT:test']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ast_token_extractor import get_tokens_from_corpus\n",
    "\n",
    "TRAIN_DATA_PATH = \"demo_data/150k_training.json\"\n",
    "TEST_DATA_PATH = \"demo_data/150k_testing.json\"\n",
    "\n",
    "list_of_lists_of_tokens = get_tokens_from_corpus(TRAIN_DATA_PATH)\n",
    "\n",
    "# Count the tokens extracted\n",
    "num_tokens_extracted = len([len(tokens_from_single_src_file) for tokens_from_single_src_file in list_of_lists_of_tokens])\n",
    "\n",
    "print(\"Extracted {0} tokens\".format(num_tokens_extracted))\n",
    "print(\"A few examples...\")\n",
    "print(list_of_lists_of_tokens[0])\n"
   ]
  },
  {
   "source": [
    "## Convert tokens to vectors: train a Word2Vec model\n",
    "Now that you have reduced your dataset to lists of tokens, you can use them to train a Word2Vec model so that it predicts a vector for each token based on lexical similarity. In other words, a token of `LIT:true` will be lexically similar to a token of `LIT:1` but not `LIT:false`.\n",
    "\n",
    "We train Word2Vec using the Continuous Bag of Words method with a 200-word window (i.e. for a given token, we use the previous 100 tokens and the following 100 tokens to learn the context of the token). Like the original authors, we limit the vocabulary size to the top 10,000 tokens from the dataset.\n",
    "\n",
    "### Note on using our code\n",
    "As long as you have one list of tokens per source file, aggregated into a master list of all source files, then you can call our ready-made `token2vectorizer.train_word2vec()` function as shown below.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Should be larger difference btwn LIT:true and LIT:false 0.8614448\nShould be smaller difference btwn LIT:true and LIT:1 0.09773022\n"
     ]
    }
   ],
   "source": [
    "from token2vectorizer import train_word2vec\n",
    "\n",
    "WORD2VEC_MODEL_SAVE_PATH = \"demo_data/word2vec.model\"\n",
    "\n",
    "model = train_word2vec(list_of_lists_of_tokens, WORD2VEC_MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"Should be larger difference btwn LIT:true and LIT:false\", model.wv.similarity(\"LIT:true\", \"LIT:false\"))\n",
    "print(\"Should be smaller difference btwn LIT:true and LIT:1\", model.wv.similarity(\"LIT:true\", \"LIT:1\"))\n"
   ]
  },
  {
   "source": [
    "## Save your token-vector vocabulary for later\n",
    "To speed things up when you're training and testing DeepBugs, you should save off your learned Word2Vec vocabulary in a dictionary for rapid lookup and sharing. Our `token2vectorizer.save_token2vec_vocabulary()` handles this for you in a jiffy.\n",
    "\n",
    "Example output:\n",
    "```\n",
    "{\n",
    "    \"LIT:true\": [-5.174832   -4.9506106   1.6868128   1.476279   -3.211739   ...],\n",
    "    ...\n",
    "}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A couple examples...\nID:Date:  [8.347164154052734, -1.7503434419631958, 2.52799654006958, 4.932645320892334, 4.3485941886901855, 2.4734597206115723, -3.104377508163452, -4.045936107635498, 7.203773498535156, -4.954057693481445, 2.7399654388427734, 7.58334493637085, -1.654502272605896, -7.7606201171875, 4.609239101409912, 0.38020211458206177, -10.710118293762207, 5.223881244659424, 2.477747917175293, 1.0023224353790283, -1.0500743389129639, -1.7351078987121582, 5.0433030128479, -4.859694004058838, -2.5254316329956055, -3.685211420059204, 3.842613697052002, -1.4842250347137451, 6.4144511222839355, -1.6959937810897827, -8.342519760131836, -8.662407875061035, -3.0137434005737305, 4.000683784484863, 6.644257068634033, 2.8766849040985107, 2.4778969287872314, 10.9281587600708, 2.503817558288574, 5.285634517669678, -3.4404542446136475, -0.23131144046783447, -3.2403600215911865, 9.892898559570312, -5.4444804191589355, -0.7952022552490234, -2.1439621448516846, 5.729167938232422, 2.913297653198242, 6.351028919219971, -10.632973670959473, -5.219524383544922, -4.408332347869873, 1.8337897062301636, 5.758188724517822, -4.1768951416015625, -2.7019200325012207, 2.829066514968872, -0.3064640760421753, 7.73088264465332, -4.690291881561279, 7.2873640060424805, 6.347229480743408, -3.3764355182647705, 1.9173465967178345, 7.06627893447876, -8.563305854797363, -7.999477386474609, 7.170079708099365, 5.661990165710449, -0.691128671169281, -12.47020435333252, -4.715322017669678, 1.6105307340621948, 0.5585780143737793, -3.1342241764068604, 0.6242284178733826, 0.2653554677963257, -1.8018417358398438, -0.6942118406295776, 4.761747360229492, -7.1280012130737305, -2.2254960536956787, -6.321258068084717, -3.191289186477661, -3.196883201599121, -1.9151408672332764, -2.300696849822998, 7.331551551818848, -5.745677947998047, -3.683633327484131, -1.088383674621582, -1.7166122198104858, -2.4791204929351807, 11.649948120117188, -13.220796585083008, -4.817105293273926, 4.473062038421631, -6.890395164489746, 0.2667549252510071] \n\nID:end:  [-0.8643208742141724, 7.5463666915893555, 0.2106315940618515, -4.696891784667969, 0.4101337492465973, -3.5511488914489746, 4.461061000823975, -2.763587474822998, 11.731926918029785, -0.8437216877937317, -4.024574279785156, 12.316003799438477, -2.807643413543701, -3.0953876972198486, -9.577333450317383, 3.689511775970459, -0.9159514307975769, -6.408436298370361, -1.1511777639389038, -0.8101019859313965, -3.5829992294311523, 1.9186807870864868, -2.5808372497558594, -3.8245675563812256, 5.397223472595215, 1.3228422403335571, -7.8572492599487305, -2.235279083251953, 3.4332189559936523, -0.8385573029518127, -1.4015963077545166, 0.5073524117469788, -4.539588928222656, 0.5381431579589844, -1.3356724977493286, -0.08196244388818741, -6.520443439483643, 5.592146873474121, -8.981557846069336, 3.220486640930176, -6.441158294677734, -2.448096513748169, 6.580772876739502, -0.5480377674102783, -1.4042646884918213, 4.901791572570801, 1.6531002521514893, -1.8628458976745605, -3.548975944519043, 7.232443332672119, 2.9267513751983643, -0.06452445685863495, -0.35293519496917725, -3.171281576156616, 4.340519428253174, 3.4684228897094727, 3.9732565879821777, -0.3475736081600189, 3.077282667160034, -1.8089007139205933, -0.8725517392158508, 6.588842868804932, 0.5473924279212952, 1.9543901681900024, -1.007826328277588, -0.649209201335907, -5.235541343688965, -2.3635287284851074, -2.8205935955047607, 0.9048876166343689, -7.379077434539795, -5.800272464752197, 2.1336452960968018, 4.186258792877197, -0.6601552963256836, 6.353935241699219, 0.8680839538574219, -3.044222354888916, 4.6930341720581055, 2.2796247005462646, 1.160461664199829, -9.122596740722656, 7.348650932312012, -4.758178234100342, 8.099509239196777, -1.1045327186584473, -2.4302752017974854, -5.389317035675049, -7.031600475311279, -7.438356399536133, 3.6593573093414307, 0.8824549913406372, 2.2604448795318604, -0.6421619057655334, 2.2371931076049805, -8.25434684753418, 3.5360751152038574, 3.748002529144287, 2.0115623474121094, -6.741771221160889]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from token2vectorizer import save_token2vec_vocabulary\n",
    "\n",
    "WORD2VEC_MODEL_READ_PATH = \"demo_data/word2vec.model\"\n",
    "VOCAB_SAVE_PATH = \"demo_data/token2vec.json\"\n",
    "\n",
    "model = Word2Vec.load(WORD2VEC_MODEL_READ_PATH)\n",
    "save_token2vec_vocabulary(model, VOCAB_SAVE_PATH)\n",
    "\n",
    "with open(VOCAB_SAVE_PATH) as example_json:\n",
    "    vocab = json.load(example_json)\n",
    "    print(\"A couple examples...\")\n",
    "    print(\"ID:Date: \", vocab[\"ID:Date\"], \"\\n\")\n",
    "    print(\"ID:end: \", vocab[\"ID:end\"])"
   ]
  },
  {
   "source": [
    "## Generate positive/negative examples\n",
    "\n",
    "In our example, we our testing for the switched-argument bug that the DeepBugs authors tested for, so we generate data by extracting all 2-argument function calls from the 150k dataset and then manually switching the arguments around to make \"buggy\" examples.\n",
    "\n",
    "### Note on using our code\n",
    "Our code is specific to switched-argument bugs. For your own bugs, you will need to write your own code to generate positive and negative training/testing examples. You can follow similar procedures to our `swarg_` scripts.\n",
    "\n",
    "We save our examples as `.npz` files, where each file is a `Tuple[List,List]`: `(Data, Labels)`. Both `Data` and `Labels` are numpy arrays of the same length, where `Labels[i]` is 1 for positive, 0 for negative"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100000it [48:05, 34.66it/s]\n",
      "50000it [16:45, 49.74it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[  0.66330397,  -4.8157773 , -15.53928185, ...,   3.67067075,\n",
       "           1.37784445,  -2.45487404],\n",
       "        [  0.66330397,  -4.8157773 , -15.53928185, ...,   3.67067075,\n",
       "           1.37784445,  -2.45487404],\n",
       "        [-15.30671787,  -2.32782769,   5.04741478, ...,   3.74800253,\n",
       "           2.01156235,  -6.74177122],\n",
       "        ...,\n",
       "        [ 11.28853989,  17.92754173,  -2.10642552, ...,   0.20978755,\n",
       "          -6.04589081,  -2.70742297],\n",
       "        [ 11.28853989,  17.92754173,  -2.10642552, ...,   0.20978755,\n",
       "          -6.04589081,  -2.70742297],\n",
       "        [ -0.11831304,  -1.30880451,  -0.03427336, ...,   7.0901885 ,\n",
       "          -0.91956091,  -5.06203985]]),\n",
       " array([1., 1., 1., ..., 0., 0., 0.]))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "import json\n",
    "from swarg_gen_train_eval import gen_good_bad_fn_args\n",
    "from swarg_fnargs2tokens import get_all_2_arg_fn_calls_from_ast\n",
    "\n",
    "VOCAB_READ_PATH = \"demo_data/token2vec.json\"\n",
    "\n",
    "SWARG_TRAIN_EXAMPLES_SAVE_PATH = \"demo_data/switch_arg_train.npz\"\n",
    "SWARG_TEST_EXAMPLES_SAVE_PATH = \"demo_data/switch_arg_test.npz\"\n",
    "\n",
    "gen_good_bad_fn_args(TRAIN_DATA_PATH, VOCAB_READ_PATH, SWARG_TRAIN_EXAMPLES_SAVE_PATH)\n",
    "gen_good_bad_fn_args(TEST_DATA_PATH, VOCAB_READ_PATH, SWARG_TEST_EXAMPLES_SAVE_PATH)\n"
   ]
  },
  {
   "source": [
    "## Train DeepBugs\n",
    "We use examples generated from the training partition of the 150K JavaScript Dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Jordan and Abhi's code can just slot in here\n"
   ]
  },
  {
   "source": [
    "## Test DeepBugs\n",
    "We use examples generated from the test partition of the 150K JavaScript Dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Jordan and Abhi's code can just slot in here"
   ]
  }
 ]
}